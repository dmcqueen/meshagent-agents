version: v1
kind: ServiceTemplate
variables: []

metadata:
  name: claude-bio-research
  description: An agent that uses Anthropic's bio research knowledge work plugin and powered by Claude Sonnet 4.5
  annotations:
    meshagent.service.id: meshagent.claude-bio-research

agents:
  - name: claude-bio-research
    annotations:
      meshagent.agent.type: ChatBot

container:
  image: us-central1-docker.pkg.dev/meshagent-life/meshagent-public/cli:{SERVER_VERSION}-esgz
  command: /bin/bash /var/start.sh
  storage:
    room:
    - path: /data
      read_only: false
    files:
    - path: /var/start.sh
      text: |
          #!/bin/bash

          set -e

          mkdir -p /data/knowledge-work-plugins
          if [ -d /knowledge-work-plugins ]; then
            cp -R -n /knowledge-work-plugins/* /data/knowledge-work-plugins/ 2>/dev/null || true
          fi

          exec /usr/bin/meshagent chatbot join \
            --model=claude-sonnet-4-5 \
            --require-storage \
            --require-web-search \
            --rule="you can use the storage tool to read skills not just the shell tool" \
            --rule="when a question could be served by a skill, read the skill and follow the pattern specified in the skill" \
            --rule="when using the storage tool to read files attached or referenced by the user, in the room, or output by the shell tool, the file path should be prefixed with /data." \
            --rule="when using the storage tool to read skills, the path should be prefixed with /data/knowledge-work-plugins (the folder where the plugins are located)" \
            --storage-tool-local-path=/knowledge-work-plugins:/knowledge-work-plugins \
            --storage-tool-room-path=/:/data \
            --script-tool \
            -rr=agents/claude-bio-research/rules.txt \
            --rule='You have customizable rules stored in agents/claude-bio-research/rules.txt, you can use the read_file tool to read your rules. You can use the write_file tool to update the contents of the rules file or other text files. Use the read_file tool to read PDFs, examine images, or read files with a text/* mime type from attachments or files.' \
            --rule='You are a MeshAgent agent. MeshAgent is an agent operating system. You can find out more at www.meshagent.com and docs.meshagent.com' \
            --rule='You have some slash commands available' \
            --rules-file /data/knowledge-work-plugins/commands/start.md \
            --skill-dir /data/knowledge-work-plugins/skills/instrument-data-to-allotrope \
            --skill-dir /data/knowledge-work-plugins/skills/nextflow-development \
            --skill-dir /data/knowledge-work-plugins/skills/scientific-problem-selection \
            --skill-dir /data/knowledge-work-plugins/skills/scvi-tools \
            --skill-dir /data/knowledge-work-plugins/skills/single-cell-rna-qc
    - path: /knowledge-work-plugins/commands/start.md
      text: |
          ---
          description: Set up your bio-research environment and explore available tools
          ---

          # Bio-Research Start

          > If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).

          You are helping a biological researcher get oriented with the bio-research plugin. Walk through the following steps in order.

          ## Step 1: Welcome

          Display this welcome message:

          ```
          Bio-Research Plugin

          Your AI-powered research assistant for the life sciences. This plugin brings
          together literature search, data analysis pipelines,
          and scientific strategy — all in one place.
          ```

          ## Step 2: Check Available MCP Servers

          Test which MCP servers are connected by listing available tools. Group the results:

          **Literature & Data Sources:**
          - ~~literature database — biomedical literature search
          - ~~literature database — preprint access (biology and medicine)
          - ~~journal access — academic publications
          - ~~data repository — collaborative research data (Sage Bionetworks)

          **Drug Discovery & Clinical:**
          - ~~chemical database — bioactive compound database
          - ~~drug target database — drug target discovery platform
          - ClinicalTrials.gov — clinical trial registry
          - ~~clinical data platform — clinical trial site ranking and platform help

          **Visualization & AI:**
          - ~~scientific illustration — create scientific figures and diagrams
          - ~~AI research platform — AI for biology (histopathology, drug discovery)

          Report which servers are connected and which are not yet set up.

          ## Step 3: Survey Available Skills

          List the analysis skills available in this plugin:

          | Skill | What It Does |
          |-------|-------------|
          | **Single-Cell RNA QC** | Quality control for scRNA-seq data with MAD-based filtering |
          | **scvi-tools** | Deep learning for single-cell omics (scVI, scANVI, totalVI, PeakVI, etc.) |
          | **Nextflow Pipelines** | Run nf-core pipelines (RNA-seq, WGS/WES, ATAC-seq) |
          | **Instrument Data Converter** | Convert lab instrument output to Allotrope ASM format |
          | **Scientific Problem Selection** | Systematic framework for choosing research problems |

          ## Step 4: Optional Setup — Binary MCP Servers

          Mention that two additional MCP servers are available as separate installations:

          - **~~genomics platform** — Access cloud analysis data and workflows
            Install: Download `txg-node.mcpb` from https://github.com/10XGenomics/txg-mcp/releases
          - **~~tool database** (Harvard MIMS) — AI tools for scientific discovery
            Install: Download `tooluniverse.mcpb` from https://github.com/mims-harvard/ToolUniverse/releases

          These require downloading binary files and are optional.

          ## Step 5: Ask How to Help

          Ask the researcher what they're working on today. Suggest starting points based on common workflows:

          1. **Literature review** — "Search ~~literature database for recent papers on [topic]"
          2. **Analyze sequencing data** — "Run QC on my single-cell data" or "Set up an RNA-seq pipeline"
          3. **Drug discovery** — "Search ~~chemical database for compounds targeting [protein]" or "Find drug targets for [disease]"
          4. **Data standardization** — "Convert my instrument data to Allotrope format"
          5. **Research strategy** — "Help me evaluate a new project idea"

          Wait for the user's response and guide them to the appropriate tools and skills.
    - path: /knowledge-work-plugins/skills/instrument-data-to-allotrope/SKILL.md
      text: |
          ---
          name: instrument-data-to-allotrope
          description: Convert laboratory instrument output files (PDF, CSV, Excel, TXT) to Allotrope Simple Model (ASM) JSON format or flattened 2D CSV. Use this skill when scientists need to standardize instrument data for LIMS systems, data lakes, or downstream analysis. Supports auto-detection of instrument types. Outputs include full ASM JSON, flattened CSV for easy import, and exportable Python code for data engineers. Common triggers include converting instrument files, standardizing lab data, preparing data for upload to LIMS/ELN systems, or generating parser code for production pipelines.
          ---

          # Instrument Data to Allotrope Converter

          Convert instrument files into standardized Allotrope Simple Model (ASM) format for LIMS upload, data lakes, or handoff to data engineering teams.

          > **Note: This is an Example Skill**
          >
          > This skill demonstrates how skills can support your data engineering tasks—automating schema transformations, parsing instrument outputs, and generating production-ready code.
          >
          > **To customize for your organization:**
          > - Modify the `references/` files to include your company's specific schemas or ontology mappings
          > - Use an MCP server to connect to systems that define your schemas (e.g., your LIMS, data catalog, or schema registry)
          > - Extend the `scripts/` to handle proprietary instrument formats or internal data standards
          >
          > This pattern can be adapted for any data transformation workflow where you need to convert between formats or validate against organizational standards.

          ## Workflow Overview

          1. **Detect instrument type** from file contents (auto-detect or user-specified)
          2. **Parse file** using allotropy library (native) or flexible fallback parser
          3. **Generate outputs**:
             - ASM JSON (full semantic structure)
             - Flattened CSV (2D tabular format)
             - Python parser code (for data engineer handoff)
          4. **Deliver** files with summary and usage instructions

          > **When Uncertain:** If you're unsure how to map a field to ASM (e.g., is this raw data or calculated? device setting or environmental condition?), ask the user for clarification. Refer to `references/field_classification_guide.md` for guidance, but when ambiguity remains, confirm with the user rather than guessing.

          ## Quick Start

          ```python
          # Install requirements first
          pip install allotropy pandas openpyxl pdfplumber --break-system-packages

          # Core conversion
          from allotropy.parser_factory import Vendor
          from allotropy.to_allotrope import allotrope_from_file

          # Convert with allotropy
          asm = allotrope_from_file("instrument_data.csv", Vendor.BECKMAN_VI_CELL_BLU)
          ```

          ## Output Format Selection

          **ASM JSON (default)** - Full semantic structure with ontology URIs
          - Best for: LIMS systems expecting ASM, data lakes, long-term archival
          - Validates against Allotrope schemas

          **Flattened CSV** - 2D tabular representation
          - Best for: Quick analysis, Excel users, systems without JSON support
          - Each measurement becomes one row with metadata repeated

          **Both** - Generate both formats for maximum flexibility

          ## Calculated Data Handling

          **IMPORTANT:** Separate raw measurements from calculated/derived values.

          - **Raw data** → `measurement-document` (direct instrument readings)
          - **Calculated data** → `calculated-data-aggregate-document` (derived values)

          Calculated values MUST include traceability via `data-source-aggregate-document`:

          ```json
          "calculated-data-aggregate-document": {
            "calculated-data-document": [{
              "calculated-data-identifier": "SAMPLE_B1_DIN_001",
              "calculated-data-name": "DNA integrity number",
              "calculated-result": {"value": 9.5, "unit": "(unitless)"},
              "data-source-aggregate-document": {
                "data-source-document": [{
                  "data-source-identifier": "SAMPLE_B1_MEASUREMENT",
                  "data-source-feature": "electrophoresis trace"
                }]
              }
            }]
          }
          ```

          **Common calculated fields by instrument type:**
          | Instrument | Calculated Fields |
          |------------|-------------------|
          | Cell counter | Viability %, cell density dilution-adjusted values |
          | Spectrophotometer | Concentration (from absorbance), 260/280 ratio |
          | Plate reader | Concentrations from standard curve, %CV |
          | Electrophoresis | DIN/RIN, region concentrations, average sizes |
          | qPCR | Relative quantities, fold change |

          See `references/field_classification_guide.md` for detailed guidance on raw vs. calculated classification.

          ## Validation

          Always validate ASM output before delivering to the user:

          ```bash
          python scripts/validate_asm.py output.json
          python scripts/validate_asm.py output.json --reference known_good.json  # Compare to reference
          python scripts/validate_asm.py output.json --strict  # Treat warnings as errors
          ```

          **Validation Rules:**
          - Based on Allotrope ASM specification (December 2024)
          - Last updated: 2026-01-07
          - Source: https://gitlab.com/allotrope-public/asm

          **Soft Validation Approach:**
          Unknown techniques, units, or sample roles generate **warnings** (not errors) to allow for forward compatibility. If Allotrope adds new values after December 2024, the validator won't block them—it will flag them for manual verification. Use `--strict` mode to treat warnings as errors if you need stricter validation.

          **What it checks:**
          - Correct technique selection (e.g., multi-analyte profiling vs plate reader)
          - Field naming conventions (space-separated, not hyphenated)
          - Calculated data has traceability (`data-source-aggregate-document`)
          - Unique identifiers exist for measurements and calculated values
          - Required metadata present
          - Valid units and sample roles (with soft validation for unknown values)

          ## Supported Instruments

          See `references/supported_instruments.md` for complete list. Key instruments:

          | Category | Instruments |
          |----------|-------------|
          | Cell Counting | Vi-CELL BLU, Vi-CELL XR, NucleoCounter |
          | Spectrophotometry | NanoDrop One/Eight/8000, Lunatic |
          | Plate Readers | SoftMax Pro, EnVision, Gen5, CLARIOstar |
          | ELISA | SoftMax Pro, BMG MARS, MSD Workbench |
          | qPCR | QuantStudio, Bio-Rad CFX |
          | Chromatography | Empower, Chromeleon |

          ## Detection & Parsing Strategy

          ### Tier 1: Native allotropy parsing (PREFERRED)
          **Always try allotropy first.** Check available vendors directly:

          ```python
          from allotropy.parser_factory import Vendor

          # List all supported vendors
          for v in Vendor:
              print(f"{v.name}")

          # Common vendors:
          # AGILENT_TAPESTATION_ANALYSIS  (for TapeStation XML)
          # BECKMAN_VI_CELL_BLU
          # THERMO_FISHER_NANODROP_EIGHT
          # MOLDEV_SOFTMAX_PRO
          # APPBIO_QUANTSTUDIO
          # ... many more
          ```

          **When the user provides a file, check if allotropy supports it before falling back to manual parsing.** The `scripts/convert_to_asm.py` auto-detection only covers a subset of allotropy vendors.

          ### Tier 2: Flexible fallback parsing
          **Only use if allotropy doesn't support the instrument.** This fallback:
          - Does NOT generate `calculated-data-aggregate-document`
          - Does NOT include full traceability
          - Produces simplified ASM structure

          Use flexible parser with:
          - Column name fuzzy matching
          - Unit extraction from headers
          - Metadata extraction from file structure

          ### Tier 3: PDF extraction
          For PDF-only files, extract tables using pdfplumber, then apply Tier 2 parsing.

          ## Pre-Parsing Checklist

          Before writing a custom parser, ALWAYS:

          1. **Check if allotropy supports it** - Use native parser if available
          2. **Find a reference ASM file** - Check `references/examples/` or ask user
          3. **Review instrument-specific guide** - Check `references/instrument_guides/`
          4. **Validate against reference** - Run `validate_asm.py --reference <file>`

          ## Common Mistakes to Avoid

          | Mistake | Correct Approach |
          |---------|------------------|
          | Manifest as object | Use URL string |
          | Lowercase detection types | Use "Absorbance" not "absorbance" |
          | "emission wavelength setting" | Use "detector wavelength setting" for emission |
          | All measurements in one document | Group by well/sample location |
          | Missing procedure metadata | Extract ALL device settings per measurement |

          ## Code Export for Data Engineers

          Generate standalone Python scripts that scientists can hand off:

          ```python
          # Export parser code
          python scripts/export_parser.py --input "data.csv" --vendor "VI_CELL_BLU" --output "parser_script.py"
          ```

          The exported script:
          - Has no external dependencies beyond pandas/allotropy
          - Includes inline documentation
          - Can run in Jupyter notebooks
          - Is production-ready for data pipelines

          ## File Structure

          ```
          instrument-data-to-allotrope/
          ├── SKILL.md                          # This file
          ├── scripts/
          │   ├── convert_to_asm.py            # Main conversion script
          │   ├── flatten_asm.py               # ASM → 2D CSV conversion
          │   ├── export_parser.py             # Generate standalone parser code
          │   └── validate_asm.py              # Validate ASM output quality
          └── references/
              ├── supported_instruments.md     # Full instrument list with Vendor enums
              ├── asm_schema_overview.md       # ASM structure reference
              ├── field_classification_guide.md # Where to put different field types
              └── flattening_guide.md          # How flattening works
          ```

          ## Usage Examples

          ### Example 1: Vi-CELL BLU file
          ```
          User: "Convert this cell counting data to Allotrope format"
          [uploads viCell_Results.xlsx]

          Claude:
          1. Detects Vi-CELL BLU (95% confidence)
          2. Converts using allotropy native parser
          3. Outputs:
             - viCell_Results_asm.json (full ASM)
             - viCell_Results_flat.csv (2D format)
             - viCell_parser.py (exportable code)
          ```

          ### Example 2: Request for code handoff
          ```
          User: "I need to give our data engineer code to parse NanoDrop files"

          Claude:
          1. Generates self-contained Python script
          2. Includes sample input/output
          3. Documents all assumptions
          4. Provides Jupyter notebook version
          ```

          ### Example 3: LIMS-ready flattened output
          ```
          User: "Convert this ELISA data to a CSV I can upload to our LIMS"

          Claude:
          1. Parses plate reader data
          2. Generates flattened CSV with columns:
             - sample_identifier, well_position, measurement_value, measurement_unit
             - instrument_serial_number, analysis_datetime, assay_type
          3. Validates against common LIMS import requirements
          ```

          ## Implementation Notes

          ### Installing allotropy
          ```bash
          pip install allotropy --break-system-packages
          ```

          ### Handling parse failures
          If allotropy native parsing fails:
          1. Log the error for debugging
          2. Fall back to flexible parser
          3. Report reduced metadata completeness to user
          4. Suggest exporting different format from instrument

          ### ASM Schema Validation
          Validate output against Allotrope schemas when available:
          ```python
          import jsonschema
          # Schema URLs in references/asm_schema_overview.md
          ```
    - path: /knowledge-work-plugins/skills/nextflow-development/SKILL.md
      text: |
          ---
          name: nextflow-development
          description: Run nf-core bioinformatics pipelines (rnaseq, sarek, atacseq) on sequencing data. Use when analyzing RNA-seq, WGS/WES, or ATAC-seq data—either local FASTQs or public datasets from GEO/SRA. Triggers on nf-core, Nextflow, FASTQ analysis, variant calling, gene expression, differential expression, GEO reanalysis, GSE/GSM/SRR accessions, or samplesheet creation.
          ---

          # nf-core Pipeline Deployment

          Run nf-core bioinformatics pipelines on local or public sequencing data.

          **Target users:** Bench scientists and researchers without specialized bioinformatics training who need to run large-scale omics analyses—differential expression, variant calling, or chromatin accessibility analysis.

          ## Workflow Checklist

          ```
          - [ ] Step 0: Acquire data (if from GEO/SRA)
          - [ ] Step 1: Environment check (MUST pass)
          - [ ] Step 2: Select pipeline (confirm with user)
          - [ ] Step 3: Run test profile (MUST pass)
          - [ ] Step 4: Create samplesheet
          - [ ] Step 5: Configure & run (confirm genome with user)
          - [ ] Step 6: Verify outputs
          ```

          ---

          ## Step 0: Acquire Data (GEO/SRA Only)

          **Skip this step if user has local FASTQ files.**

          For public datasets, fetch from GEO/SRA first. See [references/geo-sra-acquisition.md](references/geo-sra-acquisition.md) for the full workflow.

          **Quick start:**

          ```bash
          # 1. Get study info
          python scripts/sra_geo_fetch.py info GSE110004

          # 2. Download (interactive mode)
          python scripts/sra_geo_fetch.py download GSE110004 -o ./fastq -i

          # 3. Generate samplesheet
          python scripts/sra_geo_fetch.py samplesheet GSE110004 --fastq-dir ./fastq -o samplesheet.csv
          ```

          **DECISION POINT:** After fetching study info, confirm with user:
          - Which sample subset to download (if multiple data types)
          - Suggested genome and pipeline

          Then continue to Step 1.

          ---

          ## Step 1: Environment Check

          **Run first. Pipeline will fail without passing environment.**

          ```bash
          python scripts/check_environment.py
          ```

          All critical checks must pass. If any fail, provide fix instructions:

          ### Docker issues

          | Problem | Fix |
          |---------|-----|
          | Not installed | Install from https://docs.docker.com/get-docker/ |
          | Permission denied | `sudo usermod -aG docker $USER` then re-login |
          | Daemon not running | `sudo systemctl start docker` |

          ### Nextflow issues

          | Problem | Fix |
          |---------|-----|
          | Not installed | `curl -s https://get.nextflow.io \| bash && mv nextflow ~/bin/` |
          | Version < 23.04 | `nextflow self-update` |

          ### Java issues

          | Problem | Fix |
          |---------|-----|
          | Not installed / < 11 | `sudo apt install openjdk-11-jdk` |

          **Do not proceed until all checks pass.** For HPC/Singularity, see [references/troubleshooting.md](references/troubleshooting.md).

          ---

          ## Step 2: Select Pipeline

          **DECISION POINT: Confirm with user before proceeding.**

          | Data Type | Pipeline | Version | Goal |
          |-----------|----------|---------|------|
          | RNA-seq | `rnaseq` | 3.22.2 | Gene expression |
          | WGS/WES | `sarek` | 3.7.1 | Variant calling |
          | ATAC-seq | `atacseq` | 2.1.2 | Chromatin accessibility |

          Auto-detect from data:
          ```bash
          python scripts/detect_data_type.py /path/to/data
          ```

          For pipeline-specific details:
          - [references/pipelines/rnaseq.md](references/pipelines/rnaseq.md)
          - [references/pipelines/sarek.md](references/pipelines/sarek.md)
          - [references/pipelines/atacseq.md](references/pipelines/atacseq.md)

          ---

          ## Step 3: Run Test Profile

          **Validates environment with small data. MUST pass before real data.**

          ```bash
          nextflow run nf-core/<pipeline> -r <version> -profile test,docker --outdir test_output
          ```

          | Pipeline | Command |
          |----------|---------|
          | rnaseq | `nextflow run nf-core/rnaseq -r 3.22.2 -profile test,docker --outdir test_rnaseq` |
          | sarek | `nextflow run nf-core/sarek -r 3.7.1 -profile test,docker --outdir test_sarek` |
          | atacseq | `nextflow run nf-core/atacseq -r 2.1.2 -profile test,docker --outdir test_atacseq` |

          Verify:
          ```bash
          ls test_output/multiqc/multiqc_report.html
          grep "Pipeline completed successfully" .nextflow.log
          ```

          If test fails, see [references/troubleshooting.md](references/troubleshooting.md).

          ---

          ## Step 4: Create Samplesheet

          ### Generate automatically

          ```bash
          python scripts/generate_samplesheet.py /path/to/data <pipeline> -o samplesheet.csv
          ```

          The script:
          - Discovers FASTQ/BAM/CRAM files
          - Pairs R1/R2 reads
          - Infers sample metadata
          - Validates before writing

          **For sarek:** Script prompts for tumor/normal status if not auto-detected.

          ### Validate existing samplesheet

          ```bash
          python scripts/generate_samplesheet.py --validate samplesheet.csv <pipeline>
          ```

          ### Samplesheet formats

          **rnaseq:**
          ```csv
          sample,fastq_1,fastq_2,strandedness
          SAMPLE1,/abs/path/R1.fq.gz,/abs/path/R2.fq.gz,auto
          ```

          **sarek:**
          ```csv
          patient,sample,lane,fastq_1,fastq_2,status
          patient1,tumor,L001,/abs/path/tumor_R1.fq.gz,/abs/path/tumor_R2.fq.gz,1
          patient1,normal,L001,/abs/path/normal_R1.fq.gz,/abs/path/normal_R2.fq.gz,0
          ```

          **atacseq:**
          ```csv
          sample,fastq_1,fastq_2,replicate
          CONTROL,/abs/path/ctrl_R1.fq.gz,/abs/path/ctrl_R2.fq.gz,1
          ```

          ---

          ## Step 5: Configure & Run

          ### 5a. Check genome availability

          ```bash
          python scripts/manage_genomes.py check <genome>
          # If not installed:
          python scripts/manage_genomes.py download <genome>
          ```

          Common genomes: GRCh38 (human), GRCh37 (legacy), GRCm39 (mouse), R64-1-1 (yeast), BDGP6 (fly)

          ### 5b. Decision points

          **DECISION POINT: Confirm with user:**

          1. **Genome:** Which reference to use
          2. **Pipeline-specific options:**
             - **rnaseq:** aligner (star_salmon recommended, hisat2 for low memory)
             - **sarek:** tools (haplotypecaller for germline, mutect2 for somatic)
             - **atacseq:** read_length (50, 75, 100, or 150)

          ### 5c. Run pipeline

          ```bash
          nextflow run nf-core/<pipeline> \
              -r <version> \
              -profile docker \
              --input samplesheet.csv \
              --outdir results \
              --genome <genome> \
              -resume
          ```

          **Key flags:**
          - `-r`: Pin version
          - `-profile docker`: Use Docker (or `singularity` for HPC)
          - `--genome`: iGenomes key
          - `-resume`: Continue from checkpoint

          **Resource limits (if needed):**
          ```bash
          --max_cpus 8 --max_memory '32.GB' --max_time '24.h'
          ```

          ---

          ## Step 6: Verify Outputs

          ### Check completion

          ```bash
          ls results/multiqc/multiqc_report.html
          grep "Pipeline completed successfully" .nextflow.log
          ```

          ### Key outputs by pipeline

          **rnaseq:**
          - `results/star_salmon/salmon.merged.gene_counts.tsv` - Gene counts
          - `results/star_salmon/salmon.merged.gene_tpm.tsv` - TPM values

          **sarek:**
          - `results/variant_calling/*/` - VCF files
          - `results/preprocessing/recalibrated/` - BAM files

          **atacseq:**
          - `results/macs2/narrowPeak/` - Peak calls
          - `results/bwa/mergedLibrary/bigwig/` - Coverage tracks

          ---

          ## Quick Reference

          For common exit codes and fixes, see [references/troubleshooting.md](references/troubleshooting.md).

          ### Resume failed run

          ```bash
          nextflow run nf-core/<pipeline> -resume
          ```

          ---

          ## References

          - [references/geo-sra-acquisition.md](references/geo-sra-acquisition.md) - Downloading public GEO/SRA data
          - [references/troubleshooting.md](references/troubleshooting.md) - Common issues and fixes
          - [references/installation.md](references/installation.md) - Environment setup
          - [references/pipelines/rnaseq.md](references/pipelines/rnaseq.md) - RNA-seq pipeline details
          - [references/pipelines/sarek.md](references/pipelines/sarek.md) - Variant calling details
          - [references/pipelines/atacseq.md](references/pipelines/atacseq.md) - ATAC-seq details

          ---

          ## Disclaimer

          This skill is provided as a prototype example demonstrating how to integrate nf-core bioinformatics pipelines into Claude Code for automated analysis workflows. The current implementation supports three pipelines (rnaseq, sarek, and atacseq), serving as a foundation that enables the community to expand support to the full set of nf-core pipelines.

          It is intended for educational and research purposes and should not be considered production-ready without appropriate validation for your specific use case. Users are responsible for ensuring their computing environment meets pipeline requirements and for verifying analysis results.

          Anthropic does not guarantee the accuracy of bioinformatics outputs, and users should follow standard practices for validating computational analyses. This integration is not officially endorsed by or affiliated with the nf-core community.

          ## Attribution

          When publishing results, cite the appropriate pipeline. Citations are available in each nf-core repository's CITATIONS.md file (e.g., https://github.com/nf-core/rnaseq/blob/3.22.2/CITATIONS.md).

          ## Licenses

          - **nf-core pipelines:** MIT License (https://nf-co.re/about)
          - **Nextflow:** Apache License, Version 2.0 (https://www.nextflow.io/about-us.html)
          - **NCBI SRA Toolkit:** Public Domain (https://github.com/ncbi/sra-tools/blob/master/LICENSE)
    - path: /knowledge-work-plugins/skills/scientific-problem-selection/SKILL.md
      text: |
          ---
          name: scientific-problem-selection
          description: This skill should be used when scientists need help with research problem selection, project ideation, troubleshooting stuck projects, or strategic scientific decisions. Use this skill when users ask to pitch a new research idea, work through a project problem, evaluate project risks, plan research strategy, navigate decision trees, or get help choosing what scientific problem to work on. Typical requests include "I have an idea for a project", "I'm stuck on my research", "help me evaluate this project", "what should I work on", or "I need strategic advice about my research".
          ---

          # Scientific Problem Selection Skills

          A conversational framework for systematic scientific problem selection based on Fischbach & Walsh's "Problem choice and decision trees in science and engineering" (Cell, 2024).

          ## Getting Started

          Present users with three entry points:

          **1) Pitch an idea for a new project** — to work it up together

          **2) Share a problem in a current project** — to troubleshoot together

          **3) Ask a strategic question** — to navigate the decision tree together

          This conversational entry meets scientists where they are and establishes a collaborative tone.

          ---

          ## Option 1: Pitch an Idea

          ### Initial Prompt
          Ask: **"Tell me the short version of your idea (1-2 sentences)."**

          ### Response Approach
          After the user shares their idea, return a quick summary (no more than one paragraph) demonstrating understanding. Note the general area of research and rephrase the idea in a way that highlights its kernel—showing alignment and readiness to dive into details.

          ### Follow-up Prompt
          Then ask for more detail: "Now give me a bit more detail. You might include, however briefly or even say where you are unsure:
          1. What exactly you want to do
          2. How you currently plan to do it
          3. If it works, why will it be a big deal
          4. What you think are the major risks"

          ### Workflow
          From there, guide the user through the early stages of problem selection and evaluation:
          - **Skill 1: Intuition Pumps** - Refine and strengthen the idea
          - **Skill 2: Risk Assessment** - Identify and manage project risks
          - **Skill 3: Optimization Function** - Define success metrics
          - **Skill 4: Parameter Strategy** - Determine what to fix vs. keep flexible

          See `references/01-intuition-pumps.md`, `references/02-risk-assessment.md`, `references/03-optimization-function.md`, and `references/04-parameter-strategy.md` for detailed guidance.

          ---

          ## Option 2: Troubleshoot a Problem

          ### Initial Prompt
          Ask: **"Tell me a short version of your problem (1-2 sentences or whatever is easy)."**

          ### Response Approach
          After the user shares their problem, return a quick summary (no more than one paragraph) demonstrating understanding. Note the context of the project where the problem occurred and rephrase the problem—highlighting its core essence—so the user knows the situation is understood. Also raise additional questions that seem important to discuss.

          ### Follow-up Prompt
          Then ask: "Now give me a bit more detail. You might include, however briefly:
          1. The overall goal of your project (if we have not talked about it before)
          2. What exactly went wrong
          3. Your current ideas for fixing it"

          ### Workflow
          From there, guide the user through troubleshooting and decision tree navigation:
          - **Skill 5: Decision Tree Navigation** - Plan decision points and navigate between execution and strategic thinking
          - **Skill 4: Parameter Strategy** - Fix one parameter at a time, let others float
          - **Skill 6: Adversity Response** - Frame problems as opportunities for growth
          - **Skill 7: Problem Inversion** - Strategies for navigating around obstacles

          Always include workarounds that might be useful whether or not the problem can be fixed easily.

          See `references/05-decision-tree.md`, `references/06-adversity-planning.md`, `references/07-problem-inversion.md`, and `references/04-parameter-strategy.md` for detailed guidance.

          ---

          ## Option 3: Ask a Strategic Question

          ### Initial Prompt
          Ask: **"Tell me the short version of your question (1-2 sentences)."**

          ### Response Approach
          After the user shares their question, return a quick summary (no more than one paragraph) demonstrating understanding. Note the broader context and rephrase the question—highlighting its crux—to confirm alignment with their thinking.

          ### Follow-up Prompt
          Then ask: "Now give me a bit more detail. You might include, however briefly:
          1. The setting (i.e., is this about a current or future project)
          2. A bit more detail about what you're thinking"

          ### Workflow
          From there, draw on the specific modules from the problem choice framework most appropriate to the question:
          - **Skills 1-4** for future project planning (ideation, risk, optimization, parameters)
          - **Skills 5-7** for current project navigation (decision trees, adversity, inversion)
          - **Skill 8** for communication and synthesis
          - **Skill 9** for comprehensive workflow orchestration

          See the complete reference materials in the `references/` folder.

          ---

          ## Core Framework Concepts

          ### The Central Insight
          **Problem Choice >> Execution Quality**

          Even brilliant execution of a mediocre problem yields incremental impact. Good execution of an important problem yields substantial impact.

          ### The Time Paradox
          Scientists typically spend:
          - **Days** choosing a problem
          - **Years** solving it

          This imbalance limits impact. These skills help invest more time choosing wisely.

          ### Evaluation Axes
          **For Evaluating Ideas:**
          - **X-axis:** Likelihood of success
          - **Y-axis:** Impact if successful

          Skills help move ideas rightward (more feasible) and upward (more impactful).

          ### The Risk Paradox
          - Don't avoid risk—befriend it
          - No risk = incremental work
          - But: Multiple miracles = avoid or refine
          - **Balance:** Understood, quantified, manageable risk

          ### The Parameter Paradox
          - Too many fixed = brittleness
          - Too few fixed = paralysis
          - **Sweet spot:** Fix ONE meaningful constraint

          ### The Adversity Principle
          - Crises are inevitable (don't be surprised)
          - Crises are opportune (don't waste them)
          - **Strategy:** Fix problem AND upgrade project simultaneously

          ---

          ## The 9 Skills Overview

          | Skill | Purpose | Output | Time |
          |-------|---------|--------|------|
          | 1. Intuition Pumps | Generate high-quality research ideas | Problem Ideation Document | ~1 week |
          | 2. Risk Assessment | Identify and manage project risks | Risk Assessment Matrix | 3-5 days |
          | 3. Optimization Function | Define success metrics | Impact Assessment Document | 2-3 days |
          | 4. Parameter Strategy | Decide what to fix vs. keep flexible | Parameter Strategy Document | 2-3 days |
          | 5. Decision Tree Navigation | Plan decision points and altitude dance | Decision Tree Map | 2 days |
          | 6. Adversity Response | Prepare for crises as opportunities | Adversity Playbook | 2 days |
          | 7. Problem Inversion | Navigate around obstacles | Problem Inversion Analysis | 1 day |
          | 8. Integration & Synthesis | Synthesize into coherent plan | Project Communication Package | 3-5 days |
          | 9. Meta-Framework | Orchestrate complete workflow | Complete Project Package | 1-6 weeks |

          ---

          ## Skill Workflow

          ```
          SKILL 1: Intuition Pumps
                   | (generates idea)
                   v
          SKILL 2: Risk Assessment
                   | (evaluates feasibility)
                   v
          SKILL 3: Optimization Function
                   | (defines success metrics)
                   v
          SKILL 4: Parameter Strategy
                   | (determines flexibility)
                   v
          SKILL 5: Decision Tree
                   | (plans execution and evaluation)
                   v
          SKILL 6: Adversity Planning
                   | (prepares for failure modes)
                   v
          SKILL 7: Problem Inversion
                   | (provides pivot strategies)
                   v
          SKILL 8: Integration & Communication
                   | (synthesizes into coherent plan)
                   v
          SKILL 9: Meta-Skill
                   (orchestrates complete workflow)
          ```

          ---

          ## Key Design Principles

          1. **Conversational Entry** - Meet users where they are with three clear starting points
          2. **Thoughtful Interaction** - Ask clarifying questions; low confidence prompts additional input
          3. **Literature Integration** - Use PubMed searches at strategic points for validation
          4. **Concrete Outputs** - Every skill produces tangible 1-2 page documents
          5. **Building Specificity** - Progressive detail emerges through targeted questions
          6. **Flexibility** - Skills work independently, sequentially, or iteratively
          7. **Scientific Rigor** - Claims about generality and feasibility should be evidence-based

          ---

          ## Who Should Use These Skills

          ### Graduate Students (Primary Audience)
          - **When:** Choosing thesis projects, qualifying exams, committee meetings
          - **Focus:** Skills 1-3 (ideation, risk, impact) + Skill 9 (complete workflow)
          - **Timeline:** 2-4 weeks for comprehensive planning

          ### Postdocs
          - **When:** Starting new position, planning independent projects, fellowship applications
          - **Focus:** All skills, emphasizing independence and risk management
          - **Timeline:** 1-2 weeks intensive planning

          ### Principal Investigators
          - **When:** New lab, new direction, mentoring trainees, grant cycles
          - **Focus:** Skills 1, 3, 4, 6 (ideation, impact, parameters, adversity)
          - **Timeline:** Ongoing, integrate into lab culture

          ### Startup Founders
          - **When:** Company inception, pivot decisions, investor pitches
          - **Focus:** Skills 1-4 (ideation through parameters) + Skill 8 (communication)
          - **Timeline:** 1-2 weeks for initial planning, revisit quarterly

          ---

          ## Reference Materials

          Detailed skill documentation is available in the `references/` folder:

          | File | Content | Search Patterns |
          |------|---------|-----------------|
          | `01-intuition-pumps.md` | Generate research ideas | `Intuition Pump #`, `Trap #`, `Phase [0-9]` |
          | `02-risk-assessment.md` | Risk identification | `Risk.*1-5`, `go/no-go`, `assumption` |
          | `03-optimization-function.md` | Success metrics | `Generality.*Learning`, `optimization`, `impact` |
          | `04-parameter-strategy.md` | Parameter fixation | `fixed.*float`, `constraint`, `parameter` |
          | `05-decision-tree.md` | Decision tree navigation | `altitude`, `Level [0-9]`, `decision` |
          | `06-adversity-planning.md` | Adversity response | `adversity`, `crisis`, `ensemble` |
          | `07-problem-inversion.md` | Problem inversion strategies | `Strategy [0-9]`, `inversion`, `goal` |
          | `08-integration-synthesis.md` | Integration and synthesis | `narrative`, `communication`, `story` |
          | `09-meta-framework.md` | Complete workflow | `Phase`, `workflow`, `orchestrat` |

          ---

          ## Expected Outcomes

          ### Immediate (After Completing Workflow)
          - Clear project vision
          - Honest risk assessment
          - Contingency plans
          - Communication materials ready
          - Confidence in problem choice

          ### 6-Month
          - Faster decisions (have framework)
          - Productive adversity handling
          - No existential crises (risks mitigated)

          ### 2-Year
          - Published results or strong progress
          - Avoided dead-end projects
          - Career aligned with goals
          - **Time well-spent** (ultimate measure)

          ---

          ## Foundational Reference

          **Fischbach, M.A., & Walsh, C.T. (2024).** "Problem choice and decision trees in science and engineering." *Cell*, 187, 1828-1833.

          Based on course BIOE 395 taught at Stanford University.
    - path: /knowledge-work-plugins/skills/scvi-tools/SKILL.md
      text: |
          ---
          name: scvi-tools
          description: Deep learning for single-cell analysis using scvi-tools. This skill should be used when users need (1) data integration and batch correction with scVI/scANVI, (2) ATAC-seq analysis with PeakVI, (3) CITE-seq multi-modal analysis with totalVI, (4) multiome RNA+ATAC analysis with MultiVI, (5) spatial transcriptomics deconvolution with DestVI, (6) label transfer and reference mapping with scANVI/scArches, (7) RNA velocity with veloVI, or (8) any deep learning-based single-cell method. Triggers include mentions of scVI, scANVI, totalVI, PeakVI, MultiVI, DestVI, veloVI, sysVI, scArches, variational autoencoder, VAE, batch correction, data integration, multi-modal, CITE-seq, multiome, reference mapping, latent space.
          ---

          # scvi-tools Deep Learning Skill

          This skill provides guidance for deep learning-based single-cell analysis using scvi-tools, the leading framework for probabilistic models in single-cell genomics.

          ## How to Use This Skill

          1. Identify the appropriate workflow from the model/workflow tables below
          2. Read the corresponding reference file for detailed steps and code
          3. Use scripts in `scripts/` to avoid rewriting common code
          4. For installation or GPU issues, consult `references/environment_setup.md`
          5. For debugging, consult `references/troubleshooting.md`

          ## When to Use This Skill

          - When scvi-tools, scVI, scANVI, or related models are mentioned
          - When deep learning-based batch correction or integration is needed
          - When working with multi-modal data (CITE-seq, multiome)
          - When reference mapping or label transfer is required
          - When analyzing ATAC-seq or spatial transcriptomics data
          - When learning latent representations of single-cell data

          ## Model Selection Guide

          | Data Type | Model | Primary Use Case |
          |-----------|-------|------------------|
          | scRNA-seq | **scVI** | Unsupervised integration, DE, imputation |
          | scRNA-seq + labels | **scANVI** | Label transfer, semi-supervised integration |
          | CITE-seq (RNA+protein) | **totalVI** | Multi-modal integration, protein denoising |
          | scATAC-seq | **PeakVI** | Chromatin accessibility analysis |
          | Multiome (RNA+ATAC) | **MultiVI** | Joint modality analysis |
          | Spatial + scRNA reference | **DestVI** | Cell type deconvolution |
          | RNA velocity | **veloVI** | Transcriptional dynamics |
          | Cross-technology | **sysVI** | System-level batch correction |

          ## Workflow Reference Files

          | Workflow | Reference File | Description |
          |----------|---------------|-------------|
          | Environment Setup | `references/environment_setup.md` | Installation, GPU, version info |
          | Data Preparation | `references/data_preparation.md` | Formatting data for any model |
          | scRNA Integration | `references/scrna_integration.md` | scVI/scANVI batch correction |
          | ATAC-seq Analysis | `references/atac_peakvi.md` | PeakVI for accessibility |
          | CITE-seq Analysis | `references/citeseq_totalvi.md` | totalVI for protein+RNA |
          | Multiome Analysis | `references/multiome_multivi.md` | MultiVI for RNA+ATAC |
          | Spatial Deconvolution | `references/spatial_deconvolution.md` | DestVI spatial analysis |
          | Label Transfer | `references/label_transfer.md` | scANVI reference mapping |
          | scArches Mapping | `references/scarches_mapping.md` | Query-to-reference mapping |
          | Batch Correction | `references/batch_correction_sysvi.md` | Advanced batch methods |
          | RNA Velocity | `references/rna_velocity_velovi.md` | veloVI dynamics |
          | Troubleshooting | `references/troubleshooting.md` | Common issues and solutions |

          ## CLI Scripts

          Modular scripts for common workflows. Chain together or modify as needed.

          ### Pipeline Scripts

          | Script | Purpose | Usage |
          |--------|---------|-------|
          | `prepare_data.py` | QC, filter, HVG selection | `python scripts/prepare_data.py raw.h5ad prepared.h5ad --batch-key batch` |
          | `train_model.py` | Train any scvi-tools model | `python scripts/train_model.py prepared.h5ad results/ --model scvi` |
          | `cluster_embed.py` | Neighbors, UMAP, Leiden | `python scripts/cluster_embed.py adata.h5ad results/` |
          | `differential_expression.py` | DE analysis | `python scripts/differential_expression.py model/ adata.h5ad de.csv --groupby leiden` |
          | `transfer_labels.py` | Label transfer with scANVI | `python scripts/transfer_labels.py ref_model/ query.h5ad results/` |
          | `integrate_datasets.py` | Multi-dataset integration | `python scripts/integrate_datasets.py results/ data1.h5ad data2.h5ad` |
          | `validate_adata.py` | Check data compatibility | `python scripts/validate_adata.py data.h5ad --batch-key batch` |

          ### Example Workflow

          ```bash
          # 1. Validate input data
          python scripts/validate_adata.py raw.h5ad --batch-key batch --suggest

          # 2. Prepare data (QC, HVG selection)
          python scripts/prepare_data.py raw.h5ad prepared.h5ad --batch-key batch --n-hvgs 2000

          # 3. Train model
          python scripts/train_model.py prepared.h5ad results/ --model scvi --batch-key batch

          # 4. Cluster and visualize
          python scripts/cluster_embed.py results/adata_trained.h5ad results/ --resolution 0.8

          # 5. Differential expression
          python scripts/differential_expression.py results/model results/adata_clustered.h5ad results/de.csv --groupby leiden
          ```

          ### Python Utilities

          The `scripts/model_utils.py` provides importable functions for custom workflows:

          | Function | Purpose |
          |----------|---------|
          | `prepare_adata()` | Data preparation (QC, HVG, layer setup) |
          | `train_scvi()` | Train scVI or scANVI |
          | `evaluate_integration()` | Compute integration metrics |
          | `get_marker_genes()` | Extract DE markers |
          | `save_results()` | Save model, data, plots |
          | `auto_select_model()` | Suggest best model |
          | `quick_clustering()` | Neighbors + UMAP + Leiden |

          ## Critical Requirements

          1. **Raw counts required**: scvi-tools models require integer count data
             ```python
             adata.layers["counts"] = adata.X.copy()  # Before normalization
             scvi.model.SCVI.setup_anndata(adata, layer="counts")
             ```

          2. **HVG selection**: Use 2000-4000 highly variable genes
             ```python
             sc.pp.highly_variable_genes(adata, n_top_genes=2000, batch_key="batch", layer="counts", flavor="seurat_v3")
             adata = adata[:, adata.var['highly_variable']].copy()
             ```

          3. **Batch information**: Specify batch_key for integration
             ```python
             scvi.model.SCVI.setup_anndata(adata, layer="counts", batch_key="batch")
             ```

          ## Quick Decision Tree

          ```
          Need to integrate scRNA-seq data?
          ├── Have cell type labels? → scANVI (references/label_transfer.md)
          └── No labels? → scVI (references/scrna_integration.md)

          Have multi-modal data?
          ├── CITE-seq (RNA + protein)? → totalVI (references/citeseq_totalvi.md)
          ├── Multiome (RNA + ATAC)? → MultiVI (references/multiome_multivi.md)
          └── scATAC-seq only? → PeakVI (references/atac_peakvi.md)

          Have spatial data?
          └── Need cell type deconvolution? → DestVI (references/spatial_deconvolution.md)

          Have pre-trained reference model?
          └── Map query to reference? → scArches (references/scarches_mapping.md)

          Need RNA velocity?
          └── veloVI (references/rna_velocity_velovi.md)

          Strong cross-technology batch effects?
          └── sysVI (references/batch_correction_sysvi.md)
          ```

          ## Key Resources

          - [scvi-tools Documentation](https://docs.scvi-tools.org/)
          - [scvi-tools Tutorials](https://docs.scvi-tools.org/en/stable/tutorials/index.html)
          - [Model Hub](https://huggingface.co/scvi-tools)
          - [GitHub Issues](https://github.com/scverse/scvi-tools/issues)
    - path: /knowledge-work-plugins/skills/single-cell-rna-qc/SKILL.md
      text: |
          ---
          name: single-cell-rna-qc
          description: Performs quality control on single-cell RNA-seq data (.h5ad or .h5 files) using scverse best practices with MAD-based filtering and comprehensive visualizations. Use when users request QC analysis, filtering low-quality cells, assessing data quality, or following scverse/scanpy best practices for single-cell analysis.
          ---

          # Single-Cell RNA-seq Quality Control

          Automated QC workflow for single-cell RNA-seq data following scverse best practices.

          ## When to Use This Skill

          Use when users:
          - Request quality control or QC on single-cell RNA-seq data
          - Want to filter low-quality cells or assess data quality
          - Need QC visualizations or metrics
          - Ask to follow scverse/scanpy best practices
          - Request MAD-based filtering or outlier detection

          **Supported input formats:**
          - `.h5ad` files (AnnData format from scanpy/Python workflows)
          - `.h5` files (10X Genomics Cell Ranger output)

          **Default recommendation**: Use Approach 1 (complete pipeline) unless the user has specific custom requirements or explicitly requests non-standard filtering logic.

          ## Approach 1: Complete QC Pipeline (Recommended for Standard Workflows)

          For standard QC following scverse best practices, use the convenience script `scripts/qc_analysis.py`:

          ```bash
          python3 scripts/qc_analysis.py input.h5ad
          # or for 10X Genomics .h5 files:
          python3 scripts/qc_analysis.py raw_feature_bc_matrix.h5
          ```

          The script automatically detects the file format and loads it appropriately.

          **When to use this approach:**
          - Standard QC workflow with adjustable thresholds (all cells filtered the same way)
          - Batch processing multiple datasets
          - Quick exploratory analysis
          - User wants the "just works" solution

          **Requirements:** anndata, scanpy, scipy, matplotlib, seaborn, numpy

          **Parameters:**

          Customize filtering thresholds and gene patterns using command-line parameters:
          - `--output-dir` - Output directory
          - `--mad-counts`, `--mad-genes`, `--mad-mt` - MAD thresholds for counts/genes/MT%
          - `--mt-threshold` - Hard mitochondrial % cutoff
          - `--min-cells` - Gene filtering threshold
          - `--mt-pattern`, `--ribo-pattern`, `--hb-pattern` - Gene name patterns for different species

          Use `--help` to see current default values.

          **Outputs:**

          All files are saved to `<input_basename>_qc_results/` directory by default (or to the directory specified by `--output-dir`):
          - `qc_metrics_before_filtering.png` - Pre-filtering visualizations
          - `qc_filtering_thresholds.png` - MAD-based threshold overlays
          - `qc_metrics_after_filtering.png` - Post-filtering quality metrics
          - `<input_basename>_filtered.h5ad` - Clean, filtered dataset ready for downstream analysis
          - `<input_basename>_with_qc.h5ad` - Original data with QC annotations preserved

          If copying outputs for user access, copy individual files (not the entire directory) so users can preview them directly.

          ### Workflow Steps

          The script performs the following steps:

          1. **Calculate QC metrics** - Count depth, gene detection, mitochondrial/ribosomal/hemoglobin content
          2. **Apply MAD-based filtering** - Permissive outlier detection using MAD thresholds for counts/genes/MT%
          3. **Filter genes** - Remove genes detected in few cells
          4. **Generate visualizations** - Comprehensive before/after plots with threshold overlays

          ## Approach 2: Modular Building Blocks (For Custom Workflows)

          For custom analysis workflows or non-standard requirements, use the modular utility functions from `scripts/qc_core.py` and `scripts/qc_plotting.py`:

          ```python
          # Run from scripts/ directory, or add scripts/ to sys.path if needed
          import anndata as ad
          from qc_core import calculate_qc_metrics, detect_outliers_mad, filter_cells
          from qc_plotting import plot_qc_distributions  # Only if visualization needed

          adata = ad.read_h5ad('input.h5ad')
          calculate_qc_metrics(adata, inplace=True)
          # ... custom analysis logic here
          ```

          **When to use this approach:**
          - Different workflow needed (skip steps, change order, apply different thresholds to subsets)
          - Conditional logic (e.g., filter neurons differently than other cells)
          - Partial execution (only metrics/visualization, no filtering)
          - Integration with other analysis steps in a larger pipeline
          - Custom filtering criteria beyond what command-line params support

          **Available utility functions:**

          From `qc_core.py` (core QC operations):
          - `calculate_qc_metrics(adata, mt_pattern, ribo_pattern, hb_pattern, inplace=True)` - Calculate QC metrics and annotate adata
          - `detect_outliers_mad(adata, metric, n_mads, verbose=True)` - MAD-based outlier detection, returns boolean mask
          - `apply_hard_threshold(adata, metric, threshold, operator='>', verbose=True)` - Apply hard cutoffs, returns boolean mask
          - `filter_cells(adata, mask, inplace=False)` - Apply boolean mask to filter cells
          - `filter_genes(adata, min_cells=20, min_counts=None, inplace=True)` - Filter genes by detection
          - `print_qc_summary(adata, label='')` - Print summary statistics

          From `qc_plotting.py` (visualization):
          - `plot_qc_distributions(adata, output_path, title)` - Generate comprehensive QC plots
          - `plot_filtering_thresholds(adata, outlier_masks, thresholds, output_path)` - Visualize filtering thresholds
          - `plot_qc_after_filtering(adata, output_path)` - Generate post-filtering plots

          **Example custom workflows:**

          **Example 1: Only calculate metrics and visualize, don't filter yet**
          ```python
          adata = ad.read_h5ad('input.h5ad')
          calculate_qc_metrics(adata, inplace=True)
          plot_qc_distributions(adata, 'qc_before.png', title='Initial QC')
          print_qc_summary(adata, label='Before filtering')
          ```

          **Example 2: Apply only MT% filtering, keep other metrics permissive**
          ```python
          adata = ad.read_h5ad('input.h5ad')
          calculate_qc_metrics(adata, inplace=True)

          # Only filter high MT% cells
          high_mt = apply_hard_threshold(adata, 'pct_counts_mt', 10, operator='>')
          adata_filtered = filter_cells(adata, ~high_mt)
          adata_filtered.write('filtered.h5ad')
          ```

          **Example 3: Different thresholds for different subsets**
          ```python
          adata = ad.read_h5ad('input.h5ad')
          calculate_qc_metrics(adata, inplace=True)

          # Apply type-specific QC (assumes cell_type metadata exists)
          neurons = adata.obs['cell_type'] == 'neuron'
          other_cells = ~neurons

          # Neurons tolerate higher MT%, other cells use stricter threshold
          neuron_qc = apply_hard_threshold(adata[neurons], 'pct_counts_mt', 15, operator='>')
          other_qc = apply_hard_threshold(adata[other_cells], 'pct_counts_mt', 8, operator='>')
          ```

          ## Best Practices

          1. **Be permissive with filtering** - Default thresholds intentionally retain most cells to avoid losing rare populations
          2. **Inspect visualizations** - Always review before/after plots to ensure filtering makes biological sense
          3. **Consider dataset-specific factors** - Some tissues naturally have higher mitochondrial content (e.g., neurons, cardiomyocytes)
          4. **Check gene annotations** - Mitochondrial gene prefixes vary by species (mt- for mouse, MT- for human)
          5. **Iterate if needed** - QC parameters may need adjustment based on the specific experiment or tissue type

          ## Reference Materials

          For detailed QC methodology, parameter rationale, and troubleshooting guidance, see `references/scverse_qc_guidelines.md`. This reference provides:
          - Detailed explanations of each QC metric and why it matters
          - Rationale for MAD-based thresholds and why they're better than fixed cutoffs
          - Guidelines for interpreting QC visualizations (histograms, violin plots, scatter plots)
          - Species-specific considerations for gene annotations
          - When and how to adjust filtering parameters
          - Advanced QC considerations (ambient RNA correction, doublet detection)

          Load this reference when users need deeper understanding of the methodology or when troubleshooting QC issues.

          ## Next Steps After QC

          Typical downstream analysis steps:
          - Ambient RNA correction (SoupX, CellBender)
          - Doublet detection (scDblFinder)
          - Normalization (log-normalize, scran)
          - Feature selection and dimensionality reduction
          - Clustering and cell type annotation
environment:
  - name: ANTHROPIC_MAX_TOKENS
    value: "8192"
  - name: MESHAGENT_TOKEN
    token:  
      identity: claude-bio-research
      api:
        livekit: {}
        queues:
          list: true
        messaging:
          broadcast: true
          list: true
          send: true
        database:
          list_tables: true
        sync: {}
        storage: {}
        containers:
          logs: true
          use_containers: true
        developer:
          logs: true
        agents:
          register_agent: true
          register_public_toolkit: true
          register_private_toolkit: true
          call: true
          use_agents: true
          use_tools: true
          allowed_toolkits: null
