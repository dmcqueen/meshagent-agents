version: v1
kind: ServiceTemplate
variables: []

metadata:
  name: claude-bio-research
  description: An agent that uses Anthropic's bio research knowledge work plugin and powered by Claude Sonnet 4.5
  annotations:
    meshagent.service.id: meshagent.claude-bio-research

agents:
  - name: claude-bio-research
    annotations:
      meshagent.agent.type: ChatBot

container:
  image: us-central1-docker.pkg.dev/meshagent-public/images/cli:{SERVER_VERSION}-esgz
  command: /bin/bash /var/start.sh
  storage:
    room:
    - path: /data
      read_only: false
    files:
    - path: /var/start.sh
      text: |
          #!/bin/bash

          set -e

          mkdir -p /data/knowledge-work-plugins
          mkdir -p /data/agents/claude-bio-research
          if [ -d /knowledge-work-plugins ]; then
            cp -R -n /knowledge-work-plugins/* /data/knowledge-work-plugins/ 2>/dev/null || true
          fi
          if [[ ! -f /data/agents/claude-bio-research/rules.txt ]]; then
            cp /var/rules-claude-bio-research.txt /data/agents/claude-bio-research/rules.txt
          fi

          exec /usr/bin/meshagent chatbot join \
            --require-storage \
            --require-web-search \
            --require-web-fetch \
            --storage-tool-local-path=/knowledge-work-plugins:/knowledge-work-plugins \
            --storage-tool-room-path=/:/data \
            --script-tool \
            -rr=agents/claude-bio-research/rules.txt \
            --rules-file /data/knowledge-work-plugins/commands/start.md \
            --skill-dir /data/knowledge-work-plugins/skills/instrument-data-to-allotrope \
            --skill-dir /data/knowledge-work-plugins/skills/nextflow-development \
            --skill-dir /data/knowledge-work-plugins/skills/scientific-problem-selection \
            --skill-dir /data/knowledge-work-plugins/skills/scvi-tools \
            --skill-dir /data/knowledge-work-plugins/skills/single-cell-rna-qc \
    - path: /var/rules-claude-bio-research.txt
      read_only: true
      text: |
          ONLY use the knowledge-work-plugins skills to answer questions and follow the pattern specified in the skill
          you can use the storage tool to read skills not just the shell tool
          when a question could be served by a skill, read the skill and follow the pattern specified in the skill
          when using the storage tool to read files attached or referenced by the user, in the room, or output by the shell tool, the file path should be prefixed with /data.
          when using the storage tool to read skills, the path should be prefixed with /data/knowledge-work-plugins (the folder where the plugins are located)
          You have customizable rules stored in agents/claude-bio-research/rules.txt, you can use the read_file tool to read your rules. You can use the write_file tool to update the contents of the rules file or other text files. Use the read_file tool to read PDFs, examine images, or read files with a text/* mime type from attachments or files.
          You are a MeshAgent agent. MeshAgent is an agent operating system. You can find out more at www.meshagent.com and docs.meshagent.com
          You have some slash commands available
    - path: /knowledge-work-plugins/commands/start.md
      text: |
          ---
          description: Set up your bio-research environment and explore available tools
          ---

          # Bio-Research Start

          > If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).

          You are helping a biological researcher get oriented with the bio-research plugin. Walk through the following steps in order.

          ## Step 1: Welcome

          Display this welcome message:

          ```
          Bio-Research Plugin

          Your AI-powered research assistant for the life sciences. This plugin brings
          together literature search, data analysis pipelines,
          and scientific strategy — all in one place.
          ```

          ## Step 2: Check Available MCP Servers

          Test which MCP servers are connected by listing available tools. Group the results:

          **Literature & Data Sources:**
          - ~~literature database — biomedical literature search
          - ~~literature database — preprint access (biology and medicine)
          - ~~journal access — academic publications
          - ~~data repository — collaborative research data (Sage Bionetworks)

          **Drug Discovery & Clinical:**
          - ~~chemical database — bioactive compound database
          - ~~drug target database — drug target discovery platform
          - ClinicalTrials.gov — clinical trial registry
          - ~~clinical data platform — clinical trial site ranking and platform help

          **Visualization & AI:**
          - ~~scientific illustration — create scientific figures and diagrams
          - ~~AI research platform — AI for biology (histopathology, drug discovery)

          Report which servers are connected and which are not yet set up.

          ## Step 3: Survey Available Skills

          List the analysis skills available in this plugin:

          | Skill | What It Does |
          |-------|-------------|
          | **Single-Cell RNA QC** | Quality control for scRNA-seq data with MAD-based filtering |
          | **scvi-tools** | Deep learning for single-cell omics (scVI, scANVI, totalVI, PeakVI, etc.) |
          | **Nextflow Pipelines** | Run nf-core pipelines (RNA-seq, WGS/WES, ATAC-seq) |
          | **Instrument Data Converter** | Convert lab instrument output to Allotrope ASM format |
          | **Scientific Problem Selection** | Systematic framework for choosing research problems |

          ## Step 4: Optional Setup — Binary MCP Servers

          Mention that two additional MCP servers are available as separate installations:

          - **~~genomics platform** — Access cloud analysis data and workflows
            Install: Download `txg-node.mcpb` from https://github.com/10XGenomics/txg-mcp/releases
          - **~~tool database** (Harvard MIMS) — AI tools for scientific discovery
            Install: Download `tooluniverse.mcpb` from https://github.com/mims-harvard/ToolUniverse/releases

          These require downloading binary files and are optional.

          ## Step 5: Ask How to Help

          Ask the researcher what they're working on today. Suggest starting points based on common workflows:

          1. **Literature review** — "Search ~~literature database for recent papers on [topic]"
          2. **Analyze sequencing data** — "Run QC on my single-cell data" or "Set up an RNA-seq pipeline"
          3. **Drug discovery** — "Search ~~chemical database for compounds targeting [protein]" or "Find drug targets for [disease]"
          4. **Data standardization** — "Convert my instrument data to Allotrope format"
          5. **Research strategy** — "Help me evaluate a new project idea"

          Wait for the user's response and guide them to the appropriate tools and skills.
    - path: /knowledge-work-plugins/skills/instrument-data-to-allotrope/LICENSE.txt
      text: |
          Apache License
          Version 2.0, January 2004
          http://www.apache.org/licenses/

          TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

          1. Definitions.

          "License" shall mean the terms and conditions for use, reproduction,
          and distribution as defined by Sections 1 through 9 of this document.

          "Licensor" shall mean the copyright owner or entity authorized by
          the copyright owner that is granting the License.

          "Legal Entity" shall mean the union of the acting entity and all
          other entities that control, are controlled by, or are under common
          control with that entity. For the purposes of this definition,
          "control" means (i) the power, direct or indirect, to cause the
          direction or management of such entity, whether by contract or
          otherwise, or (ii) ownership of fifty percent (50%) or more of the
          outstanding shares, or (iii) beneficial ownership of such entity.

          "You" (or "Your") shall mean an individual or Legal Entity
          exercising permissions granted by this License.

          "Source" form shall mean the preferred form for making modifications,
          including but not limited to software source code, documentation
          source, and configuration files.

          "Object" form shall mean any form resulting from mechanical
          transformation or translation of a Source form, including but
          not limited to compiled object code, generated documentation,
          and conversions to other media types.

          "Work" shall mean the work of authorship, whether in Source or
          Object form, made available under the License, as indicated by a
          copyright notice that is included in or attached to the work
          (an example is provided in the Appendix below).

          "Derivative Works" shall mean any work, whether in Source or Object
          form, that is based on (or derived from) the Work and for which the
          editorial revisions, annotations, elaborations, or other modifications
          represent, as a whole, an original work of authorship. For the purposes
          of this License, Derivative Works shall not include works that remain
          separable from, or merely link (or bind by name) to the interfaces of,
          the Work and Derivative Works thereof.

          "Contribution" shall mean any work of authorship, including
          the original version of the Work and any modifications or additions
          to that Work or Derivative Works thereof, that is intentionally
          submitted to Licensor for inclusion in the Work by the copyright owner
          or by an individual or Legal Entity authorized to submit on behalf of
          the copyright owner. For the purposes of this definition, "submitted"
          means any form of electronic, verbal, or written communication sent
          to the Licensor or its representatives, including but not limited to
          communication on electronic mailing lists, source code control systems,
          and issue tracking systems that are managed by, or on behalf of, the
          Licensor for the purpose of discussing and improving the Work, but
          excluding communication that is conspicuously marked or otherwise
          designated in writing by the copyright owner as "Not a Contribution."

          "Contributor" shall mean Licensor and any individual or Legal Entity
          on behalf of whom a Contribution has been received by Licensor and
          subsequently incorporated within the Work.

          2. Grant of Copyright License. Subject to the terms and conditions of
          this License, each Contributor hereby grants to You a perpetual,
          worldwide, non-exclusive, no-charge, royalty-free, irrevocable
          copyright license to reproduce, prepare Derivative Works of,
          publicly display, publicly perform, sublicense, and distribute the
          Work and such Derivative Works in Source or Object form.

          3. Grant of Patent License. Subject to the terms and conditions of
          this License, each Contributor hereby grants to You a perpetual,
          worldwide, non-exclusive, no-charge, royalty-free, irrevocable
          (except as stated in this section) patent license to make, have made,
          use, offer to sell, sell, import, and otherwise transfer the Work,
          where such license applies only to those patent claims licensable
          by such Contributor that are necessarily infringed by their
          Contribution(s) alone or by combination of their Contribution(s)
          with the Work to which such Contribution(s) was submitted. If You
          institute patent litigation against any entity (including a
          cross-claim or counterclaim in a lawsuit) alleging that the Work
          or a Contribution incorporated within the Work constitutes direct
          or contributory patent infringement, then any patent licenses
          granted to You under this License for that Work shall terminate
          as of the date such litigation is filed.

          4. Redistribution. You may reproduce and distribute copies of the
          Work or Derivative Works thereof in any medium, with or without
          modifications, and in Source or Object form, provided that You
          meet the following conditions:

          (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

          (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

          (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

          (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

          You may add Your own copyright statement to Your modifications and
          may provide additional or different license terms and conditions
          for use, reproduction, or distribution of Your modifications, or
          for any such Derivative Works as a whole, provided Your use,
          reproduction, and distribution of the Work otherwise complies with
          the conditions stated in this License.

          5. Submission of Contributions. Unless You explicitly state otherwise,
          any Contribution intentionally submitted for inclusion in the Work
          by You to the Licensor shall be under the terms and conditions of
          this License, without any additional terms or conditions.
          Notwithstanding the above, nothing herein shall supersede or modify
          the terms of any separate license agreement you may have executed
          with Licensor regarding such Contributions.

          6. Trademarks. This License does not grant permission to use the trade
          names, trademarks, service marks, or product names of the Licensor,
          except as required for reasonable and customary use in describing the
          origin of the Work and reproducing the content of the NOTICE file.

          7. Disclaimer of Warranty. Unless required by applicable law or
          agreed to in writing, Licensor provides the Work (and each
          Contributor provides its Contributions) on an "AS IS" BASIS,
          WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
          implied, including, without limitation, any warranties or conditions
          of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
          PARTICULAR PURPOSE. You are solely responsible for determining the
          appropriateness of using or redistributing the Work and assume any
          risks associated with Your exercise of permissions under this License.

          8. Limitation of Liability. In no event and under no legal theory,
          whether in tort (including negligence), contract, or otherwise,
          unless required by applicable law (such as deliberate and grossly
          negligent acts) or agreed to in writing, shall any Contributor be
          liable to You for damages, including any direct, indirect, special,
          incidental, or consequential damages of any character arising as a
          result of this License or out of the use or inability to use the
          Work (including but not limited to damages for loss of goodwill,
          work stoppage, computer failure or malfunction, or any and all
          other commercial damages or losses), even if such Contributor
          has been advised of the possibility of such damages.

          9. Accepting Warranty or Additional Liability. While redistributing
          the Work or Derivative Works thereof, You may choose to offer,
          and charge a fee for, acceptance of support, warranty, indemnity,
          or other liability obligations and/or rights consistent with this
          License. However, in accepting such obligations, You may act only
          on Your own behalf and on Your sole responsibility, not on behalf
          of any other Contributor, and only if You agree to indemnify,
          defend, and hold each Contributor harmless for any liability
          incurred by, or claims asserted against, such Contributor by reason
          of your accepting any such warranty or additional liability.

          END OF TERMS AND CONDITIONS

          APPENDIX: How to apply the Apache License to your work.

          To apply the Apache License to your work, attach the following
          boilerplate notice, with the fields enclosed by brackets "[]"
          replaced with your own identifying information. (Don't include
          the brackets!) The text should be enclosed in the appropriate
          comment syntax for the file format. We also recommend that a
          file or class name and description of purpose be included on the
          same "printed page" as the copyright notice for easier
          identification within third-party archives.

          Copyright [yyyy] [name of copyright owner]

          Licensed under the Apache License, Version 2.0 (the "License");
          you may not use this file except in compliance with the License.
          You may obtain a copy of the License at

          http://www.apache.org/licenses/LICENSE-2.0

          Unless required by applicable law or agreed to in writing, software
          distributed under the License is distributed on an "AS IS" BASIS,
          WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
          See the License for the specific language governing permissions and
          limitations under the License.
    - path: /knowledge-work-plugins/skills/instrument-data-to-allotrope/SKILL.md
      text: |
          ---
          name: instrument-data-to-allotrope
          description: Convert laboratory instrument output files (PDF, CSV, Excel, TXT) to Allotrope Simple Model (ASM) JSON format or flattened 2D CSV. Use this skill when scientists need to standardize instrument data for LIMS systems, data lakes, or downstream analysis. Supports auto-detection of instrument types. Outputs include full ASM JSON, flattened CSV for easy import, and exportable Python code for data engineers. Common triggers include converting instrument files, standardizing lab data, preparing data for upload to LIMS/ELN systems, or generating parser code for production pipelines.
          ---

          # Instrument Data to Allotrope Converter

          Convert instrument files into standardized Allotrope Simple Model (ASM) format for LIMS upload, data lakes, or handoff to data engineering teams.

          > **Note: This is an Example Skill**
          >
          > This skill demonstrates how skills can support your data engineering tasks—automating schema transformations, parsing instrument outputs, and generating production-ready code.
          >
          > **To customize for your organization:**
          > - Modify the `references/` files to include your company's specific schemas or ontology mappings
          > - Use an MCP server to connect to systems that define your schemas (e.g., your LIMS, data catalog, or schema registry)
          > - Extend the `scripts/` to handle proprietary instrument formats or internal data standards
          >
          > This pattern can be adapted for any data transformation workflow where you need to convert between formats or validate against organizational standards.

          ## Workflow Overview

          1. **Detect instrument type** from file contents (auto-detect or user-specified)
          2. **Parse file** using allotropy library (native) or flexible fallback parser
          3. **Generate outputs**:
             - ASM JSON (full semantic structure)
             - Flattened CSV (2D tabular format)
             - Python parser code (for data engineer handoff)
          4. **Deliver** files with summary and usage instructions

          > **When Uncertain:** If you're unsure how to map a field to ASM (e.g., is this raw data or calculated? device setting or environmental condition?), ask the user for clarification. Refer to `references/field_classification_guide.md` for guidance, but when ambiguity remains, confirm with the user rather than guessing.

          ## Quick Start

          ```python
          # Install requirements first
          pip install allotropy pandas openpyxl pdfplumber --break-system-packages

          # Core conversion
          from allotropy.parser_factory import Vendor
          from allotropy.to_allotrope import allotrope_from_file

          # Convert with allotropy
          asm = allotrope_from_file("instrument_data.csv", Vendor.BECKMAN_VI_CELL_BLU)
          ```

          ## Output Format Selection

          **ASM JSON (default)** - Full semantic structure with ontology URIs
          - Best for: LIMS systems expecting ASM, data lakes, long-term archival
          - Validates against Allotrope schemas

          **Flattened CSV** - 2D tabular representation
          - Best for: Quick analysis, Excel users, systems without JSON support
          - Each measurement becomes one row with metadata repeated

          **Both** - Generate both formats for maximum flexibility

          ## Calculated Data Handling

          **IMPORTANT:** Separate raw measurements from calculated/derived values.

          - **Raw data** → `measurement-document` (direct instrument readings)
          - **Calculated data** → `calculated-data-aggregate-document` (derived values)

          Calculated values MUST include traceability via `data-source-aggregate-document`:

          ```json
          "calculated-data-aggregate-document": {
            "calculated-data-document": [{
              "calculated-data-identifier": "SAMPLE_B1_DIN_001",
              "calculated-data-name": "DNA integrity number",
              "calculated-result": {"value": 9.5, "unit": "(unitless)"},
              "data-source-aggregate-document": {
                "data-source-document": [{
                  "data-source-identifier": "SAMPLE_B1_MEASUREMENT",
                  "data-source-feature": "electrophoresis trace"
                }]
              }
            }]
          }
          ```

          **Common calculated fields by instrument type:**
          | Instrument | Calculated Fields |
          |------------|-------------------|
          | Cell counter | Viability %, cell density dilution-adjusted values |
          | Spectrophotometer | Concentration (from absorbance), 260/280 ratio |
          | Plate reader | Concentrations from standard curve, %CV |
          | Electrophoresis | DIN/RIN, region concentrations, average sizes |
          | qPCR | Relative quantities, fold change |

          See `references/field_classification_guide.md` for detailed guidance on raw vs. calculated classification.

          ## Validation

          Always validate ASM output before delivering to the user:

          ```bash
          python scripts/validate_asm.py output.json
          python scripts/validate_asm.py output.json --reference known_good.json  # Compare to reference
          python scripts/validate_asm.py output.json --strict  # Treat warnings as errors
          ```

          **Validation Rules:**
          - Based on Allotrope ASM specification (December 2024)
          - Last updated: 2026-01-07
          - Source: https://gitlab.com/allotrope-public/asm

          **Soft Validation Approach:**
          Unknown techniques, units, or sample roles generate **warnings** (not errors) to allow for forward compatibility. If Allotrope adds new values after December 2024, the validator won't block them—it will flag them for manual verification. Use `--strict` mode to treat warnings as errors if you need stricter validation.

          **What it checks:**
          - Correct technique selection (e.g., multi-analyte profiling vs plate reader)
          - Field naming conventions (space-separated, not hyphenated)
          - Calculated data has traceability (`data-source-aggregate-document`)
          - Unique identifiers exist for measurements and calculated values
          - Required metadata present
          - Valid units and sample roles (with soft validation for unknown values)

          ## Supported Instruments

          See `references/supported_instruments.md` for complete list. Key instruments:

          | Category | Instruments |
          |----------|-------------|
          | Cell Counting | Vi-CELL BLU, Vi-CELL XR, NucleoCounter |
          | Spectrophotometry | NanoDrop One/Eight/8000, Lunatic |
          | Plate Readers | SoftMax Pro, EnVision, Gen5, CLARIOstar |
          | ELISA | SoftMax Pro, BMG MARS, MSD Workbench |
          | qPCR | QuantStudio, Bio-Rad CFX |
          | Chromatography | Empower, Chromeleon |

          ## Detection & Parsing Strategy

          ### Tier 1: Native allotropy parsing (PREFERRED)
          **Always try allotropy first.** Check available vendors directly:

          ```python
          from allotropy.parser_factory import Vendor

          # List all supported vendors
          for v in Vendor:
              print(f"{v.name}")

          # Common vendors:
          # AGILENT_TAPESTATION_ANALYSIS  (for TapeStation XML)
          # BECKMAN_VI_CELL_BLU
          # THERMO_FISHER_NANODROP_EIGHT
          # MOLDEV_SOFTMAX_PRO
          # APPBIO_QUANTSTUDIO
          # ... many more
          ```

          **When the user provides a file, check if allotropy supports it before falling back to manual parsing.** The `scripts/convert_to_asm.py` auto-detection only covers a subset of allotropy vendors.

          ### Tier 2: Flexible fallback parsing
          **Only use if allotropy doesn't support the instrument.** This fallback:
          - Does NOT generate `calculated-data-aggregate-document`
          - Does NOT include full traceability
          - Produces simplified ASM structure

          Use flexible parser with:
          - Column name fuzzy matching
          - Unit extraction from headers
          - Metadata extraction from file structure

          ### Tier 3: PDF extraction
          For PDF-only files, extract tables using pdfplumber, then apply Tier 2 parsing.

          ## Pre-Parsing Checklist

          Before writing a custom parser, ALWAYS:

          1. **Check if allotropy supports it** - Use native parser if available
          2. **Find a reference ASM file** - Check `references/examples/` or ask user
          3. **Review instrument-specific guide** - Check `references/instrument_guides/`
          4. **Validate against reference** - Run `validate_asm.py --reference <file>`

          ## Common Mistakes to Avoid

          | Mistake | Correct Approach |
          |---------|------------------|
          | Manifest as object | Use URL string |
          | Lowercase detection types | Use "Absorbance" not "absorbance" |
          | "emission wavelength setting" | Use "detector wavelength setting" for emission |
          | All measurements in one document | Group by well/sample location |
          | Missing procedure metadata | Extract ALL device settings per measurement |

          ## Code Export for Data Engineers

          Generate standalone Python scripts that scientists can hand off:

          ```python
          # Export parser code
          python scripts/export_parser.py --input "data.csv" --vendor "VI_CELL_BLU" --output "parser_script.py"
          ```

          The exported script:
          - Has no external dependencies beyond pandas/allotropy
          - Includes inline documentation
          - Can run in Jupyter notebooks
          - Is production-ready for data pipelines

          ## File Structure

          ```
          instrument-data-to-allotrope/
          ├── SKILL.md                          # This file
          ├── scripts/
          │   ├── convert_to_asm.py            # Main conversion script
          │   ├── flatten_asm.py               # ASM → 2D CSV conversion
          │   ├── export_parser.py             # Generate standalone parser code
          │   └── validate_asm.py              # Validate ASM output quality
          └── references/
              ├── supported_instruments.md     # Full instrument list with Vendor enums
              ├── asm_schema_overview.md       # ASM structure reference
              ├── field_classification_guide.md # Where to put different field types
              └── flattening_guide.md          # How flattening works
          ```

          ## Usage Examples

          ### Example 1: Vi-CELL BLU file
          ```
          User: "Convert this cell counting data to Allotrope format"
          [uploads viCell_Results.xlsx]

          Claude:
          1. Detects Vi-CELL BLU (95% confidence)
          2. Converts using allotropy native parser
          3. Outputs:
             - viCell_Results_asm.json (full ASM)
             - viCell_Results_flat.csv (2D format)
             - viCell_parser.py (exportable code)
          ```

          ### Example 2: Request for code handoff
          ```
          User: "I need to give our data engineer code to parse NanoDrop files"

          Claude:
          1. Generates self-contained Python script
          2. Includes sample input/output
          3. Documents all assumptions
          4. Provides Jupyter notebook version
          ```

          ### Example 3: LIMS-ready flattened output
          ```
          User: "Convert this ELISA data to a CSV I can upload to our LIMS"

          Claude:
          1. Parses plate reader data
          2. Generates flattened CSV with columns:
             - sample_identifier, well_position, measurement_value, measurement_unit
             - instrument_serial_number, analysis_datetime, assay_type
          3. Validates against common LIMS import requirements
          ```

          ## Implementation Notes

          ### Installing allotropy
          ```bash
          pip install allotropy --break-system-packages
          ```

          ### Handling parse failures
          If allotropy native parsing fails:
          1. Log the error for debugging
          2. Fall back to flexible parser
          3. Report reduced metadata completeness to user
          4. Suggest exporting different format from instrument

          ### ASM Schema Validation
          Validate output against Allotrope schemas when available:
          ```python
          import jsonschema
          # Schema URLs in references/asm_schema_overview.md
          ```
    - path: /knowledge-work-plugins/skills/instrument-data-to-allotrope/references/asm_schema_overview.md
      text: |
          # ASM Schema Overview

          The Allotrope Simple Model (ASM) is a JSON-based standard for representing laboratory instrument data with semantic consistency.

          ## Core Concepts

          ### Structure
          ASM uses a hierarchical document structure:
          - **Manifest** - Links to ontologies and schemas
          - **Data** - The actual measurement data organized by technique

          ### Key Components

          ```json
          {
            "$asm.manifest": {
              "vocabulary": ["http://purl.allotrope.org/voc/afo/REC/2023/09/"],
              "contexts": ["http://purl.allotrope.org/json-ld/afo-context-REC-2023-09.jsonld"]
            },
            "<technique>-aggregate-document": {
              "device-system-document": { ... },
              "<technique>-document": [
                {
                  "measurement-aggregate-document": {
                    "measurement-document": [ ... ]
                  }
                }
              ]
            }
          }
          ```

          ## Required Metadata Documents

          ### data system document
          Every ASM output MUST include this document with:
          - `ASM file identifier`: Output filename
          - `data system instance identifier`: System ID or "N/A"
          - `file name`: Source input filename
          - `UNC path`: Path to source file
          - `ASM converter name`: Parser identifier (e.g., "allotropy_beckman_coulter_biomek")
          - `ASM converter version`: Version string
          - `software name`: Instrument software that generated the source file

          ### device system document
          Every ASM output MUST include this document with:
          - `equipment serial number`: Main instrument serial
          - `product manufacturer`: Vendor name
          - `device document`: Array of sub-components (probes, pods, etc.)
            - `device type`: Standardized type (e.g., "liquid handler probe head")
            - `device identifier`: Logical name (e.g., "Pod1", not serial number)
            - `equipment serial number`: Component serial
            - `product manufacturer`: Component vendor

          ## Available ASM Techniques

          The official ASM repository includes **65 technique schemas**:

          ```
          absorbance, automated-reactors, balance, bga, binding-affinity, bulk-density,
          cell-counting, cell-culture-analyzer, chromatography, code-reader, conductance,
          conductivity, disintegration, dsc, dvs, electronic-lab-notebook,
          electronic-spectrometry, electrophoresis, flow-cytometry, fluorescence,
          foam-height, foam-qualification, fplc, ftir, gas-chromatography, gc-ms, gloss,
          hot-tack, impedance, lc-ms, light-obscuration, liquid-chromatography,
          loss-on-drying, luminescence, mass-spectrometry, metabolite-analyzer,
          multi-analyte-profiling, nephelometry, nmr, optical-imaging, optical-microscopy,
          osmolality, oven-kf, pcr, ph, plate-reader, pressure-monitoring, psd, pumping,
          raman, rheometry, sem, solution-analyzer, specific-rotation, spectrophotometry,
          stirring, surface-area-analysis, tablet-hardness, temperature-monitoring,
          tensile-test, thermogravimetric-analysis, titration, ultraviolet-absorbance,
          x-ray-powder-diffraction
          ```

          See: https://gitlab.com/allotrope-public/asm/-/tree/main/json-schemas/adm

          ## Common ASM Schemas by Technique

          Below are details for frequently-used techniques:

          ### Cell Counting
          Schema: `cell-counting/REC/2024/09/cell-counting.schema.json`

          Key fields:
          - `viable-cell-density` (cells/mL)
          - `viability` (percentage)
          - `total-cell-count`
          - `dead-cell-count`
          - `cell-diameter-distribution-datum`

          ### Spectrophotometry (UV-Vis)
          Schema: `spectrophotometry/REC/2024/06/spectrophotometry.schema.json`

          Key fields:
          - `absorbance` (dimensionless)
          - `wavelength` (nm)
          - `transmittance` (percentage)
          - `pathlength` (cm)
          - `concentration` with units

          ### Plate Reader
          Schema: `plate-reader/REC/2024/06/plate-reader.schema.json`

          Key fields:
          - `absorbance`
          - `fluorescence`
          - `luminescence`
          - `well-location` (A1-H12)
          - `plate-identifier`

          ### qPCR
          Schema: `pcr/REC/2024/06/pcr.schema.json`

          Key fields:
          - `cycle-threshold-result`
          - `amplification-efficiency`
          - `melt-curve-datum`
          - `target-DNA-description`

          ### Chromatography
          Schema: `liquid-chromatography/REC/2023/09/liquid-chromatography.schema.json`

          Key fields:
          - `retention-time` (minutes)
          - `peak-area`
          - `peak-height`
          - `peak-width`
          - `chromatogram-data-cube`

          ## Data Patterns

          ### Value Datum
          Simple value with unit:
          ```json
          {
            "value": 1.5,
            "unit": "mL"
          }
          ```

          ### Aggregate Datum
          Collection of related values:
          ```json
          {
            "measurement-aggregate-document": {
              "measurement-document": [
                { "viable-cell-density": {"value": 2.5e6, "unit": "(cell/mL)"} },
                { "viability": {"value": 95.2, "unit": "%"} }
              ]
            }
          }
          ```

          ### Data Cube
          Multi-dimensional array data:
          ```json
          {
            "cube-structure": {
              "dimensions": [{"@componentDatatype": "double", "concept": "elapsed time"}],
              "measures": [{"@componentDatatype": "double", "concept": "absorbance"}]
            },
            "data": {
              "dimensions": [[0, 1, 2, 3, 4]],
              "measures": [[0.1, 0.2, 0.3, 0.4, 0.5]]
            }
          }
          ```

          ## Validation

          Validate ASM output against official schemas:

          ```python
          import json
          import jsonschema
          from urllib.request import urlopen

          # Load ASM output
          with open("output.json") as f:
              asm = json.load(f)

          # Get schema URL from manifest
          schema_url = asm.get("$asm.manifest", {}).get("$ref")

          # Validate (simplified - real validation more complex)
          # Note: Full validation requires resolving $ref references
          ```

          ## Schema Repository

          Official schemas: https://gitlab.com/allotrope-public/asm/-/tree/main/json-schemas/adm

          Schema structure:
          ```
          json-schemas/adm/
          ├── cell-counting/
          │   └── REC/2024/09/
          │       └── cell-counting.schema.json
          ├── spectrophotometry/
          │   └── REC/2024/06/
          │       └── spectrophotometry.schema.json
          ├── plate-reader/
          │   └── REC/2024/06/
          │       └── plate-reader.schema.json
          └── ...
          ```

          ## Common Issues

          ### Missing Fields
          Not all instrument exports contain all ASM fields. Report completeness:
          ```python
          def report_completeness(asm, expected_fields):
              found = set(extract_all_fields(asm))
              missing = expected_fields - found
              return len(found) / len(expected_fields) * 100
          ```

          ### Unit Variations
          Instruments may use different unit formats. The allotropy library normalizes these:
          - "cells/mL" → "(cell/mL)"
          - "%" → "%"
          - "nm" → "nm"

          ### Date Formats
          ASM uses ISO 8601: `2024-01-15T10:30:00Z`
    - path: /knowledge-work-plugins/skills/instrument-data-to-allotrope/references/field_classification_guide.md
      text: |
          # Field Classification Guide

          This guide helps classify instrument data fields into the correct ASM document locations. Use this when mapping raw instrument output to Allotrope Simple Model structure.

          ## ASM Document Hierarchy

          ```
          <technique>-aggregate-document
          ├── device-system-document          # Instrument hardware info
          ├── data-system-document            # Software/conversion info
          ├── <technique>-document[]          # Per-run/sequence data
          │   ├── analyst                     # Who performed the analysis
          │   ├── measurement-aggregate-document
          │   │   ├── measurement-time
          │   │   ├── measurement-document[]  # Individual measurements
          │   │   │   ├── sample-document
          │   │   │   ├── device-control-aggregate-document
          │   │   │   └── [measurement fields]
          │   │   └── [aggregate-level metadata]
          │   ├── processed-data-aggregate-document
          │   │   └── processed-data-document[]
          │   │       ├── data-processing-document
          │   │       └── [processed results]
          │   └── calculated-data-aggregate-document
          │       └── calculated-data-document[]
          ```

          ## Field Classification Categories

          ### 1. Device/Instrument Information → `device-system-document`

          Hardware and firmware details about the physical instrument.

          | Field Type | ASM Field | Examples |
          |------------|-----------|----------|
          | Instrument name | `model-number` | "Vi-CELL BLU", "NanoDrop One" |
          | Serial number | `equipment-serial-number` | "VCB-12345", "SN001234" |
          | Manufacturer | `product-manufacturer` | "Beckman Coulter", "Thermo Fisher" |
          | Firmware version | `firmware-version` | "v2.1.3" |
          | Device ID | `device-identifier` | "Instrument_01" |
          | Brand | `brand-name` | "Beckman Coulter" |

          **Rule:** If the value describes the physical instrument and doesn't change between runs, it goes in `device-system-document`.

          ---

          ### 2. Software/Data System Information → `data-system-document`

          Information about software used for acquisition, analysis, or conversion.

          | Field Type | ASM Field | Examples |
          |------------|-----------|----------|
          | Software name | `software-name` | "Chromeleon", "Gen5" |
          | Software version | `software-version` | "7.3.2" |
          | File name | `file-name` | "experiment_001.xlsx" |
          | File path | `file-identifier` | "/data/runs/2024-01-15/" |
          | Database ID | `ASM-converter-name` | "allotropy v0.1.55" |

          **Rule:** If the value describes software, file metadata, or data provenance, it goes in `data-system-document`.

          ---

          ### 3. Sample Information → `sample-document`

          Metadata about the biological/chemical sample being analyzed.

          | Field Type | ASM Field | Examples |
          |------------|-----------|----------|
          | Sample ID | `sample-identifier` | "Sample_A", "LIMS-001234" |
          | Sample name | `written-name` | "CHO Cell Culture Day 5" |
          | Sample type/role | `sample-role-type` | "unknown sample role", "control sample role" |
          | Batch ID | `batch-identifier` | "Batch-2024-001" |
          | Description | `description` | "Protein expression sample" |
          | Well position | `location-identifier` | "A1", "B3" |

          **Rule:** If the value identifies or describes what was measured (not how), it goes in `sample-document`.

          ---

          ### 4. Device Control Settings → `device-control-aggregate-document`

          Instrument settings and parameters used during measurement.

          | Field Type | ASM Field | Examples |
          |------------|-----------|----------|
          | Injection volume | `sample-volume-setting` | 10 µL |
          | Wavelength | `detector-wavelength-setting` | 254 nm |
          | Temperature | `compartment-temperature` | 37°C |
          | Flow rate | `flow-rate` | 1.0 mL/min |
          | Exposure time | `exposure-duration-setting` | 500 ms |
          | Detector gain | `detector-gain-setting` | 1.5 |
          | Illumination | `illumination-setting` | 80% |

          **Rule:** If the value is a configurable instrument parameter that affects measurement, it goes in `device-control-aggregate-document`.

          ---

          ### 5. Environmental Conditions → `device-control-document` or technique-specific

          Ambient or controlled environmental parameters during measurement.

          | Field Type | ASM Field | Examples |
          |------------|-----------|----------|
          | Ambient temperature | `ambient-temperature` | 22.5°C |
          | Humidity | `ambient-relative-humidity` | 45% |
          | Column temperature | `compartment-temperature` | 30°C |
          | Sample temperature | `sample-temperature` | 4°C |
          | Electrophoresis temp | (technique-specific) | 26.4°C |

          **Rule:** Environmental conditions that affect measurement quality go with device control or in technique-specific locations.

          ---

          ### 6. Raw Measurement Data → `measurement-document`

          Direct instrument readings - the "ground truth" data.

          | Field Type | ASM Field | Examples |
          |------------|-----------|----------|
          | Absorbance | `absorbance` | 0.523 AU |
          | Fluorescence | `fluorescence` | 12500 RFU |
          | Cell count | `total-cell-count` | 2.5e6 cells |
          | Peak area | `peak-area` | 1234.5 mAU·min |
          | Retention time | `retention-time` | 5.67 min |
          | Ct value | `cycle-threshold-result` | 24.5 |
          | Concentration (measured) | `mass-concentration` | 1.5 mg/mL |

          **Rule:** If the value is a direct instrument reading that wasn't computed from other values in this analysis, it goes in `measurement-document`.

          ---

          ### 7. Calculated/Derived Data → `calculated-data-aggregate-document`

          Values computed from raw measurements.

          | Field Type | ASM Field | Examples |
          |------------|-----------|----------|
          | Viability % | `calculated-result` | 95.2% |
          | Concentration (from std curve) | `calculated-result` | 125 ng/µL |
          | Ratio (260/280) | `calculated-result` | 1.89 |
          | Relative quantity | `calculated-result` | 2.5x |
          | % Recovery | `calculated-result` | 98.7% |
          | CV% | `calculated-result` | 2.3% |

          **Calculated data document structure:**
          ```json
          {
            "calculated-data-name": "viability",
            "calculated-result": {"value": 95.2, "unit": "%"},
            "calculation-description": "viable cells / total cells * 100"
          }
          ```

          **Rule:** If the value was computed from other measurements in this analysis, it goes in `calculated-data-aggregate-document`. Include `calculation-description` when possible.

          ---

          ### 8. Processed/Analyzed Data → `processed-data-aggregate-document`

          Results from data processing algorithms (peak integration, cell classification, etc.).

          | Field Type | ASM Field | Examples |
          |------------|-----------|----------|
          | Peak list | `peak-list` | Integrated peak results |
          | Cell size distribution | `cell-diameter-distribution` | Histogram data |
          | Baseline-corrected data | (in processed-data-document) | Corrected spectra |
          | Fitted curve | (in processed-data-document) | Standard curve fit |

          **Associated `data-processing-document`:**
          ```json
          {
            "cell-type-processing-method": "trypan blue exclusion",
            "cell-density-dilution-factor": {"value": 2, "unit": "(unitless)"},
            "minimum-cell-diameter-setting": {"value": 5, "unit": "µm"},
            "maximum-cell-diameter-setting": {"value": 50, "unit": "µm"}
          }
          ```

          **Rule:** If the value results from an algorithm or processing method applied to raw data, it goes in `processed-data-aggregate-document` with its processing parameters in `data-processing-document`.

          ---

          ### 9. Timing/Timestamps → Various locations

          | Timestamp Type | Location | ASM Field |
          |----------------|----------|-----------|
          | Measurement time | `measurement-document` | `measurement-time` |
          | Run start time | `analysis-sequence-document` | `analysis-sequence-start-time` |
          | Run end time | `analysis-sequence-document` | `analysis-sequence-end-time` |
          | Data export time | `data-system-document` | (custom) |

          **Rule:** Use ISO 8601 format: `2024-01-15T10:30:00Z`

          ---

          ### 10. Analyst/Operator Information → `<technique>-document`

          | Field Type | ASM Field | Examples |
          |------------|-----------|----------|
          | Operator name | `analyst` | "jsmith" |
          | Reviewer | (custom or extension) | "Pending" |

          **Rule:** Analyst goes at the technique-document level, not in individual measurements.

          ---

          ## Decision Tree

          ```
          Is this field about...

          THE INSTRUMENT ITSELF?
          ├── Hardware specs → device-system-document
          └── Software/files → data-system-document

          THE SAMPLE?
          └── Sample ID, name, type, batch → sample-document

          INSTRUMENT SETTINGS?
          └── Configurable parameters → device-control-aggregate-document

          ENVIRONMENTAL CONDITIONS?
          └── Temp, humidity, etc. → device-control-document

          A DIRECT READING?
          └── Raw instrument output → measurement-document

          A COMPUTED VALUE?
          ├── From other measurements → calculated-data-document
          └── From processing algorithm → processed-data-document

          TIMING?
          ├── When measured → measurement-document.measurement-time
          └── When run started/ended → analysis-sequence-document

          WHO DID IT?
          └── Operator/analyst → <technique>-document.analyst
          ```

          ## Common Instrument-to-ASM Mappings

          > **Note:** These mappings are derived from the [Benchling allotropy library](https://github.com/Benchling-Open-Source/allotropy/tree/main/src/allotropy/parsers). For authoritative mappings, consult the parser source code for your specific instrument.

          ### Cell Counter (Vi-CELL BLU)
          *Source: `allotropy/parsers/beckman_vi_cell_blu/vi_cell_blu_structure.py`*

          | Instrument Field | ASM Field |
          |-----------------|-----------|
          | Sample ID | `sample_identifier` |
          | Analysis date/time | `measurement_time` |
          | Analysis by | `analyst` |
          | Viability (%) | `viability` |
          | Viable (x10^6) cells/mL | `viable_cell_density` |
          | Total (x10^6) cells/mL | `total_cell_density` |
          | Cell count | `total_cell_count` |
          | Viable cells | `viable_cell_count` |
          | Average diameter (μm) | `average_total_cell_diameter` |
          | Average viable diameter (μm) | `average_live_cell_diameter` |
          | Average circularity | `average_total_cell_circularity` |
          | Cell type | `cell_type_processing_method` (data-processing) |
          | Dilution | `cell_density_dilution_factor` (data-processing) |
          | Min/Max Diameter | `minimum/maximum_cell_diameter_setting` (data-processing) |

          ### Spectrophotometer (NanoDrop)
          | Instrument Field | ASM Field |
          |-----------------|-----------|
          | Sample Name | `sample_identifier` |
          | A260, A280 | `absorbance` (with wavelength) |
          | Concentration | `mass_concentration` |
          | 260/280 ratio | `a260_a280_ratio` |
          | Pathlength | `pathlength` |

          ### Plate Reader
          | Instrument Field | ASM Field |
          |-----------------|-----------|
          | Well | `location_identifier` |
          | Sample Type | `sample_role_type` |
          | Absorbance/OD | `absorbance` |
          | Fluorescence | `fluorescence` |
          | Plate ID | `container_identifier` |

          ### Chromatography (HPLC)
          | Instrument Field | ASM Field |
          |-----------------|-----------|
          | Sample ID | `sample_identifier` |
          | Injection Volume | `injection_volume` |
          | Retention Time | `retention_time` |
          | Peak Area | `peak_area` |
          | Peak Height | `peak_height` |
          | Column Temp | `column_oven_temperature` |
          | Flow Rate | `flow_rate` |

          ## Unit Handling

          Only use units explicitly present in source data. If a value has no unit specified:
          - Use `(unitless)` as the unit value
          - Do NOT infer units based on domain knowledge

          ## Calculated Data Traceability

          When creating calculated values, always link them to their source data using `data-source-aggregate-document`:

          ```json
          {
              "calculated-data-name": "DIN",
              "calculated-result": {"value": 5.8, "unit": "(unitless)"},
              "calculated-data-identifier": "TEST_ID_147",
              "data-source-aggregate-document": {
                  "data-source-document": [{
                      "data-source-identifier": "TEST_ID_145",
                      "data-source-feature": "sample"
                  }]
              }
          }
          ```

          This declares: "DIN 5.8 was calculated from the sample at `TEST_ID_145`."

          **Why this matters:**
          - **Audits**: Prove a value came from specific raw data
          - **Debugging**: Trace unexpected results back to their source
          - **Reprocessing**: Know which inputs to re-analyze if algorithms change

          **Assign unique IDs to:**
          - Measurements, peaks, regions, and calculated values
          - Use a consistent naming pattern (e.g., `INSTRUMENT_TYPE_TEST_ID_N`)

          This enables bidirectional traversal: trace from calculated → raw, or raw → all derived values.

          ---

          ## Nested Document Structure (Critical)

          A common mistake is "flattening" fields directly onto measurement documents when they should be wrapped in nested structures. This breaks schema compliance and loses semantic context.

          ### Why Nesting Matters

          ASM uses nested documents for semantic grouping:

          | Document | Purpose | Contains |
          |----------|---------|----------|
          | `sample document` | What was measured | Sample ID, locations, plate identifiers |
          | `device control aggregate document` | How instrument operated | Settings, parameters, techniques |
          | `custom information document` | Vendor-specific fields | Non-standard fields that don't map to ASM |

          ### Sample Document Fields

          These fields MUST be inside `sample document`, not flattened on measurement:

          ```json
          // ❌ WRONG - Fields flattened on measurement
          {
            "measurement identifier": "TEST_001",
            "sample identifier": "Sample_A",
            "location identifier": "A1",
            "absorbance": {"value": 0.5, "unit": "(unitless)"}
          }

          // ✅ CORRECT - Fields nested in sample document
          {
            "measurement identifier": "TEST_001",
            "sample document": {
              "sample identifier": "Sample_A",
              "location identifier": "A1",
              "well plate identifier": "96WP001"
            },
            "absorbance": {"value": 0.5, "unit": "(unitless)"}
          }
          ```

          **Fields belonging in sample document:**
          - `sample identifier` - Sample ID/name
          - `written name` - Descriptive sample name
          - `batch identifier` - Batch/lot number
          - `sample role type` - Standard, blank, control, unknown
          - `location identifier` - Well position (A1, B3, etc.)
          - `well plate identifier` - Plate barcode
          - `description` - Sample description

          ### Device Control Document Fields

          Instrument settings MUST be inside `device control aggregate document`:

          ```json
          // ❌ WRONG - Device settings flattened
          {
            "measurement identifier": "TEST_001",
            "device identifier": "Pod1",
            "technique": "Custom",
            "volume": {"value": 26, "unit": "μL"}
          }

          // ✅ CORRECT - Settings nested in device control
          {
            "measurement identifier": "TEST_001",
            "device control aggregate document": {
              "device control document": [{
                "device type": "liquid handler",
                "device identifier": "Pod1"
              }]
            },
            "aspiration volume": {"value": 26, "unit": "μL"}
          }
          ```

          **Fields belonging in device control:**
          - `device type` - Type of device
          - `device identifier` - Device ID
          - `detector wavelength setting` - Wavelength for detection
          - `compartment temperature` - Temperature setting
          - `sample volume setting` - Volume setting
          - `flow rate` - Flow rate setting

          ### Custom Information Document

          Vendor-specific fields that don't map to standard ASM terms go in `custom information document`:

          ```json
          "device control document": [{
            "device type": "liquid handler",
            "custom information document": {
              "probe": "2",
              "pod": "Pod1",
              "source labware name": "Inducer",
              "destination labware name": "GRP1"
            }
          }]
          ```

          ### Liquid Handler: Transfer Pairing

          For liquid handlers, a measurement represents a complete transfer (aspirate + dispense), not separate operations:

          ```json
          // ❌ WRONG - Separate records for aspirate and dispense
          [
            {"measurement identifier": "OP_001", "transfer type": "Aspirate", "volume": {"value": 26, "unit": "μL"}},
            {"measurement identifier": "OP_002", "transfer type": "Dispense", "volume": {"value": 26, "unit": "μL"}}
          ]

          // ✅ CORRECT - Single record with source and destination
          {
            "measurement identifier": "TRANSFER_001",
            "sample document": {
              "source well location identifier": "1",
              "destination well location identifier": "2",
              "source well plate identifier": "96WP001",
              "destination well plate identifier": "96WP002"
            },
            "aspiration volume": {"value": 26, "unit": "μL"},
            "transfer volume": {"value": 26, "unit": "μL"}
          }
          ```

          **Pairing logic:**
          1. Match aspirate and dispense operations by probe number
          2. Create one measurement per matched pair
          3. Use `source_*` fields for aspirate location
          4. Use `destination_*` fields for dispense location
          5. Include both `aspiration volume` and `transfer volume`

          ### Quick Reference: Nesting Decision

          ```
          Is this field about...

          THE SAMPLE BEING MEASURED?
          ├── Sample ID, name, batch → sample document
          ├── Well position → sample document.location identifier
          ├── Plate barcode → sample document.well plate identifier
          └── Source/destination locations → sample document (with prefixes)

          INSTRUMENT SETTINGS?
          ├── Standard settings → device control aggregate document
          └── Vendor-specific → custom information document

          A MEASUREMENT VALUE?
          └── Direct on measurement document (e.g., absorbance, volume)

          TRANSFER OPERATION TYPE?
          └── DON'T use "transfer type" - pair into single measurement
              with source/destination fields instead
          ```

          ### Validation

          Use `validate_asm.py` to check for nesting issues:
          ```bash
          python scripts/validate_asm.py output.json --reference known_good.json
          ```

          The validator checks for:
          - Fields incorrectly flattened on measurements
          - Missing `sample document` wrapper
          - Missing `device control aggregate document` wrapper
          - Missing `custom information document` for vendor fields
          - Liquid handler: separate transfer types instead of paired records

          ## Sources

          - [Allotrope Simple Model Introduction](https://www.allotrope.org/introduction-to-allotrope-simple-model)
          - [Benchling allotropy library](https://github.com/Benchling-Open-Source/allotropy)
          - [Allotrope Foundation ASM Overview](https://www.allotrope.org/asm)
    - path: /knowledge-work-plugins/skills/instrument-data-to-allotrope/references/flattening_guide.md
      text: |
          # Flattening ASM to 2D CSV

          Converting hierarchical ASM JSON to flat 2D tables for LIMS import, spreadsheet analysis, or data engineering pipelines.

          ## Why Flatten?

          ASM is semantically rich but hierarchical. Many systems need flat tables:
          - LIMS import (Benchling, STARLIMS, LabWare)
          - Excel/CSV analysis
          - Database loading
          - Quick visual inspection

          ## Flattening Strategy

          ### Core Principle
          Each **measurement** becomes one **row**. Metadata is repeated per row.

          ### What's Excluded
          The flattening intentionally **omits top-level ASM metadata** such as:
          - `$asm.manifest` (model version, schema URIs)
          - Root-level fields outside the technique aggregate document

          This keeps the output focused on experimental data. If you need schema version tracking for compliance or audit purposes, consider storing the original ASM JSON alongside the flattened CSV, or modify the flattening script to include these fields.

          ### Hierarchy to Columns
          ```
          ASM Hierarchy                    → Flat Column
          ─────────────────────────────────────────────────
          device-system-document.
            device-identifier              → instrument_serial_number
            model-number                   → instrument_model
            
          measurement-aggregate-document.
            analyst                        → analyst
            measurement-time               → measurement_datetime
            
          measurement-document[].
            sample-identifier              → sample_id
            viable-cell-density.value      → viable_cell_density
            viable-cell-density.unit       → viable_cell_density_unit
            viability.value                → viability_percent
          ```

          ## Column Naming Convention

          Use snake_case with descriptive suffixes:

          | ASM Field | Flat Column |
          |-----------|-------------|
          | `viable-cell-density` | `viable_cell_density` |
          | `.value` | `_value` (or omit if obvious) |
          | `.unit` | `_unit` |
          | `measurement-time` | `measurement_datetime` |

          ## Example: Cell Counting

          ### ASM Input (simplified)
          ```json
          {
            "cell-counting-aggregate-document": {
              "device-system-document": {
                "device-identifier": "VCB001",
                "model-number": "Vi-CELL BLU"
              },
              "cell-counting-document": [{
                "measurement-aggregate-document": {
                  "analyst": "jsmith",
                  "measurement-time": "2024-01-15T10:30:00Z",
                  "measurement-document": [
                    {
                      "sample-identifier": "Sample_A",
                      "viable-cell-density": {"value": 2500000, "unit": "(cell/mL)"},
                      "viability": {"value": 95.2, "unit": "%"}
                    },
                    {
                      "sample-identifier": "Sample_B",
                      "viable-cell-density": {"value": 1800000, "unit": "(cell/mL)"},
                      "viability": {"value": 88.7, "unit": "%"}
                    }
                  ]
                }
              }]
            }
          }
          ```

          ### Flattened Output
          ```csv
          sample_id,viable_cell_density,viable_cell_density_unit,viability_percent,analyst,measurement_datetime,instrument_serial_number,instrument_model
          Sample_A,2500000,(cell/mL),95.2,jsmith,2024-01-15T10:30:00Z,VCB001,Vi-CELL BLU
          Sample_B,1800000,(cell/mL),88.7,jsmith,2024-01-15T10:30:00Z,VCB001,Vi-CELL BLU
          ```

          ## Example: Plate Reader

          ### ASM Input (simplified)
          ```json
          {
            "plate-reader-aggregate-document": {
              "plate-reader-document": [{
                "measurement-aggregate-document": {
                  "plate-identifier": "ELISA_001",
                  "measurement-document": [
                    {"well-location": "A1", "absorbance": {"value": 0.125, "unit": "mAU"}},
                    {"well-location": "A2", "absorbance": {"value": 0.892, "unit": "mAU"}},
                    {"well-location": "A3", "absorbance": {"value": 1.456, "unit": "mAU"}}
                  ]
                }
              }]
            }
          }
          ```

          ### Flattened Output
          ```csv
          plate_id,well_position,absorbance,absorbance_unit
          ELISA_001,A1,0.125,mAU
          ELISA_001,A2,0.892,mAU
          ELISA_001,A3,1.456,mAU
          ```

          ## Handling Data Cubes

          Data cubes (time series, spectra) need special handling:

          ### Option 1: Expand to rows
          Each point becomes a row:
          ```csv
          sample_id,time_seconds,absorbance
          Sample_A,0,0.100
          Sample_A,60,0.125
          Sample_A,120,0.150
          ```

          ### Option 2: Wide format
          Measurements as columns:
          ```csv
          sample_id,abs_0s,abs_60s,abs_120s
          Sample_A,0.100,0.125,0.150
          ```

          ### Option 3: JSON array in cell
          Keep as array (some systems support this):
          ```csv
          sample_id,absorbance_timeseries
          Sample_A,"[0.100,0.125,0.150]"
          ```

          ## Standard Column Sets by Technique

          ### Cell Counting
          ```
          sample_id, viable_cell_density, viable_cell_density_unit, total_cell_count,
          viability_percent, average_cell_diameter, average_cell_diameter_unit,
          analyst, measurement_datetime, instrument_serial_number
          ```

          ### Spectrophotometry
          ```
          sample_id, wavelength_nm, absorbance, pathlength_cm, concentration,
          concentration_unit, a260_a280_ratio, a260_a230_ratio,
          analyst, measurement_datetime, instrument_serial_number
          ```

          ### Plate Reader / ELISA
          ```
          plate_id, well_position, sample_type, sample_id, absorbance, absorbance_unit,
          concentration, concentration_unit, dilution_factor, cv_percent,
          analyst, measurement_datetime, instrument_serial_number
          ```

          ### qPCR
          ```
          sample_id, target_name, well_position, ct_value, ct_mean, ct_sd,
          quantity, quantity_unit, amplification_efficiency,
          analyst, measurement_datetime, instrument_serial_number
          ```

          ## Python Implementation

          ```python
          import json
          import pandas as pd

          def flatten_asm(asm_dict, technique="cell-counting"):
              """
              Flatten ASM JSON to pandas DataFrame.
              
              Args:
                  asm_dict: Parsed ASM JSON
                  technique: ASM technique type
                  
              Returns:
                  pandas DataFrame with one row per measurement
              """
              rows = []
              
              # Get aggregate document
              agg_key = f"{technique}-aggregate-document"
              agg_doc = asm_dict.get(agg_key, {})
              
              # Extract device info
              device = agg_doc.get("device-system-document", {})
              device_info = {
                  "instrument_serial_number": device.get("device-identifier"),
                  "instrument_model": device.get("model-number")
              }
              
              # Get technique documents
              doc_key = f"{technique}-document"
              for doc in agg_doc.get(doc_key, []):
                  meas_agg = doc.get("measurement-aggregate-document", {})
                  
                  # Extract common metadata
                  common = {
                      "analyst": meas_agg.get("analyst"),
                      "measurement_datetime": meas_agg.get("measurement-time"),
                      **device_info
                  }
                  
                  # Extract each measurement
                  for meas in meas_agg.get("measurement-document", []):
                      row = {**common}
                      
                      # Flatten measurement fields
                      for key, value in meas.items():
                          if isinstance(value, dict) and "value" in value:
                              # Value datum pattern
                              col = key.replace("-", "_")
                              row[col] = value["value"]
                              if "unit" in value:
                                  row[f"{col}_unit"] = value["unit"]
                          else:
                              row[key.replace("-", "_")] = value
                      
                      rows.append(row)
              
              return pd.DataFrame(rows)

          # Usage
          with open("asm_output.json") as f:
              asm = json.load(f)

          df = flatten_asm(asm, "cell-counting")
          df.to_csv("flattened_output.csv", index=False)
          ```

          ## LIMS Import Considerations

          When importing flattened data into a LIMS:
          - Match column names to your LIMS schema field names
          - Use ISO 8601 date format for timestamps
          - Ensure sample IDs match existing LIMS sample identifiers
          - Check if your LIMS expects units in separate columns or embedded in values
    - path: /knowledge-work-plugins/skills/instrument-data-to-allotrope/references/supported_instruments.md
      text: |
          # Supported Instruments

          ## What Can This Skill Convert?

          **Any instrument data that maps to an Allotrope schema can be converted.** The skill uses a tiered parsing approach:

          1. **Native allotropy parsers** (listed below) - Highest fidelity, validated against vendor-specific formats
          2. **Flexible fallback parser** - Handles any tabular data (CSV, Excel, TXT) by mapping columns to ASM fields
          3. **PDF extraction** - Extracts tables from PDFs, then applies flexible parsing

          If your instrument isn't listed below, the skill can still convert it as long as your data contains recognizable measurement fields (sample IDs, values, units, timestamps, etc.) that map to an ASM technique schema.

          ---

          ## Instruments with Native Allotropy Parsers

          The following instruments have optimized parsers in the allotropy library with their Vendor enum values.

          ## Cell Counting

          | Instrument | Vendor Enum | File Types |
          |------------|-------------|------------|
          | Beckman Coulter Vi-CELL BLU | `BECKMAN_VI_CELL_BLU` | .csv |
          | Beckman Coulter Vi-CELL XR | `BECKMAN_VI_CELL_XR` | .txt, .xls, .xlsx |
          | ChemoMetec NucleoView NC-200 | `CHEMOMETEC_NUCLEOVIEW` | .xlsx |
          | ChemoMetec NC-View | `CHEMOMETEC_NC_VIEW` | .xlsx |
          | Revvity Matrix | `REVVITY_MATRIX` | .csv |

          ## Spectrophotometry (UV-Vis)

          | Instrument | Vendor Enum | File Types |
          |------------|-------------|------------|
          | Thermo Fisher NanoDrop One | `THERMO_FISHER_NANODROP_ONE` | .csv, .xlsx |
          | Thermo Fisher NanoDrop Eight | `THERMO_FISHER_NANODROP_EIGHT` | .tsv, .txt |
          | Thermo Fisher NanoDrop 8000 | `THERMO_FISHER_NANODROP_8000` | .csv |
          | Unchained Labs Lunatic | `UNCHAINED_LABS_LUNATIC` | .csv, .xlsx |
          | Thermo Fisher Genesys 30 | `THERMO_FISHER_GENESYS30` | .csv |

          ## Plate Readers (Multi-mode, Absorbance, Fluorescence)

          | Instrument | Vendor Enum | File Types |
          |------------|-------------|------------|
          | Molecular Devices SoftMax Pro | `MOLDEV_SOFTMAX_PRO` | .txt |
          | PerkinElmer EnVision | `PERKIN_ELMER_ENVISION` | .csv |
          | Agilent Gen5 (BioTek) | `AGILENT_GEN5` | .xlsx |
          | Agilent Gen5 Image | `AGILENT_GEN5_IMAGE` | .xlsx |
          | BMG MARS (CLARIOstar) | `BMG_MARS` | .csv, .txt |
          | BMG LabTech Smart Control | `BMG_LABTECH_SMART_CONTROL` | .csv |
          | Thermo SkanIt | `THERMO_SKANIT` | .xlsx |
          | Revvity Kaleido | `REVVITY_KALEIDO` | .csv |
          | Tecan Magellan | `TECAN_MAGELLAN` | .xlsx |

          ## ELISA / Immunoassay

          | Instrument | Vendor Enum | File Types |
          |------------|-------------|------------|
          | Molecular Devices SoftMax Pro | `MOLDEV_SOFTMAX_PRO` | .txt |
          | MSD Discovery Workbench | `MSD_WORKBENCH` | .txt |
          | MSD Methodical Mind | `METHODICAL_MIND` | .xlsx |
          | BMG MARS | `BMG_MARS` | .csv, .txt |

          ## qPCR / PCR

          | Instrument | Vendor Enum | File Types |
          |------------|-------------|------------|
          | Applied Biosystems QuantStudio | `APPBIO_QUANTSTUDIO` | .xlsx |
          | Applied Biosystems QuantStudio Design & Analysis | `APPBIO_QUANTSTUDIO_DESIGNANALYSIS` | .xlsx, .csv |
          | Bio-Rad CFX Maestro | `BIORAD_CFX_MAESTRO` | .csv, .xlsx |
          | Roche LightCycler | `ROCHE_LIGHTCYCLER` | .txt |

          ## Chromatography (HPLC, LC)

          | Instrument | Vendor Enum | File Types |
          |------------|-------------|------------|
          | Waters Empower | `WATERS_EMPOWER` | .xml |
          | Thermo Fisher Chromeleon | `THERMO_FISHER_CHROMELEON` | .xml |
          | Agilent ChemStation | `AGILENT_CHEMSTATION` | .csv |

          ## Electrophoresis

          | Instrument | Vendor Enum | File Types |
          |------------|-------------|------------|
          | Agilent TapeStation | `AGILENT_TAPESTATION` | .csv |
          | PerkinElmer LabChip | `PERKIN_ELMER_LABCHIP` | .csv |

          ## Flow Cytometry

          | Instrument | Vendor Enum | File Types |
          |------------|-------------|------------|
          | BD Biosciences FACSDiva | `BD_BIOSCIENCES_FACSDIVA` | .xml |
          | FlowJo | `FLOWJO` | .wsp |

          ## Solution Analysis

          | Instrument | Vendor Enum | File Types |
          |------------|-------------|------------|
          | Roche Cedex BioHT | `ROCHE_CEDEX_BIOHT` | .xlsx |
          | Beckman Coulter Biomek | `BECKMAN_COULTER_BIOMEK` | .csv |

          ## Auto-Detection Patterns

          The skill attempts to identify instrument type from file contents using these patterns:

          ### Vi-CELL BLU
          - Column headers: "Sample ID", "Viable cells (x10^6 cells/mL)", "Viability (%)"
          - File structure: CSV with specific column order

          ### Vi-CELL XR
          - Column headers: "Sample", "Total cells/ml", "Viable cells/ml"
          - Multiple export formats supported

          ### NanoDrop
          - Column headers: "Sample Name", "Nucleic Acid Conc.", "A260", "A280"
          - 260/280 and 260/230 ratio columns

          ### Plate Readers (General)
          - Well identifiers (A1-H12 pattern)
          - "Plate", "Well", "Sample" columns
          - Block-based structure with metadata headers

          ### ELISA
          - Standard curve data with concentrations
          - OD/absorbance readings
          - Sample/blank/standard classification

          ## Using Vendor Enums

          ```python
          from allotropy.parser_factory import Vendor
          from allotropy.to_allotrope import allotrope_from_file

          # List all supported vendors
          for v in Vendor:
              print(f"{v.name}: {v.value}")

          # Convert file
          asm = allotrope_from_file("data.csv", Vendor.BECKMAN_VI_CELL_BLU)
          ```

          ## Checking Supported Status

          ```python
          from allotropy.parser_factory import get_parser

          # Check if a vendor/file combo is supported
          try:
              parser = get_parser(Vendor.BECKMAN_VI_CELL_BLU)
              print("Supported!")
          except Exception as e:
              print(f"Not supported: {e}")
          ```
    - path: /knowledge-work-plugins/skills/instrument-data-to-allotrope/requirements.txt
      text: |
          # Instrument Data to Allotrope Skill - Pinned Dependencies
          #
          # These versions are pinned for reproducibility and determinism.
          # All scientists using this skill should install these exact versions
          # to ensure identical ASM output from the same input files.
          #
          # Installation:
          #   pip install -r requirements.txt --break-system-packages
          #
          # Note: Versions pinned as of 2025-01-05

          # Core parsing library - provides native instrument parsers
          allotropy==0.1.55

          # Data manipulation and file reading
          pandas==2.0.3

          # Excel file support (required by pandas for .xlsx files)
          openpyxl==3.1.2

          # PDF parsing support (for instruments that export PDFs)
          pdfplumber==0.9.0

          # Scientific computing (optional, but recommended for advanced analysis)
          # numpy==1.24.3  # Uncomment if needed
          # scipy==1.11.1  # Uncomment if needed
    - path: /knowledge-work-plugins/skills/nextflow-development/LICENSE.txt
      text: |
          Apache License
          Version 2.0, January 2004
          http://www.apache.org/licenses/

          TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

          1. Definitions.

          "License" shall mean the terms and conditions for use, reproduction,
          and distribution as defined by Sections 1 through 9 of this document.

          "Licensor" shall mean the copyright owner or entity authorized by
          the copyright owner that is granting the License.

          "Legal Entity" shall mean the union of the acting entity and all
          other entities that control, are controlled by, or are under common
          control with that entity. For the purposes of this definition,
          "control" means (i) the power, direct or indirect, to cause the
          direction or management of such entity, whether by contract or
          otherwise, or (ii) ownership of fifty percent (50%) or more of the
          outstanding shares, or (iii) beneficial ownership of such entity.

          "You" (or "Your") shall mean an individual or Legal Entity
          exercising permissions granted by this License.

          "Source" form shall mean the preferred form for making modifications,
          including but not limited to software source code, documentation
          source, and configuration files.

          "Object" form shall mean any form resulting from mechanical
          transformation or translation of a Source form, including but
          not limited to compiled object code, generated documentation,
          and conversions to other media types.

          "Work" shall mean the work of authorship, whether in Source or
          Object form, made available under the License, as indicated by a
          copyright notice that is included in or attached to the work
          (an example is provided in the Appendix below).

          "Derivative Works" shall mean any work, whether in Source or Object
          form, that is based on (or derived from) the Work and for which the
          editorial revisions, annotations, elaborations, or other modifications
          represent, as a whole, an original work of authorship. For the purposes
          of this License, Derivative Works shall not include works that remain
          separable from, or merely link (or bind by name) to the interfaces of,
          the Work and Derivative Works thereof.

          "Contribution" shall mean any work of authorship, including
          the original version of the Work and any modifications or additions
          to that Work or Derivative Works thereof, that is intentionally
          submitted to Licensor for inclusion in the Work by the copyright owner
          or by an individual or Legal Entity authorized to submit on behalf of
          the copyright owner. For the purposes of this definition, "submitted"
          means any form of electronic, verbal, or written communication sent
          to the Licensor or its representatives, including but not limited to
          communication on electronic mailing lists, source code control systems,
          and issue tracking systems that are managed by, or on behalf of, the
          Licensor for the purpose of discussing and improving the Work, but
          excluding communication that is conspicuously marked or otherwise
          designated in writing by the copyright owner as "Not a Contribution."

          "Contributor" shall mean Licensor and any individual or Legal Entity
          on behalf of whom a Contribution has been received by Licensor and
          subsequently incorporated within the Work.

          2. Grant of Copyright License. Subject to the terms and conditions of
          this License, each Contributor hereby grants to You a perpetual,
          worldwide, non-exclusive, no-charge, royalty-free, irrevocable
          copyright license to reproduce, prepare Derivative Works of,
          publicly display, publicly perform, sublicense, and distribute the
          Work and such Derivative Works in Source or Object form.

          3. Grant of Patent License. Subject to the terms and conditions of
          this License, each Contributor hereby grants to You a perpetual,
          worldwide, non-exclusive, no-charge, royalty-free, irrevocable
          (except as stated in this section) patent license to make, have made,
          use, offer to sell, sell, import, and otherwise transfer the Work,
          where such license applies only to those patent claims licensable
          by such Contributor that are necessarily infringed by their
          Contribution(s) alone or by combination of their Contribution(s)
          with the Work to which such Contribution(s) was submitted. If You
          institute patent litigation against any entity (including a
          cross-claim or counterclaim in a lawsuit) alleging that the Work
          or a Contribution incorporated within the Work constitutes direct
          or contributory patent infringement, then any patent licenses
          granted to You under this License for that Work shall terminate
          as of the date such litigation is filed.

          4. Redistribution. You may reproduce and distribute copies of the
          Work or Derivative Works thereof in any medium, with or without
          modifications, and in Source or Object form, provided that You
          meet the following conditions:

          (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

          (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

          (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

          (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

          You may add Your own copyright statement to Your modifications and
          may provide additional or different license terms and conditions
          for use, reproduction, or distribution of Your modifications, or
          for any such Derivative Works as a whole, provided Your use,
          reproduction, and distribution of the Work otherwise complies with
          the conditions stated in this License.

          5. Submission of Contributions. Unless You explicitly state otherwise,
          any Contribution intentionally submitted for inclusion in the Work
          by You to the Licensor shall be under the terms and conditions of
          this License, without any additional terms or conditions.
          Notwithstanding the above, nothing herein shall supersede or modify
          the terms of any separate license agreement you may have executed
          with Licensor regarding such Contributions.

          6. Trademarks. This License does not grant permission to use the trade
          names, trademarks, service marks, or product names of the Licensor,
          except as required for reasonable and customary use in describing the
          origin of the Work and reproducing the content of the NOTICE file.

          7. Disclaimer of Warranty. Unless required by applicable law or
          agreed to in writing, Licensor provides the Work (and each
          Contributor provides its Contributions) on an "AS IS" BASIS,
          WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
          implied, including, without limitation, any warranties or conditions
          of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
          PARTICULAR PURPOSE. You are solely responsible for determining the
          appropriateness of using or redistributing the Work and assume any
          risks associated with Your exercise of permissions under this License.

          8. Limitation of Liability. In no event and under no legal theory,
          whether in tort (including negligence), contract, or otherwise,
          unless required by applicable law (such as deliberate and grossly
          negligent acts) or agreed to in writing, shall any Contributor be
          liable to You for damages, including any direct, indirect, special,
          incidental, or consequential damages of any character arising as a
          result of this License or out of the use or inability to use the
          Work (including but not limited to damages for loss of goodwill,
          work stoppage, computer failure or malfunction, or any and all
          other commercial damages or losses), even if such Contributor
          has been advised of the possibility of such damages.

          9. Accepting Warranty or Additional Liability. While redistributing
          the Work or Derivative Works thereof, You may choose to offer,
          and charge a fee for, acceptance of support, warranty, indemnity,
          or other liability obligations and/or rights consistent with this
          License. However, in accepting such obligations, You may act only
          on Your own behalf and on Your sole responsibility, not on behalf
          of any other Contributor, and only if You agree to indemnify,
          defend, and hold each Contributor harmless for any liability
          incurred by, or claims asserted against, such Contributor by reason
          of your accepting any such warranty or additional liability.

          END OF TERMS AND CONDITIONS

          APPENDIX: How to apply the Apache License to your work.

          To apply the Apache License to your work, attach the following
          boilerplate notice, with the fields enclosed by brackets "[]"
          replaced with your own identifying information. (Don't include
          the brackets!) The text should be enclosed in the appropriate
          comment syntax for the file format. We also recommend that a
          file or class name and description of purpose be included on the
          same "printed page" as the copyright notice for easier
          identification within third-party archives.

          Copyright [yyyy] [name of copyright owner]

          Licensed under the Apache License, Version 2.0 (the "License");
          you may not use this file except in compliance with the License.
          You may obtain a copy of the License at

          http://www.apache.org/licenses/LICENSE-2.0

          Unless required by applicable law or agreed to in writing, software
          distributed under the License is distributed on an "AS IS" BASIS,
          WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
          See the License for the specific language governing permissions and
          limitations under the License.
    - path: /knowledge-work-plugins/skills/nextflow-development/SKILL.md
      text: |
          ---
          name: nextflow-development
          description: Run nf-core bioinformatics pipelines (rnaseq, sarek, atacseq) on sequencing data. Use when analyzing RNA-seq, WGS/WES, or ATAC-seq data—either local FASTQs or public datasets from GEO/SRA. Triggers on nf-core, Nextflow, FASTQ analysis, variant calling, gene expression, differential expression, GEO reanalysis, GSE/GSM/SRR accessions, or samplesheet creation.
          ---

          # nf-core Pipeline Deployment

          Run nf-core bioinformatics pipelines on local or public sequencing data.

          **Target users:** Bench scientists and researchers without specialized bioinformatics training who need to run large-scale omics analyses—differential expression, variant calling, or chromatin accessibility analysis.

          ## Workflow Checklist

          ```
          - [ ] Step 0: Acquire data (if from GEO/SRA)
          - [ ] Step 1: Environment check (MUST pass)
          - [ ] Step 2: Select pipeline (confirm with user)
          - [ ] Step 3: Run test profile (MUST pass)
          - [ ] Step 4: Create samplesheet
          - [ ] Step 5: Configure & run (confirm genome with user)
          - [ ] Step 6: Verify outputs
          ```

          ---

          ## Step 0: Acquire Data (GEO/SRA Only)

          **Skip this step if user has local FASTQ files.**

          For public datasets, fetch from GEO/SRA first. See [references/geo-sra-acquisition.md](references/geo-sra-acquisition.md) for the full workflow.

          **Quick start:**

          ```bash
          # 1. Get study info
          python scripts/sra_geo_fetch.py info GSE110004

          # 2. Download (interactive mode)
          python scripts/sra_geo_fetch.py download GSE110004 -o ./fastq -i

          # 3. Generate samplesheet
          python scripts/sra_geo_fetch.py samplesheet GSE110004 --fastq-dir ./fastq -o samplesheet.csv
          ```

          **DECISION POINT:** After fetching study info, confirm with user:
          - Which sample subset to download (if multiple data types)
          - Suggested genome and pipeline

          Then continue to Step 1.

          ---

          ## Step 1: Environment Check

          **Run first. Pipeline will fail without passing environment.**

          ```bash
          python scripts/check_environment.py
          ```

          All critical checks must pass. If any fail, provide fix instructions:

          ### Docker issues

          | Problem | Fix |
          |---------|-----|
          | Not installed | Install from https://docs.docker.com/get-docker/ |
          | Permission denied | `sudo usermod -aG docker $USER` then re-login |
          | Daemon not running | `sudo systemctl start docker` |

          ### Nextflow issues

          | Problem | Fix |
          |---------|-----|
          | Not installed | `curl -s https://get.nextflow.io \| bash && mv nextflow ~/bin/` |
          | Version < 23.04 | `nextflow self-update` |

          ### Java issues

          | Problem | Fix |
          |---------|-----|
          | Not installed / < 11 | `sudo apt install openjdk-11-jdk` |

          **Do not proceed until all checks pass.** For HPC/Singularity, see [references/troubleshooting.md](references/troubleshooting.md).

          ---

          ## Step 2: Select Pipeline

          **DECISION POINT: Confirm with user before proceeding.**

          | Data Type | Pipeline | Version | Goal |
          |-----------|----------|---------|------|
          | RNA-seq | `rnaseq` | 3.22.2 | Gene expression |
          | WGS/WES | `sarek` | 3.7.1 | Variant calling |
          | ATAC-seq | `atacseq` | 2.1.2 | Chromatin accessibility |

          Auto-detect from data:
          ```bash
          python scripts/detect_data_type.py /path/to/data
          ```

          For pipeline-specific details:
          - [references/pipelines/rnaseq.md](references/pipelines/rnaseq.md)
          - [references/pipelines/sarek.md](references/pipelines/sarek.md)
          - [references/pipelines/atacseq.md](references/pipelines/atacseq.md)

          ---

          ## Step 3: Run Test Profile

          **Validates environment with small data. MUST pass before real data.**

          ```bash
          nextflow run nf-core/<pipeline> -r <version> -profile test,docker --outdir test_output
          ```

          | Pipeline | Command |
          |----------|---------|
          | rnaseq | `nextflow run nf-core/rnaseq -r 3.22.2 -profile test,docker --outdir test_rnaseq` |
          | sarek | `nextflow run nf-core/sarek -r 3.7.1 -profile test,docker --outdir test_sarek` |
          | atacseq | `nextflow run nf-core/atacseq -r 2.1.2 -profile test,docker --outdir test_atacseq` |

          Verify:
          ```bash
          ls test_output/multiqc/multiqc_report.html
          grep "Pipeline completed successfully" .nextflow.log
          ```

          If test fails, see [references/troubleshooting.md](references/troubleshooting.md).

          ---

          ## Step 4: Create Samplesheet

          ### Generate automatically

          ```bash
          python scripts/generate_samplesheet.py /path/to/data <pipeline> -o samplesheet.csv
          ```

          The script:
          - Discovers FASTQ/BAM/CRAM files
          - Pairs R1/R2 reads
          - Infers sample metadata
          - Validates before writing

          **For sarek:** Script prompts for tumor/normal status if not auto-detected.

          ### Validate existing samplesheet

          ```bash
          python scripts/generate_samplesheet.py --validate samplesheet.csv <pipeline>
          ```

          ### Samplesheet formats

          **rnaseq:**
          ```csv
          sample,fastq_1,fastq_2,strandedness
          SAMPLE1,/abs/path/R1.fq.gz,/abs/path/R2.fq.gz,auto
          ```

          **sarek:**
          ```csv
          patient,sample,lane,fastq_1,fastq_2,status
          patient1,tumor,L001,/abs/path/tumor_R1.fq.gz,/abs/path/tumor_R2.fq.gz,1
          patient1,normal,L001,/abs/path/normal_R1.fq.gz,/abs/path/normal_R2.fq.gz,0
          ```

          **atacseq:**
          ```csv
          sample,fastq_1,fastq_2,replicate
          CONTROL,/abs/path/ctrl_R1.fq.gz,/abs/path/ctrl_R2.fq.gz,1
          ```

          ---

          ## Step 5: Configure & Run

          ### 5a. Check genome availability

          ```bash
          python scripts/manage_genomes.py check <genome>
          # If not installed:
          python scripts/manage_genomes.py download <genome>
          ```

          Common genomes: GRCh38 (human), GRCh37 (legacy), GRCm39 (mouse), R64-1-1 (yeast), BDGP6 (fly)

          ### 5b. Decision points

          **DECISION POINT: Confirm with user:**

          1. **Genome:** Which reference to use
          2. **Pipeline-specific options:**
             - **rnaseq:** aligner (star_salmon recommended, hisat2 for low memory)
             - **sarek:** tools (haplotypecaller for germline, mutect2 for somatic)
             - **atacseq:** read_length (50, 75, 100, or 150)

          ### 5c. Run pipeline

          ```bash
          nextflow run nf-core/<pipeline> \
              -r <version> \
              -profile docker \
              --input samplesheet.csv \
              --outdir results \
              --genome <genome> \
              -resume
          ```

          **Key flags:**
          - `-r`: Pin version
          - `-profile docker`: Use Docker (or `singularity` for HPC)
          - `--genome`: iGenomes key
          - `-resume`: Continue from checkpoint

          **Resource limits (if needed):**
          ```bash
          --max_cpus 8 --max_memory '32.GB' --max_time '24.h'
          ```

          ---

          ## Step 6: Verify Outputs

          ### Check completion

          ```bash
          ls results/multiqc/multiqc_report.html
          grep "Pipeline completed successfully" .nextflow.log
          ```

          ### Key outputs by pipeline

          **rnaseq:**
          - `results/star_salmon/salmon.merged.gene_counts.tsv` - Gene counts
          - `results/star_salmon/salmon.merged.gene_tpm.tsv` - TPM values

          **sarek:**
          - `results/variant_calling/*/` - VCF files
          - `results/preprocessing/recalibrated/` - BAM files

          **atacseq:**
          - `results/macs2/narrowPeak/` - Peak calls
          - `results/bwa/mergedLibrary/bigwig/` - Coverage tracks

          ---

          ## Quick Reference

          For common exit codes and fixes, see [references/troubleshooting.md](references/troubleshooting.md).

          ### Resume failed run

          ```bash
          nextflow run nf-core/<pipeline> -resume
          ```

          ---

          ## References

          - [references/geo-sra-acquisition.md](references/geo-sra-acquisition.md) - Downloading public GEO/SRA data
          - [references/troubleshooting.md](references/troubleshooting.md) - Common issues and fixes
          - [references/installation.md](references/installation.md) - Environment setup
          - [references/pipelines/rnaseq.md](references/pipelines/rnaseq.md) - RNA-seq pipeline details
          - [references/pipelines/sarek.md](references/pipelines/sarek.md) - Variant calling details
          - [references/pipelines/atacseq.md](references/pipelines/atacseq.md) - ATAC-seq details

          ---

          ## Disclaimer

          This skill is provided as a prototype example demonstrating how to integrate nf-core bioinformatics pipelines into Claude Code for automated analysis workflows. The current implementation supports three pipelines (rnaseq, sarek, and atacseq), serving as a foundation that enables the community to expand support to the full set of nf-core pipelines.

          It is intended for educational and research purposes and should not be considered production-ready without appropriate validation for your specific use case. Users are responsible for ensuring their computing environment meets pipeline requirements and for verifying analysis results.

          Anthropic does not guarantee the accuracy of bioinformatics outputs, and users should follow standard practices for validating computational analyses. This integration is not officially endorsed by or affiliated with the nf-core community.

          ## Attribution

          When publishing results, cite the appropriate pipeline. Citations are available in each nf-core repository's CITATIONS.md file (e.g., https://github.com/nf-core/rnaseq/blob/3.22.2/CITATIONS.md).

          ## Licenses

          - **nf-core pipelines:** MIT License (https://nf-co.re/about)
          - **Nextflow:** Apache License, Version 2.0 (https://www.nextflow.io/about-us.html)
          - **NCBI SRA Toolkit:** Public Domain (https://github.com/ncbi/sra-tools/blob/master/LICENSE)
    - path: /knowledge-work-plugins/skills/nextflow-development/references/geo-sra-acquisition.md
      text: |
          # GEO/SRA Data Acquisition

          Download raw sequencing data from NCBI GEO/SRA and prepare it for nf-core pipelines.

          **Use this when:** Reanalyzing published datasets, validating findings, or comparing results against public cohorts.

          ## Table of Contents

          - [Workflow Overview](#workflow-overview)
          - [Step 1: Fetch Study Information](#step-1-fetch-study-information)
          - [Step 2: Review Sample Groups](#step-2-review-sample-groups)
          - [Step 3: Download FASTQ Files](#step-3-download-fastq-files)
          - [Step 4: Generate Samplesheet](#step-4-generate-samplesheet)
          - [Step 5: Run nf-core Pipeline](#step-5-run-nf-core-pipeline)
          - [Supported Pipelines](#supported-pipelines)
          - [Supported Organisms](#supported-organisms)
          - [Complete Example](#complete-example)
          - [Troubleshooting](#troubleshooting)

          ---

          ## Workflow Overview

          Example: "Find differentially expressed genes in GSE309891 (drug-treated vs control)"

          ```
          ┌─────────────────────────────────────────────────────────────────┐
          │                    GEO/SRA DATA ACQUISITION                     │
          └─────────────────────────────────────────────────────────────────┘
                                        │
                                        ▼
                           ┌────────────────────────┐
                           │   Fetch study info     │
                           │   • Query NCBI/SRA     │
                           │   • Get metadata       │
                           │   • Detect organism    │
                           │   • Identify data type │
                           └────────────────────────┘
                                        │
                                        ▼
                           ┌────────────────────────┐
                           │   Present summary      │
                           │   • Organism: Human    │
                           │   • Genome: GRCh38     │
                           │   • Type: RNA-Seq      │
                           │   • Pipeline: rnaseq   │
                           │   • Samples: 12        │
                           │     (6 treated,        │
                           │      6 control)        │
                           │   • Size: ~24 GB       │
                           └────────────────────────┘
                                        │
                                        ▼
                              ┌─────────────────┐
                              │  USER CONFIRMS  │◄──── Decision point
                              │  genome/pipeline│
                              └─────────────────┘
                                        │
                                        ▼
                           ┌────────────────────────┐
                           │   Select samples       │
                           │   • Group by condition │
                           │   • Show treated/ctrl  │
                           └────────────────────────┘
                                        │
                                        ▼
                              ┌─────────────────┐
                              │  USER SELECTS   │◄──── Decision point
                              │  sample subset  │
                              └─────────────────┘
                                        │
                                        ▼
                           ┌────────────────────────┐
                           │   Download FASTQs      │
                           │   • 24 files (R1+R2)   │
                           │   • Parallel transfers │
                           │   • Auto-resume        │
                           └────────────────────────┘
                                        │
                                        ▼
                           ┌────────────────────────┐
                           │   Generate samplesheet │
                           │   • Map SRR to files   │
                           │   • Pair R1/R2         │
                           │   • Assign conditions  │
                           └────────────────────────┘
                                        │
                                        ▼
          ┌─────────────────────────────────────────────────────────────────┐
          │                    NF-CORE PIPELINE EXECUTION                   │
          │              (Continue with Step 1 of main workflow)            │
          └─────────────────────────────────────────────────────────────────┘
          ```

          ---

          ## Instructions for Claude

          When assisting users with GEO/SRA data acquisition:

          1. **Always fetch study info first** to show the user what data is available
          2. **Ask for confirmation before downloading** - Present the sample groups and sizes, then ask which subset to download using AskUserQuestion
          3. **Suggest appropriate genome and pipeline** based on the organism and data type
          4. **Return to main SKILL.md workflow** after data preparation is complete

          Example confirmation question:
          ```
          Question: "Which sample group would you like to download?"
          Options:
            - "RNA-Seq:PAIRED (42 samples, ~87 GB)"
            - "RNA-Seq:SINGLE (7 samples, ~4.5 GB)"
            - "All samples (49 samples, ~92 GB)"
          ```

          ---

          ## Step 1: Fetch Study Information

          Get metadata about a GEO study before downloading.

          ```bash
          python scripts/sra_geo_fetch.py info <GEO_ID>
          ```

          **Example:**
          ```bash
          python scripts/sra_geo_fetch.py info GSE110004
          ```

          **Output includes:**
          - Study title and summary
          - Organism (with auto-suggested genome)
          - Number of samples and runs
          - Data types (RNA-Seq, ATAC-seq, etc.)
          - Estimated download size
          - Suggested nf-core pipeline

          **Save info to JSON:**
          ```bash
          python scripts/sra_geo_fetch.py info GSE110004 -o study_info.json
          ```

          ---

          ## Step 2: Review Sample Groups

          View sample groups organized by data type and layout. This is useful for studies with mixed data types.

          ```bash
          python scripts/sra_geo_fetch.py groups <GEO_ID>
          ```

          **Example output:**
          ```
          Sample Group          Count Layout     GSM Range                    Est. Size
          --------------------------------------------------------------------------------
          RNA-Seq                  42 PAIRED     GSM2879618...(42 samples)      87.4 GB
          RNA-Seq                   7 SINGLE     GSM2976181-GSM2976187           4.5 GB
          --------------------------------------------------------------------------------
          TOTAL                    49                                           91.9 GB

          Available groups for --subset option:
            1. "RNA-Seq:PAIRED" - 42 samples (~87.4 GB)
            2. "RNA-Seq:SINGLE" - 7 samples (~4.5 GB)
          ```

          **List individual runs:**
          ```bash
          python scripts/sra_geo_fetch.py list <GEO_ID>

          # Filter by data type
          python scripts/sra_geo_fetch.py list GSE110004 --filter "RNA-Seq:PAIRED"
          ```

          **DECISION POINT:** Review the sample groups. Decide which subset to download if the study has multiple data types.

          ---

          ## Step 3: Download FASTQ Files

          Download FASTQ files from ENA (faster than SRA).

          ```bash
          python scripts/sra_geo_fetch.py download <GEO_ID> -o <OUTPUT_DIR>
          ```

          **Options:**
          - `-o, --output`: Output directory (required)
          - `-i, --interactive`: Interactively select sample group to download
          - `-s, --subset`: Filter by data type (e.g., "RNA-Seq:PAIRED")
          - `-p, --parallel`: Parallel downloads (default: 4)
          - `-t, --timeout`: Download timeout in seconds (default: 600)

          ### Interactive Mode (Recommended)

          Use `-i` flag for interactive sample selection when the study has multiple data types:

          ```bash
          python scripts/sra_geo_fetch.py download GSE110004 -o ./fastq -i
          ```

          **Interactive output:**
          ```
          ============================================================
            SELECT SAMPLE GROUP TO DOWNLOAD
          ============================================================

            [1] RNA-Seq (paired)
                Samples: 42
                GSM: GSM2879618...(42 samples)
                Size: ~87.4 GB

            [2] RNA-Seq (single)
                Samples: 7
                GSM: GSM2976181-GSM2976187
                Size: ~4.5 GB

            [0] Download ALL (49 samples)
          ------------------------------------------------------------

          Enter selection (0-2):
          ```

          ### Direct Subset Selection

          Alternatively, specify the subset directly:

          ```bash
          # Download only RNA-Seq paired-end data
          python scripts/sra_geo_fetch.py download GSE110004 -o ./fastq \
              --subset "RNA-Seq:PAIRED" --parallel 6
          ```

          **Note:** Downloads automatically skip existing files. Resume interrupted downloads by re-running the command.

          ---

          ## Step 4: Generate Samplesheet

          Create a samplesheet compatible with nf-core pipelines.

          ```bash
          python scripts/sra_geo_fetch.py samplesheet <GEO_ID> \
              --fastq-dir <FASTQ_DIR> \
              -o samplesheet.csv
          ```

          **Options:**
          - `-f, --fastq-dir`: Directory containing downloaded FASTQ files (required)
          - `-o, --output`: Output samplesheet path (default: samplesheet.csv)
          - `-p, --pipeline`: Target pipeline (auto-detected if not specified)

          **Example:**
          ```bash
          python scripts/sra_geo_fetch.py samplesheet GSE110004 \
              --fastq-dir ./fastq \
              -o samplesheet.csv
          ```

          **Output:** The script will:
          1. Create samplesheet in the format required by the target pipeline
          2. Display suggested genome reference
          3. Show suggested nf-core command

          ---

          ## Step 5: Run nf-core Pipeline

          After generating the samplesheet, the script provides a suggested command.

          **Example output:**
          ```
          Suggested command:
             nextflow run nf-core/rnaseq \
                 --input samplesheet.csv \
                 --outdir results \
                 --genome R64-1-1 \
                 -profile docker
          ```

          **DECISION POINT:** Review and confirm:
          1. Is the suggested pipeline correct?
          2. Is the genome reference correct for your organism?
          3. Do you need additional pipeline options?

          Then return to the main SKILL.md workflow (Step 1: Environment Check) to proceed with pipeline execution.

          ---

          ## Supported Pipelines

          The skill auto-detects appropriate pipelines based on library strategy. Pipelines marked with ★ are fully supported with configs, samplesheet generation, and documentation. Others are suggested but require manual setup following nf-core documentation.

          | Library Strategy | Suggested Pipeline | Support |
          |------------------|--------------------|---------|
          | RNA-Seq          | nf-core/rnaseq     | ★ Full  |
          | ATAC-seq         | nf-core/atacseq    | ★ Full  |
          | WGS/WXS          | nf-core/sarek      | ★ Full  |
          | ChIP-seq         | nf-core/chipseq    | Manual  |
          | Bisulfite-Seq    | nf-core/methylseq  | Manual  |
          | miRNA-Seq        | nf-core/smrnaseq   | Manual  |
          | Amplicon         | nf-core/ampliseq   | Manual  |

          ---

          ## Supported Organisms

          Common organisms with auto-suggested genomes:

          | Organism | Genome | Notes |
          |----------|--------|-------|
          | Homo sapiens | GRCh38 | Human reference |
          | Mus musculus | GRCm39 | Mouse reference |
          | Saccharomyces cerevisiae | R64-1-1 | Yeast S288C |
          | Drosophila melanogaster | BDGP6 | Fruit fly |
          | Caenorhabditis elegans | WBcel235 | C. elegans |
          | Danio rerio | GRCz11 | Zebrafish |
          | Arabidopsis thaliana | TAIR10 | Arabidopsis |
          | Rattus norvegicus | Rnor_6.0 | Rat |

          See `scripts/config/genomes.yaml` for the full list.

          ---

          ## Complete Example

          Reanalyze GSE110004 (yeast RNA-seq):

          ```bash
          # 1. Get study info and sample groups
          python scripts/sra_geo_fetch.py info GSE110004

          # 2. Download with interactive selection
          python scripts/sra_geo_fetch.py download GSE110004 -o ./fastq -i
          # Select option [1] for RNA-Seq paired-end samples

          # 3. Generate samplesheet
          python scripts/sra_geo_fetch.py samplesheet GSE110004 \
              --fastq-dir ./fastq \
              -o samplesheet.csv

          # 4. Run nf-core/rnaseq (continue with main SKILL.md workflow)
          nextflow run nf-core/rnaseq \
              --input samplesheet.csv \
              --outdir results \
              --genome R64-1-1 \
              -profile docker
          ```

          ### Alternative: Non-interactive Download

          ```bash
          # Review sample groups first
          python scripts/sra_geo_fetch.py groups GSE110004

          # Download specific subset directly
          python scripts/sra_geo_fetch.py download GSE110004 \
              --subset "RNA-Seq:PAIRED" \
              -o ./fastq \
              --parallel 4
          ```

          ---

          ## Troubleshooting

          ### ENA Download Fails
          If ENA downloads fail, the data may need to be fetched directly from SRA:

          ```bash
          # Create SRA tools environment
          conda create -n sra_tools -c bioconda sra-tools

          # Download with prefetch + fasterq-dump
          conda run -n sra_tools prefetch SRR6357070
          conda run -n sra_tools fasterq-dump SRR6357070 -O ./fastq
          ```

          ### No SRA Runs Found
          Some GEO datasets only have processed data, not raw sequencing reads. Check:
          ```bash
          python scripts/sra_geo_fetch.py info <GEO_ID>
          ```
          If "Runs: 0", the dataset may not have raw data in SRA.

          ### SuperSeries Support
          GEO SuperSeries (which contain multiple SubSeries) are automatically handled. The tool will:
          1. Detect that a GEO ID is a SuperSeries
          2. Find the linked BioProject accession
          3. Fetch all SRA runs from the BioProject

          Example: GSE110004 is a SuperSeries that links to BioProject PRJNA432544.

          ### Genome Not Recognized
          If the organism is not in the genome mapping, manually specify the genome:
          ```bash
          # Check available iGenomes
          python scripts/manage_genomes.py list

          # Or provide custom reference files to nf-core
          nextflow run nf-core/rnaseq --fasta /path/to/genome.fa --gtf /path/to/genes.gtf
          ```

          ---

          ## Requirements

          - Python 3.8+
          - `requests` library (optional but recommended)
          - `pyyaml` library (optional, for genome config)
          - Network access to NCBI and ENA

          Install optional dependencies:
          ```bash
          pip install requests pyyaml
          ```
    - path: /knowledge-work-plugins/skills/nextflow-development/references/installation.md
      text: |
          # Installation

          ## Contents
          - [Quick install](#quick-install)
          - [Docker setup](#docker-setup)
          - [Singularity setup (HPC)](#singularity-setup-hpc)
          - [nf-core tools (optional)](#nf-core-tools-optional)
          - [Verify installation](#verify-installation)
          - [Common issues](#common-issues)

          ## Quick install

          ```bash
          # Nextflow
          curl -s https://get.nextflow.io | bash
          mv nextflow ~/bin/
          export PATH="$HOME/bin:$PATH"

          # Verify
          nextflow -version
          java -version  # Requires 11+
          ```

          ## Docker setup

          ### Linux
          ```bash
          sudo apt-get update && sudo apt-get install docker.io
          sudo systemctl enable --now docker
          sudo usermod -aG docker $USER
          # Log out and back in
          ```

          ### macOS
          Download Docker Desktop: https://docker.com/products/docker-desktop

          ### Verify
          ```bash
          docker run hello-world
          ```

          ## Singularity setup (HPC)

          ```bash
          # Ubuntu/Debian
          sudo apt-get install singularity-container

          # Or via conda
          conda install -c conda-forge singularity
          ```

          ### Configure cache
          ```bash
          export NXF_SINGULARITY_CACHEDIR="$HOME/.singularity/cache"
          mkdir -p $NXF_SINGULARITY_CACHEDIR
          echo 'export NXF_SINGULARITY_CACHEDIR="$HOME/.singularity/cache"' >> ~/.bashrc
          ```

          ## nf-core tools (optional)

          ```bash
          pip install nf-core
          ```

          Useful commands:
          ```bash
          nf-core list                    # Available pipelines
          nf-core launch rnaseq           # Interactive parameter selection
          nf-core download rnaseq -r 3.14.0  # Download for offline use
          ```

          ## Verify installation

          ```bash
          nextflow run nf-core/demo -profile test,docker --outdir test_demo
          ls test_demo/
          ```

          ## Common issues

          **Java version wrong:**
          ```bash
          export JAVA_HOME=/path/to/java11
          ```

          **Docker permission denied:**
          ```bash
          sudo usermod -aG docker $USER
          # Log out and back in
          ```

          **Nextflow not found:**
          ```bash
          echo 'export PATH="$HOME/bin:$PATH"' >> ~/.bashrc
          source ~/.bashrc
          ```
    - path: /knowledge-work-plugins/skills/nextflow-development/references/pipelines/atacseq.md
      text: |
          # nf-core/atacseq

          **Version:** 2.1.2

          **Official Documentation:** https://nf-co.re/atacseq/2.1.2/
          **GitHub:** https://github.com/nf-core/atacseq

          > **Note:** When updating to a new version, check the [releases page](https://github.com/nf-core/atacseq/releases) for breaking changes and update the version in commands below.

          ## Contents
          - [Test command](#test-command)
          - [Samplesheet format](#samplesheet-format)
          - [Parameters](#parameters)
          - [Output files](#output-files)
          - [Quality metrics](#quality-metrics)

          ## Test command

          ```bash
          nextflow run nf-core/atacseq -r 2.1.2 -profile test,docker --outdir test_atacseq
          ```

          Expected: ~15 min, creates peaks and BigWig tracks.

          ## Samplesheet format

          ```csv
          sample,fastq_1,fastq_2,replicate
          CONTROL,/path/to/ctrl_rep1_R1.fq.gz,/path/to/ctrl_rep1_R2.fq.gz,1
          CONTROL,/path/to/ctrl_rep2_R1.fq.gz,/path/to/ctrl_rep2_R2.fq.gz,2
          TREATMENT,/path/to/treat_rep1_R1.fq.gz,/path/to/treat_rep1_R2.fq.gz,1
          TREATMENT,/path/to/treat_rep2_R1.fq.gz,/path/to/treat_rep2_R2.fq.gz,2
          ```

          | Column | Required | Description |
          |--------|----------|-------------|
          | sample | Yes | Condition/group identifier |
          | fastq_1 | Yes | Absolute path to R1 |
          | fastq_2 | Yes | Absolute path to R2 (paired-end required) |
          | replicate | Yes | Replicate number (integer) |

          ### Design file for differential analysis
          ```csv
          sample,condition
          CONTROL,control
          TREATMENT,treatment
          ```

          Use with `--deseq2_design design.csv`.

          ## Parameters

          ### Minimal run
          ```bash
          nextflow run nf-core/atacseq -r 2.1.2 -profile docker \
              --input samplesheet.csv --outdir results --genome GRCh38 --read_length 50
          ```

          ### Common parameters

          | Parameter | Default | Description |
          |-----------|---------|-------------|
          | `--genome` | - | `GRCh38`, `GRCh37`, `mm10` |
          | `--read_length` | 50 | Read length for MACS2 optimization |
          | `--narrow_peak` | true | Narrow peaks (false for broad) |
          | `--mito_name` | chrM | Mitochondrial chromosome name |
          | `--keep_mito` | false | Keep mitochondrial reads |
          | `--min_reps_consensus` | 1 | Min replicates for consensus peaks |

          ### Differential accessibility
          ```bash
          --deseq2_design design.csv
          ```

          ## Output files

          ```
          results/
          ├── bwa/mergedLibrary/
          │   ├── *.mLb.mkD.sorted.bam     # Filtered, deduplicated alignments
          │   └── bigwig/
          │       └── *.bigWig             # Coverage tracks
          ├── macs2/narrowPeak/
          │   ├── *.narrowPeak             # Peak calls
          │   └── consensus/
          │       └── consensus_peaks.bed  # Merged peaks across replicates
          ├── deeptools/
          │   ├── plotFingerprint/         # Library complexity
          │   └── plotProfile/             # TSS enrichment
          ├── deseq2/                      # If --deseq2_design provided
          └── multiqc/
          ```

          **Key outputs:**
          - `*.mLb.mkD.sorted.bam`: Analysis-ready alignments
          - `*.narrowPeak`: MACS2 peak calls (BED format)
          - `consensus_peaks.bed`: Consensus peaks across replicates
          - `*.bigWig`: Genome browser tracks

          ## Quality metrics

          | Metric | Good | Acceptable | Poor |
          |--------|------|------------|------|
          | Mapped reads | >80% | 60-80% | <60% |
          | Mitochondrial | <20% | 20-40% | >40% |
          | Duplicates | <30% | 30-50% | >50% |
          | FRiP | >30% | 15-30% | <15% |
          | TSS enrichment | >6 | 4-6 | <4 |

          **Fragment size**: Should show nucleosomal periodicity (~50bp nucleosome-free, ~200bp mono-nucleosome).

          ## Downstream analysis

          ```r
          library(ChIPseeker)
          library(GenomicRanges)
          peaks <- import("consensus_peaks.bed")
          peakAnno <- annotatePeak(peaks, TxDb = TxDb.Hsapiens.UCSC.hg38.knownGene)
          ```

          **Motif analysis:**
          ```bash
          findMotifsGenome.pl consensus_peaks.bed hg38 motifs/ -size 200
          ```

          ## Troubleshooting

          **Low FRiP**: Check library complexity in `plotFingerprint/`. May indicate over-transposition.

          **Few peaks**: Lower threshold with `--macs_qvalue 0.1` or use `--narrow_peak false` for broader peaks.

          **High duplicates**: Normal for low-input; pipeline removes by default.

          ## More Information

          - **Full parameter list:** https://nf-co.re/atacseq/2.1.2/parameters/
          - **Output documentation:** https://nf-co.re/atacseq/2.1.2/docs/output/
          - **Usage documentation:** https://nf-co.re/atacseq/2.1.2/docs/usage/
    - path: /knowledge-work-plugins/skills/nextflow-development/references/pipelines/rnaseq.md
      text: |
          # nf-core/rnaseq

          **Version:** 3.22.2

          **Official Documentation:** https://nf-co.re/rnaseq/3.22.2/
          **GitHub:** https://github.com/nf-core/rnaseq

          > **Note:** When updating to a new version, check the [releases page](https://github.com/nf-core/rnaseq/releases) for breaking changes and update the version in commands below.

          ## Contents
          - [Test command](#test-command)
          - [Samplesheet format](#samplesheet-format)
          - [Parameters](#parameters)
          - [Output files](#output-files)
          - [Downstream analysis](#downstream-analysis)

          ## Test command

          ```bash
          nextflow run nf-core/rnaseq -r 3.22.2 -profile test,docker --outdir test_rnaseq
          ```

          Expected: ~15 min, creates `multiqc/multiqc_report.html`.

          ## Samplesheet format

          ```csv
          sample,fastq_1,fastq_2,strandedness
          CONTROL_REP1,/path/to/ctrl1_R1.fq.gz,/path/to/ctrl1_R2.fq.gz,auto
          CONTROL_REP2,/path/to/ctrl2_R1.fq.gz,/path/to/ctrl2_R2.fq.gz,auto
          TREATMENT_REP1,/path/to/treat1_R1.fq.gz,/path/to/treat1_R2.fq.gz,auto
          ```

          | Column | Required | Values |
          |--------|----------|--------|
          | sample | Yes | Alphanumeric, underscores allowed |
          | fastq_1 | Yes | Absolute path to R1 |
          | fastq_2 | No | Absolute path to R2 (empty for single-end) |
          | strandedness | Yes | `auto`, `forward`, `reverse`, `unstranded` |

          **Strandedness guide:**
          - `auto`: Inferred from data (recommended)
          - `forward`: TruSeq Stranded, dUTP protocols
          - `reverse`: Ligation-based protocols
          - `unstranded`: Non-stranded protocols

          ## Parameters

          ### Minimal run
          ```bash
          nextflow run nf-core/rnaseq -r 3.22.2 -profile docker \
              --input samplesheet.csv --outdir results --genome GRCh38
          ```

          ### Common parameters

          | Parameter | Default | Description |
          |-----------|---------|-------------|
          | `--aligner` | `star_salmon` | Options: `star_salmon`, `star_rsem`, `hisat2` |
          | `--genome` | - | `GRCh38`, `GRCh37`, `mm10`, `BDGP6` |
          | `--pseudo_aligner` | - | Set to `salmon` for pseudo-alignment only |
          | `--skip_trimming` | false | Skip adapter trimming |
          | `--skip_alignment` | false | Pseudo-alignment only |

          ### Custom reference
          ```bash
          --fasta /path/to/genome.fa \
          --gtf /path/to/annotation.gtf \
          --star_index /path/to/star/  # Optional, builds if absent
          ```

          ## Output files

          ```
          results/
          ├── star_salmon/
          │   ├── salmon.merged.gene_counts.tsv    # Raw counts for DESeq2
          │   ├── salmon.merged.gene_tpm.tsv       # TPM values
          │   └── *.bam                            # Alignments
          ├── multiqc/
          │   └── multiqc_report.html              # QC summary
          └── pipeline_info/
          ```

          **Key outputs:**
          - `salmon.merged.gene_counts.tsv`: Input for DESeq2/edgeR
          - `salmon.merged.gene_tpm.tsv`: Normalized expression

          ## Downstream analysis

          ```r
          library(DESeq2)
          counts <- read.delim("salmon.merged.gene_counts.tsv", row.names=1)
          coldata <- data.frame(
              condition = factor(c("control", "control", "treatment", "treatment"))
          )
          dds <- DESeqDataSetFromMatrix(
              countData = round(counts),
              colData = coldata,
              design = ~ condition
          )
          dds <- DESeq(dds)
          res <- results(dds, contrast = c("condition", "treatment", "control"))
          ```

          ## Troubleshooting

          **STAR index fails**: Increase memory with `--max_memory '64.GB'` or provide pre-built `--star_index`.

          **Low alignment rate**: Verify genome matches species; check FastQC for adapter contamination.

          **Strandedness detection fails**: Specify explicitly with `--strandedness reverse`.

          ## More Information

          - **Full parameter list:** https://nf-co.re/rnaseq/3.22.2/parameters/
          - **Output documentation:** https://nf-co.re/rnaseq/3.22.2/docs/output/
          - **Usage documentation:** https://nf-co.re/rnaseq/3.22.2/docs/usage/
    - path: /knowledge-work-plugins/skills/nextflow-development/references/pipelines/sarek.md
      text: |
          # nf-core/sarek

          **Version:** 3.7.1

          **Official Documentation:** https://nf-co.re/sarek/3.7.1/
          **GitHub:** https://github.com/nf-core/sarek

          > **Note:** When updating to a new version, check the [releases page](https://github.com/nf-core/sarek/releases) for breaking changes and update the version in commands below.

          ## Contents
          - [Test command](#test-command)
          - [Samplesheet format](#samplesheet-format)
          - [Variant calling modes](#variant-calling-modes)
          - [Parameters](#parameters)
          - [Output files](#output-files)

          ## Test command

          ```bash
          nextflow run nf-core/sarek -r 3.7.1 -profile test,docker --outdir test_sarek
          ```

          Expected: ~20 min, creates aligned BAMs and variant calls.

          ## Samplesheet format

          ### From FASTQ
          ```csv
          patient,sample,lane,fastq_1,fastq_2
          patient1,tumor,L001,/path/to/tumor_L001_R1.fq.gz,/path/to/tumor_L001_R2.fq.gz
          patient1,tumor,L002,/path/to/tumor_L002_R1.fq.gz,/path/to/tumor_L002_R2.fq.gz
          patient1,normal,L001,/path/to/normal_R1.fq.gz,/path/to/normal_R2.fq.gz
          ```

          ### From BAM/CRAM
          ```csv
          patient,sample,bam,bai
          patient1,tumor,/path/to/tumor.bam,/path/to/tumor.bam.bai
          patient1,normal,/path/to/normal.bam,/path/to/normal.bam.bai
          ```

          ### With tumor/normal status
          ```csv
          patient,sample,lane,fastq_1,fastq_2,status
          patient1,tumor,L001,tumor_R1.fq.gz,tumor_R2.fq.gz,1
          patient1,normal,L001,normal_R1.fq.gz,normal_R2.fq.gz,0
          ```

          `status`: 0 = normal, 1 = tumor

          ## Variant calling modes

          ### Germline (single sample)
          ```bash
          nextflow run nf-core/sarek -r 3.7.1 -profile docker \
              --input samplesheet.csv --outdir results --genome GRCh38 \
              --tools haplotypecaller,snpeff
          ```

          ### Somatic (tumor-normal pair)
          ```bash
          nextflow run nf-core/sarek -r 3.7.1 -profile docker \
              --input samplesheet.csv --outdir results --genome GRCh38 \
              --tools mutect2,strelka,snpeff
          ```

          ### WES (exome)
          ```bash
          nextflow run nf-core/sarek -r 3.7.1 -profile docker \
              --input samplesheet.csv --outdir results --genome GRCh38 \
              --wes --intervals /path/to/targets.bed \
              --tools haplotypecaller,snpeff
          ```

          ### Joint germline (cohort)
          ```bash
          --tools haplotypecaller --joint_germline
          ```

          ## Parameters

          ### Available tools

          **Germline callers:**
          - `haplotypecaller`: GATK HaplotypeCaller
          - `freebayes`: FreeBayes
          - `deepvariant`: DeepVariant (GPU optional)
          - `strelka`: Strelka2 germline

          **Somatic callers:**
          - `mutect2`: GATK Mutect2
          - `strelka`: Strelka2 somatic
          - `manta`: Structural variants

          **CNV callers:**
          - `ascat`: Copy number
          - `controlfreec`: CNV detection
          - `tiddit`: SV calling

          **Annotation:**
          - `snpeff`: Functional annotation
          - `vep`: Variant Effect Predictor

          ### Key parameters

          | Parameter | Default | Description |
          |-----------|---------|-------------|
          | `--tools` | - | Comma-separated list of tools |
          | `--genome` | - | `GRCh38`, `GRCh37` |
          | `--wes` | false | Exome mode (requires `--intervals`) |
          | `--intervals` | - | BED file for targeted regions |
          | `--joint_germline` | false | Joint calling for cohorts |
          | `--skip_bqsr` | false | Skip base quality recalibration |

          ## Output files

          ```
          results/
          ├── preprocessing/
          │   └── recalibrated/           # Analysis-ready BAMs
          │       └── *.recal.bam
          ├── variant_calling/
          │   ├── haplotypecaller/        # Germline VCFs
          │   ├── mutect2/                # Somatic VCFs (filtered)
          │   └── strelka/
          ├── annotation/
          │   └── snpeff/                 # Annotated VCFs
          └── multiqc/
          ```

          ## Troubleshooting

          **BQSR fails**: Check known sites available for genome. Skip with `--skip_bqsr` for non-standard references.

          **Mutect2 no variants**: Verify tumor/normal pairing in samplesheet (check `status` column).

          **Out of memory**: `--max_memory '128.GB'` for WGS.

          **DeepVariant GPU**: Ensure NVIDIA Docker runtime configured.

          ## More Information

          - **Full parameter list:** https://nf-co.re/sarek/3.7.1/parameters/
          - **Output documentation:** https://nf-co.re/sarek/3.7.1/docs/output/
          - **Usage documentation:** https://nf-co.re/sarek/3.7.1/docs/usage/
    - path: /knowledge-work-plugins/skills/nextflow-development/references/troubleshooting.md
      text: |
          # Troubleshooting

          Quick fixes for common nf-core pipeline issues.

          ## Contents
          - [Exit Codes](#exit-codes)
          - [HPC/Singularity Issues](#hpcsingularity-issues)
          - [Pipeline Failures](#pipeline-failures)
          - [RNA-seq Specific](#rna-seq-specific)
          - [Sarek Specific](#sarek-specific)
          - [ATAC-seq Specific](#atac-seq-specific)
          - [Resource Management](#resource-management)
          - [Getting Help](#getting-help)

          ## Exit Codes

          Common exit codes indicating resource issues (per [nf-core docs](https://nf-co.re/docs/usage/troubleshooting/crash_halfway)):

          | Code | Cause | Fix |
          |------|-------|-----|
          | 137 | Out of memory | `--max_memory '32.GB'` or `'64.GB'` for WGS |
          | 143 | Out of memory | `--max_memory '32.GB'` or `'64.GB'` for WGS |
          | 104, 134, 139, 247 | Out of memory | Increase `--max_memory` |
          | 1 | General error | Check `.nextflow.log` for details |

          Most pipelines auto-retry with 2x then 3x resources before failing.

          ## HPC/Singularity Issues

          ### Singularity cache issues
          ```bash
          export NXF_SINGULARITY_CACHEDIR="$HOME/.singularity/cache"
          mkdir -p $NXF_SINGULARITY_CACHEDIR
          ```

          ### Using Singularity instead of Docker
          On HPC systems without Docker, use Singularity:
          ```bash
          nextflow run nf-core/<pipeline> -profile singularity ...
          ```

          > **Note**: For basic environment setup (Docker, Nextflow, Java installation), see the inline instructions in Step 1 of SKILL.md.

          ## Pipeline Failures

          ### Container pull failed
          - Check network connectivity
          - Try: `-profile singularity` instead of docker
          - For offline: `nf-core download <pipeline> -r <version>`

          ### "No such file" errors
          - Use **absolute paths** in samplesheet
          - Verify files exist: `ls /path/to/file`

          ### Resume not working
          ```bash
          # Check work directory exists
          ls -la work/

          # Force clean restart (loses cache)
          rm -rf work/ .nextflow*
          nextflow run nf-core/<pipeline> ...
          ```

          ## RNA-seq Specific

          ### STAR index fails
          - Increase memory: `--max_memory '64.GB'`
          - Or provide pre-built: `--star_index /path/to/star/`

          ### Low alignment rate
          - Verify genome matches species
          - Check FastQC for adapter contamination
          - Try different aligner: `--aligner hisat2`

          ### Strandedness detection fails
          - Specify explicitly: `--strandedness reverse`
          - Common values: `forward`, `reverse`, `unstranded`

          ## Sarek Specific

          ### BQSR fails
          - Check known sites for genome
          - Skip for non-standard references: `--skip_bqsr`

          ### Mutect2 no variants
          - Verify tumor/normal pairing
          - Check samplesheet `status` column: 0=normal, 1=tumor

          ### Out of memory for WGS
          ```bash
          --max_memory '128.GB' --max_cpus 16
          ```

          ### DeepVariant GPU issues
          - Ensure NVIDIA Docker runtime configured
          - Or use CPU mode (slower)

          ## ATAC-seq Specific

          ### Low FRiP score
          - Check library complexity in `plotFingerprint/`
          - May indicate over-transposition

          ### Few peaks called
          - Lower threshold: `--macs_qvalue 0.1`
          - Use broad peaks: `--narrow_peak false`

          ### High duplicates
          - Normal for low-input samples
          - Pipeline removes by default
          - Consider deeper sequencing

          ## Resource Management

          ### Set resource limits
          ```bash
          --max_cpus 8 --max_memory '32.GB' --max_time '24.h'
          ```

          ### Check available resources
          ```bash
          # CPUs
          nproc

          # Memory
          free -h

          # Disk
          df -h .
          ```

          ## Getting Help

          1. Check `.nextflow.log` for error details
          2. Search nf-core Slack: https://nf-co.re/join
          3. Open issue on GitHub: https://github.com/nf-core/<pipeline>/issues
    - path: /knowledge-work-plugins/skills/nextflow-development/scripts/config/genomes.yaml
      text: |
          # Organism to Genome Mapping for nf-core Pipelines
          # Maps organism names (as they appear in GEO/SRA) to iGenomes keys

          organisms:
            # Human
            "Homo sapiens":
              genome: "GRCh38"
              taxid: 9606
              aliases: ["human", "hg38", "GRCh38"]
              notes: "Primary human reference genome"

            "Homo sapiens (legacy)":
              genome: "GRCh37"
              taxid: 9606
              aliases: ["hg19", "GRCh37"]
              notes: "Legacy human reference, still used for some clinical data"

            # Mouse
            "Mus musculus":
              genome: "GRCm39"
              taxid: 10090
              aliases: ["mouse", "mm39", "GRCm39"]
              notes: "Current mouse reference genome"

            "Mus musculus (legacy)":
              genome: "GRCm38"
              taxid: 10090
              aliases: ["mm10", "GRCm38"]
              notes: "Legacy mouse reference"

            # Yeast
            "Saccharomyces cerevisiae":
              genome: "R64-1-1"
              taxid: 4932
              aliases: ["yeast", "sacCer3", "S288C", "budding yeast"]
              notes: "S288C reference strain"

            # Fruit fly
            "Drosophila melanogaster":
              genome: "BDGP6"
              taxid: 7227
              aliases: ["fly", "dm6", "fruit fly", "Dmel"]
              notes: "Berkeley Drosophila Genome Project release 6"

            # Worm
            "Caenorhabditis elegans":
              genome: "WBcel235"
              taxid: 6239
              aliases: ["worm", "ce11", "C. elegans", "Cele"]
              notes: "WormBase reference"

            # Zebrafish
            "Danio rerio":
              genome: "GRCz11"
              taxid: 7955
              aliases: ["zebrafish", "danRer11", "Drer"]
              notes: "Genome Reference Consortium Zebrafish Build 11"

            # Arabidopsis
            "Arabidopsis thaliana":
              genome: "TAIR10"
              taxid: 3702
              aliases: ["arabidopsis", "thale cress", "Atha"]
              notes: "The Arabidopsis Information Resource v10"

            # Rat
            "Rattus norvegicus":
              genome: "Rnor_6.0"
              taxid: 10116
              aliases: ["rat", "rn6", "Rnor"]
              notes: "Rnor 6.0 reference"

            # Chicken
            "Gallus gallus":
              genome: "GRCg6a"
              taxid: 9031
              aliases: ["chicken", "galGal6", "Ggal"]
              notes: "Genome Reference Consortium Chicken Build 6a"

            # Pig
            "Sus scrofa":
              genome: "Sscrofa11.1"
              taxid: 9823
              aliases: ["pig", "susScr11", "Sscr"]
              notes: "Swine genome assembly 11.1"

            # Cow
            "Bos taurus":
              genome: "ARS-UCD1.2"
              taxid: 9913
              aliases: ["cow", "bosTau9", "cattle", "Btau"]
              notes: "USDA ARS assembly"

            # Dog
            "Canis lupus familiaris":
              genome: "CanFam3.1"
              taxid: 9615
              aliases: ["dog", "canFam3", "Clup"]
              notes: "Broad Institute CanFam3.1"

            # Frog
            "Xenopus tropicalis":
              genome: "JGI_4.2"
              taxid: 8364
              aliases: ["frog", "xenTro9", "Xtro"]
              notes: "JGI assembly version 4.2"

            # Maize/Corn
            "Zea mays":
              genome: "Zm-B73-REFERENCE-NAM-5.0"
              taxid: 4577
              aliases: ["maize", "corn", "Zmay"]
              notes: "B73 reference genome v5"

            # Rice
            "Oryza sativa":
              genome: "IRGSP-1.0"
              taxid: 39947
              aliases: ["rice", "Osat"]
              notes: "International Rice Genome Sequencing Project"

            # E. coli (common bacterial model)
            "Escherichia coli":
              genome: null
              taxid: 562
              aliases: ["E. coli", "Ecol"]
              notes: "Use specific strain reference; K-12 MG1655 common"

            # Fission yeast
            "Schizosaccharomyces pombe":
              genome: "ASM294v2"
              taxid: 4896
              aliases: ["fission yeast", "S. pombe", "Spom"]
              notes: "PomBase reference"

          # Pipeline mapping based on library strategy
          pipeline_suggestions:
            "RNA-SEQ": "rnaseq"
            "ATAC-SEQ": "atacseq"
            "CHIP-SEQ": "chipseq"
            "WGS": "sarek"
            "WXS": "sarek"
            "EXOME": "sarek"
            "AMPLICON": "ampliseq"
            "BISULFITE-SEQ": "methylseq"
            "HI-C": "hic"
            "MIRNA-SEQ": "smrnaseq"
            "RRBS": "methylseq"
    - path: /knowledge-work-plugins/skills/nextflow-development/scripts/config/pipelines/atacseq.yaml
      text: |
          name: atacseq
          version: "2.1.2"
          description: "Chromatin accessibility analysis and peak calling"

          # Documentation and source - NOTE: Update version in URLs when upgrading pipeline
          urls:
            documentation: "https://nf-co.re/atacseq/{version}/"
            parameters: "https://nf-co.re/atacseq/{version}/parameters/"
            output_docs: "https://nf-co.re/atacseq/{version}/docs/output/"
            github: "https://github.com/nf-core/atacseq"
            releases: "https://github.com/nf-core/atacseq/releases"

          data_types:
            - ATAC-seq
            - chromatin accessibility
            - open chromatin

          detection_hints:
            filename:
              - atac
              - atacseq
              - chromatin
              - accessibility
            directory:
              - atac
              - atacseq
              - chromatin
              - epigenome
              - epigenetics

          samplesheet:
            input_types:
              - fastq

            columns:
              - name: sample
                required: true
                type: string
                inference: filename
                description: "Condition/group identifier (replicates share same name)"

              - name: fastq_1
                required: true
                type: path
                inference: auto
                description: "Absolute path to R1 FASTQ"

              - name: fastq_2
                required: true
                type: path
                inference: auto
                description: "Absolute path to R2 FASTQ (paired-end required)"

              - name: replicate
                required: true
                type: integer
                default: 1
                inference: filename
                description: "Replicate number (integer)"

          decision_points:
            - parameter: genome
              prompt: "Which reference genome matches your organism?"
              options:
                - value: GRCh38
                  label: "Human GRCh38/hg38 (recommended)"
                  description: "Latest human reference"
                - value: GRCh37
                  label: "Human GRCh37/hg19 (legacy)"
                  description: "Older human reference"
                - value: mm10
                  label: "Mouse mm10"
                  description: "Mouse reference genome"
              default: GRCh38
              recommendation: "Default to GRCh38 for human samples"

            - parameter: read_length
              prompt: "What is the read length of your sequencing data?"
              options:
                - value: 50
                  label: "50 bp"
                  description: "Short reads"
                - value: 75
                  label: "75 bp"
                  description: "Standard length"
                - value: 100
                  label: "100 bp"
                  description: "Common for modern sequencers"
                - value: 150
                  label: "150 bp"
                  description: "Long reads"
              default: 50
              recommendation: "Check FASTQ files or sequencing report for exact length"

            - parameter: narrow_peak
              prompt: "What type of peaks are you expecting?"
              options:
                - value: "true"
                  label: "Narrow peaks (default for ATAC-seq)"
                  description: "Standard ATAC-seq open chromatin regions"
                - value: "false"
                  label: "Broad peaks"
                  description: "For histone marks or broader accessibility regions"
              default: "true"
              recommendation: "Use narrow peaks for standard ATAC-seq"

          test_profile:
            command: "nextflow run nf-core/atacseq -r 2.1.2 -profile test,docker --outdir test_atacseq"
            duration: "15 minutes"
            success_indicators:
              - "test_atacseq/multiqc/multiqc_report.html"
            log_pattern: "Pipeline completed successfully"

          run_command:
            template: |
              nextflow run nf-core/atacseq \
                  -r 2.1.2 \
                  -profile docker \
                  --input {samplesheet} \
                  --outdir {outdir} \
                  --genome {genome} \
                  --read_length {read_length} \
                  -resume

          outputs:
            primary:
              - path: "bwa/mergedLibrary/*.mLb.mkD.sorted.bam"
                description: "Filtered, deduplicated alignments"
              - path: "bwa/mergedLibrary/bigwig/*.bigWig"
                description: "Coverage tracks for genome browsers"
              - path: "macs2/narrowPeak/*.narrowPeak"
                description: "Peak calls (BED format)"
              - path: "macs2/narrowPeak/consensus/consensus_peaks.bed"
                description: "Consensus peaks across replicates"

            validation:
              - file: "multiqc/multiqc_report.html"
                check: exists
                description: "QC report must exist"
              - file: "macs2/narrowPeak"
                check: exists
                description: "Peak calls directory"

          quality_metrics:
            - name: mapped_reads
              good: ">80%"
              acceptable: "60-80%"
              poor: "<60%"
            - name: mitochondrial
              good: "<20%"
              acceptable: "20-40%"
              poor: ">40%"
            - name: duplicates
              good: "<30%"
              acceptable: "30-50%"
              poor: ">50%"
            - name: frip
              good: ">30%"
              acceptable: "15-30%"
              poor: "<15%"
            - name: tss_enrichment
              good: ">6"
              acceptable: "4-6"
              poor: "<4"

          resources:
            min_memory: "8.GB"
            recommended_memory: "32.GB"
            min_cpus: 4
            recommended_cpus: 8
            disk_space: "100.GB"

          troubleshooting:
            - error: "Low FRiP score"
              fix: "Check library complexity in plotFingerprint. May indicate over-transposition or low quality"
            - error: "Few peaks called"
              fix: "Lower threshold with --macs_qvalue 0.1 or use --narrow_peak false for broader peaks"
            - error: "High duplicates"
              fix: "Normal for low-input samples. Pipeline removes by default. Consider deeper sequencing"
            - error: "High mitochondrial reads"
              fix: "Sample quality issue. Pipeline filters mito by default (--keep_mito false)"

          replicate_patterns:
            - "_rep(\\d+)"
            - "_R(\\d+)_"
            - "_(\\d+)$"
            - "_replicate(\\d+)"
    - path: /knowledge-work-plugins/skills/nextflow-development/scripts/config/pipelines/rnaseq.yaml
      text: |
          name: rnaseq
          version: "3.22.2"
          description: "Gene expression quantification and differential expression analysis"

          # Documentation and source - NOTE: Update version in URLs when upgrading pipeline
          urls:
            documentation: "https://nf-co.re/rnaseq/{version}/"
            parameters: "https://nf-co.re/rnaseq/{version}/parameters/"
            output_docs: "https://nf-co.re/rnaseq/{version}/docs/output/"
            github: "https://github.com/nf-core/rnaseq"
            releases: "https://github.com/nf-core/rnaseq/releases"

          data_types:
            - RNA-seq
            - mRNA-seq
            - bulk RNA-seq

          detection_hints:
            filename:
              - rna
              - rnaseq
              - mrna
              - expression
            directory:
              - rnaseq
              - rna
              - expression
              - transcriptome

          samplesheet:
            input_types:
              - fastq

            columns:
              - name: sample
                required: true
                type: string
                inference: filename
                description: "Sample identifier"

              - name: fastq_1
                required: true
                type: path
                inference: auto
                description: "Absolute path to R1 FASTQ"

              - name: fastq_2
                required: false
                type: path
                inference: auto
                description: "Absolute path to R2 FASTQ (empty for single-end)"

              - name: strandedness
                required: true
                type: enum
                allowed:
                  - auto
                  - forward
                  - reverse
                  - unstranded
                default: "auto"
                inference: default
                description: "Library strandedness (auto recommended)"

          decision_points:
            - parameter: genome
              prompt: "Which reference genome matches your organism?"
              options:
                - value: GRCh38
                  label: "Human GRCh38/hg38 (recommended for human)"
                  description: "Latest human reference assembly"
                - value: GRCh37
                  label: "Human GRCh37/hg19 (legacy)"
                  description: "Older human reference for compatibility"
                - value: mm10
                  label: "Mouse mm10/GRCm38"
                  description: "Mouse reference genome"
                - value: BDGP6
                  label: "Drosophila BDGP6"
                  description: "Fruit fly reference"
              default: GRCh38
              recommendation: "Default to GRCh38 for human samples"

            - parameter: aligner
              prompt: "Which alignment strategy would you prefer?"
              options:
                - value: star_salmon
                  label: "STAR + Salmon (recommended)"
                  description: "Most accurate, standard for differential expression"
                - value: star_rsem
                  label: "STAR + RSEM"
                  description: "Better for isoform-level quantification"
                - value: hisat2
                  label: "HISAT2"
                  description: "Lower memory requirements, faster"
              default: star_salmon
              recommendation: "Use star_salmon unless memory-constrained or need isoforms"

          test_profile:
            command: "nextflow run nf-core/rnaseq -r 3.22.2 -profile test,docker --outdir test_rnaseq"
            duration: "15 minutes"
            success_indicators:
              - "test_rnaseq/multiqc/multiqc_report.html"
            log_pattern: "Pipeline completed successfully"

          run_command:
            template: |
              nextflow run nf-core/rnaseq \
                  -r 3.22.2 \
                  -profile docker \
                  --input {samplesheet} \
                  --outdir {outdir} \
                  --genome {genome} \
                  --aligner {aligner} \
                  -resume

          outputs:
            primary:
              - path: "star_salmon/salmon.merged.gene_counts.tsv"
                description: "Raw gene counts for DESeq2/edgeR"
              - path: "star_salmon/salmon.merged.gene_tpm.tsv"
                description: "TPM normalized expression values"
              - path: "star_salmon/*.bam"
                description: "Aligned reads"

            validation:
              - file: "multiqc/multiqc_report.html"
                check: exists
                description: "QC report must exist"
              - file: "star_salmon/salmon.merged.gene_counts.tsv"
                check: non_empty
                description: "Count matrix must have data"

          resources:
            min_memory: "8.GB"
            recommended_memory: "32.GB"
            min_cpus: 4
            recommended_cpus: 8
            disk_space: "100.GB"

          troubleshooting:
            - error: "STAR index fails"
              fix: "Increase memory with --max_memory '64.GB' or provide pre-built --star_index"
            - error: "Low alignment rate"
              fix: "Verify genome matches species; check FastQC for adapter contamination"
            - error: "Strandedness detection fails"
              fix: "Specify explicitly with --strandedness reverse (or forward/unstranded)"
    - path: /knowledge-work-plugins/skills/nextflow-development/scripts/config/pipelines/sarek.yaml
      text: |
          name: sarek
          version: "3.7.1"
          description: "Variant calling for WGS/WES data (germline and somatic)"

          # Documentation and source - NOTE: Update version in URLs when upgrading pipeline
          urls:
            documentation: "https://nf-co.re/sarek/{version}/"
            parameters: "https://nf-co.re/sarek/{version}/parameters/"
            output_docs: "https://nf-co.re/sarek/{version}/docs/output/"
            github: "https://github.com/nf-core/sarek"
            releases: "https://github.com/nf-core/sarek/releases"

          data_types:
            - WGS
            - WES
            - whole genome sequencing
            - whole exome sequencing
            - tumor-normal
            - germline
            - somatic

          detection_hints:
            filename:
              - tumor
              - normal
              - germline
              - wgs
              - wes
              - exome
              - dna
              - variant
            directory:
              - variant
              - wgs
              - wes
              - exome
              - germline
              - somatic

          samplesheet:
            input_types:
              - fastq
              - bam
              - cram

            columns:
              - name: patient
                required: true
                type: string
                inference: filename
                description: "Patient/subject identifier for grouping samples"

              - name: sample
                required: true
                type: string
                inference: filename
                description: "Sample identifier (e.g., tumor, normal)"

              - name: lane
                required: false
                type: string
                default: "L001"
                inference: filename
                description: "Sequencing lane"

              - name: fastq_1
                required: true
                type: path
                inference: auto
                condition: "input_type == 'fastq'"
                description: "Absolute path to R1 FASTQ"

              - name: fastq_2
                required: false
                type: path
                inference: auto
                condition: "input_type == 'fastq'"
                description: "Absolute path to R2 FASTQ"

              - name: bam
                required: true
                type: path
                inference: auto
                condition: "input_type in ['bam', 'cram']"
                description: "Absolute path to BAM/CRAM file"

              - name: bai
                required: true
                type: path
                inference: auto
                condition: "input_type in ['bam', 'cram']"
                description: "Absolute path to BAM/CRAM index"

              - name: status
                required: false
                type: integer
                allowed:
                  - 0
                  - 1
                default: 0
                inference: filename
                description: "0=normal, 1=tumor (critical for somatic calling)"

          decision_points:
            - parameter: genome
              prompt: "Which reference genome should be used?"
              options:
                - value: GRCh38
                  label: "Human GRCh38/hg38 (recommended)"
                  description: "Latest human reference with most annotation support"
                - value: GRCh37
                  label: "Human GRCh37/hg19 (legacy)"
                  description: "For compatibility with older datasets"
                - value: mm10
                  label: "Mouse mm10"
                  description: "Mouse reference genome"
              default: GRCh38
              recommendation: "Default to GRCh38 for human data"

            - parameter: tools
              prompt: "What type of variant calling do you need?"
              options:
                - value: "haplotypecaller,snpeff"
                  label: "Germline variants (single samples)"
                  description: "For finding inherited variants in normal samples"
                  condition: "no tumor samples detected"
                - value: "mutect2,strelka,snpeff"
                  label: "Somatic variants (tumor-normal pairs)"
                  description: "For finding cancer mutations with matched normal"
                  condition: "tumor-normal pairs detected"
                - value: "haplotypecaller,deepvariant,snpeff"
                  label: "Germline with DeepVariant"
                  description: "Higher accuracy germline calling (requires GPU)"
                - value: "mutect2,manta,snpeff"
                  label: "Somatic with structural variants"
                  description: "Comprehensive tumor analysis including SVs"
              default: "haplotypecaller,snpeff"
              recommendation: "Use somatic tools if tumor/normal pairs detected, otherwise germline"

            - parameter: wes
              prompt: "Is this whole exome sequencing (WES) data?"
              options:
                - value: "false"
                  label: "No - Whole Genome Sequencing (WGS)"
                  description: "Full genome coverage"
                - value: "true"
                  label: "Yes - Whole Exome Sequencing (WES)"
                  description: "Requires --intervals BED file"
              default: "false"
              recommendation: "If WES, user must provide intervals BED file"

          test_profile:
            command: "nextflow run nf-core/sarek -r 3.7.1 -profile test,docker --outdir test_sarek"
            duration: "20 minutes"
            success_indicators:
              - "test_sarek/multiqc/multiqc_report.html"
            log_pattern: "Pipeline completed successfully"

          run_command:
            template: |
              nextflow run nf-core/sarek \
                  -r 3.7.1 \
                  -profile docker \
                  --input {samplesheet} \
                  --outdir {outdir} \
                  --genome {genome} \
                  --tools {tools} \
                  -resume

            wes_template: |
              nextflow run nf-core/sarek \
                  -r 3.7.1 \
                  -profile docker \
                  --input {samplesheet} \
                  --outdir {outdir} \
                  --genome {genome} \
                  --tools {tools} \
                  --wes \
                  --intervals {intervals} \
                  -resume

          outputs:
            primary:
              - path: "preprocessing/recalibrated/*.recal.bam"
                description: "Analysis-ready BAM files"
              - path: "variant_calling/*/*.vcf.gz"
                description: "Variant call files"
              - path: "annotation/snpeff/*.ann.vcf.gz"
                description: "Annotated variants"

            validation:
              - file: "multiqc/multiqc_report.html"
                check: exists
                description: "QC report must exist"
              - file: "preprocessing/recalibrated"
                check: exists
                description: "Recalibrated BAMs directory"

          resources:
            min_memory: "16.GB"
            recommended_memory: "64.GB"
            wgs_memory: "128.GB"
            min_cpus: 4
            recommended_cpus: 16
            disk_space: "500.GB"

          troubleshooting:
            - error: "BQSR fails"
              fix: "Check known sites available for genome. Skip with --skip_bqsr for non-standard references"
            - error: "Mutect2 no variants"
              fix: "Verify tumor/normal pairing in samplesheet (check status column: 0=normal, 1=tumor)"
            - error: "Out of memory"
              fix: "--max_memory '128.GB' for WGS data"
            - error: "DeepVariant GPU issues"
              fix: "Ensure NVIDIA Docker runtime configured, or use CPU mode"

          tumor_normal_keywords:
            tumor:
              - tumor
              - tumour
              - met
              - metastasis
              - primary
              - cancer
              - malignant
            normal:
              - normal
              - germline
              - blood
              - pbmc
              - control
              - healthy
              - matched
    - path: /knowledge-work-plugins/skills/scientific-problem-selection/LICENSE.txt
      text: |
          Apache License
          Version 2.0, January 2004
          http://www.apache.org/licenses/

          TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

          1. Definitions.

          "License" shall mean the terms and conditions for use, reproduction,
          and distribution as defined by Sections 1 through 9 of this document.

          "Licensor" shall mean the copyright owner or entity authorized by
          the copyright owner that is granting the License.

          "Legal Entity" shall mean the union of the acting entity and all
          other entities that control, are controlled by, or are under common
          control with that entity. For the purposes of this definition,
          "control" means (i) the power, direct or indirect, to cause the
          direction or management of such entity, whether by contract or
          otherwise, or (ii) ownership of fifty percent (50%) or more of the
          outstanding shares, or (iii) beneficial ownership of such entity.

          "You" (or "Your") shall mean an individual or Legal Entity
          exercising permissions granted by this License.

          "Source" form shall mean the preferred form for making modifications,
          including but not limited to software source code, documentation
          source, and configuration files.

          "Object" form shall mean any form resulting from mechanical
          transformation or translation of a Source form, including but
          not limited to compiled object code, generated documentation,
          and conversions to other media types.

          "Work" shall mean the work of authorship, whether in Source or
          Object form, made available under the License, as indicated by a
          copyright notice that is included in or attached to the work
          (an example is provided in the Appendix below).

          "Derivative Works" shall mean any work, whether in Source or Object
          form, that is based on (or derived from) the Work and for which the
          editorial revisions, annotations, elaborations, or other modifications
          represent, as a whole, an original work of authorship. For the purposes
          of this License, Derivative Works shall not include works that remain
          separable from, or merely link (or bind by name) to the interfaces of,
          the Work and Derivative Works thereof.

          "Contribution" shall mean any work of authorship, including
          the original version of the Work and any modifications or additions
          to that Work or Derivative Works thereof, that is intentionally
          submitted to Licensor for inclusion in the Work by the copyright owner
          or by an individual or Legal Entity authorized to submit on behalf of
          the copyright owner. For the purposes of this definition, "submitted"
          means any form of electronic, verbal, or written communication sent
          to the Licensor or its representatives, including but not limited to
          communication on electronic mailing lists, source code control systems,
          and issue tracking systems that are managed by, or on behalf of, the
          Licensor for the purpose of discussing and improving the Work, but
          excluding communication that is conspicuously marked or otherwise
          designated in writing by the copyright owner as "Not a Contribution."

          "Contributor" shall mean Licensor and any individual or Legal Entity
          on behalf of whom a Contribution has been received by Licensor and
          subsequently incorporated within the Work.

          2. Grant of Copyright License. Subject to the terms and conditions of
          this License, each Contributor hereby grants to You a perpetual,
          worldwide, non-exclusive, no-charge, royalty-free, irrevocable
          copyright license to reproduce, prepare Derivative Works of,
          publicly display, publicly perform, sublicense, and distribute the
          Work and such Derivative Works in Source or Object form.

          3. Grant of Patent License. Subject to the terms and conditions of
          this License, each Contributor hereby grants to You a perpetual,
          worldwide, non-exclusive, no-charge, royalty-free, irrevocable
          (except as stated in this section) patent license to make, have made,
          use, offer to sell, sell, import, and otherwise transfer the Work,
          where such license applies only to those patent claims licensable
          by such Contributor that are necessarily infringed by their
          Contribution(s) alone or by combination of their Contribution(s)
          with the Work to which such Contribution(s) was submitted. If You
          institute patent litigation against any entity (including a
          cross-claim or counterclaim in a lawsuit) alleging that the Work
          or a Contribution incorporated within the Work constitutes direct
          or contributory patent infringement, then any patent licenses
          granted to You under this License for that Work shall terminate
          as of the date such litigation is filed.

          4. Redistribution. You may reproduce and distribute copies of the
          Work or Derivative Works thereof in any medium, with or without
          modifications, and in Source or Object form, provided that You
          meet the following conditions:

          (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

          (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

          (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

          (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

          You may add Your own copyright statement to Your modifications and
          may provide additional or different license terms and conditions
          for use, reproduction, or distribution of Your modifications, or
          for any such Derivative Works as a whole, provided Your use,
          reproduction, and distribution of the Work otherwise complies with
          the conditions stated in this License.

          5. Submission of Contributions. Unless You explicitly state otherwise,
          any Contribution intentionally submitted for inclusion in the Work
          by You to the Licensor shall be under the terms and conditions of
          this License, without any additional terms or conditions.
          Notwithstanding the above, nothing herein shall supersede or modify
          the terms of any separate license agreement you may have executed
          with Licensor regarding such Contributions.

          6. Trademarks. This License does not grant permission to use the trade
          names, trademarks, service marks, or product names of the Licensor,
          except as required for reasonable and customary use in describing the
          origin of the Work and reproducing the content of the NOTICE file.

          7. Disclaimer of Warranty. Unless required by applicable law or
          agreed to in writing, Licensor provides the Work (and each
          Contributor provides its Contributions) on an "AS IS" BASIS,
          WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
          implied, including, without limitation, any warranties or conditions
          of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
          PARTICULAR PURPOSE. You are solely responsible for determining the
          appropriateness of using or redistributing the Work and assume any
          risks associated with Your exercise of permissions under this License.

          8. Limitation of Liability. In no event and under no legal theory,
          whether in tort (including negligence), contract, or otherwise,
          unless required by applicable law (such as deliberate and grossly
          negligent acts) or agreed to in writing, shall any Contributor be
          liable to You for damages, including any direct, indirect, special,
          incidental, or consequential damages of any character arising as a
          result of this License or out of the use or inability to use the
          Work (including but not limited to damages for loss of goodwill,
          work stoppage, computer failure or malfunction, or any and all
          other commercial damages or losses), even if such Contributor
          has been advised of the possibility of such damages.

          9. Accepting Warranty or Additional Liability. While redistributing
          the Work or Derivative Works thereof, You may choose to offer,
          and charge a fee for, acceptance of support, warranty, indemnity,
          or other liability obligations and/or rights consistent with this
          License. However, in accepting such obligations, You may act only
          on Your own behalf and on Your sole responsibility, not on behalf
          of any other Contributor, and only if You agree to indemnify,
          defend, and hold each Contributor harmless for any liability
          incurred by, or claims asserted against, such Contributor by reason
          of your accepting any such warranty or additional liability.

          END OF TERMS AND CONDITIONS

          APPENDIX: How to apply the Apache License to your work.

          To apply the Apache License to your work, attach the following
          boilerplate notice, with the fields enclosed by brackets "[]"
          replaced with your own identifying information. (Don't include
          the brackets!) The text should be enclosed in the appropriate
          comment syntax for the file format. We also recommend that a
          file or class name and description of purpose be included on the
          same "printed page" as the copyright notice for easier
          identification within third-party archives.

          Copyright [yyyy] [name of copyright owner]

          Licensed under the Apache License, Version 2.0 (the "License");
          you may not use this file except in compliance with the License.
          You may obtain a copy of the License at

          http://www.apache.org/licenses/LICENSE-2.0

          Unless required by applicable law or agreed to in writing, software
          distributed under the License is distributed on an "AS IS" BASIS,
          WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
          See the License for the specific language governing permissions and
          limitations under the License.
    - path: /knowledge-work-plugins/skills/scientific-problem-selection/SKILL.md
      text: |
          ---
          name: scientific-problem-selection
          description: This skill should be used when scientists need help with research problem selection, project ideation, troubleshooting stuck projects, or strategic scientific decisions. Use this skill when users ask to pitch a new research idea, work through a project problem, evaluate project risks, plan research strategy, navigate decision trees, or get help choosing what scientific problem to work on. Typical requests include "I have an idea for a project", "I'm stuck on my research", "help me evaluate this project", "what should I work on", or "I need strategic advice about my research".
          ---

          # Scientific Problem Selection Skills

          A conversational framework for systematic scientific problem selection based on Fischbach & Walsh's "Problem choice and decision trees in science and engineering" (Cell, 2024).

          ## Getting Started

          Present users with three entry points:

          **1) Pitch an idea for a new project** — to work it up together

          **2) Share a problem in a current project** — to troubleshoot together

          **3) Ask a strategic question** — to navigate the decision tree together

          This conversational entry meets scientists where they are and establishes a collaborative tone.

          ---

          ## Option 1: Pitch an Idea

          ### Initial Prompt
          Ask: **"Tell me the short version of your idea (1-2 sentences)."**

          ### Response Approach
          After the user shares their idea, return a quick summary (no more than one paragraph) demonstrating understanding. Note the general area of research and rephrase the idea in a way that highlights its kernel—showing alignment and readiness to dive into details.

          ### Follow-up Prompt
          Then ask for more detail: "Now give me a bit more detail. You might include, however briefly or even say where you are unsure:
          1. What exactly you want to do
          2. How you currently plan to do it
          3. If it works, why will it be a big deal
          4. What you think are the major risks"

          ### Workflow
          From there, guide the user through the early stages of problem selection and evaluation:
          - **Skill 1: Intuition Pumps** - Refine and strengthen the idea
          - **Skill 2: Risk Assessment** - Identify and manage project risks
          - **Skill 3: Optimization Function** - Define success metrics
          - **Skill 4: Parameter Strategy** - Determine what to fix vs. keep flexible

          See `references/01-intuition-pumps.md`, `references/02-risk-assessment.md`, `references/03-optimization-function.md`, and `references/04-parameter-strategy.md` for detailed guidance.

          ---

          ## Option 2: Troubleshoot a Problem

          ### Initial Prompt
          Ask: **"Tell me a short version of your problem (1-2 sentences or whatever is easy)."**

          ### Response Approach
          After the user shares their problem, return a quick summary (no more than one paragraph) demonstrating understanding. Note the context of the project where the problem occurred and rephrase the problem—highlighting its core essence—so the user knows the situation is understood. Also raise additional questions that seem important to discuss.

          ### Follow-up Prompt
          Then ask: "Now give me a bit more detail. You might include, however briefly:
          1. The overall goal of your project (if we have not talked about it before)
          2. What exactly went wrong
          3. Your current ideas for fixing it"

          ### Workflow
          From there, guide the user through troubleshooting and decision tree navigation:
          - **Skill 5: Decision Tree Navigation** - Plan decision points and navigate between execution and strategic thinking
          - **Skill 4: Parameter Strategy** - Fix one parameter at a time, let others float
          - **Skill 6: Adversity Response** - Frame problems as opportunities for growth
          - **Skill 7: Problem Inversion** - Strategies for navigating around obstacles

          Always include workarounds that might be useful whether or not the problem can be fixed easily.

          See `references/05-decision-tree.md`, `references/06-adversity-planning.md`, `references/07-problem-inversion.md`, and `references/04-parameter-strategy.md` for detailed guidance.

          ---

          ## Option 3: Ask a Strategic Question

          ### Initial Prompt
          Ask: **"Tell me the short version of your question (1-2 sentences)."**

          ### Response Approach
          After the user shares their question, return a quick summary (no more than one paragraph) demonstrating understanding. Note the broader context and rephrase the question—highlighting its crux—to confirm alignment with their thinking.

          ### Follow-up Prompt
          Then ask: "Now give me a bit more detail. You might include, however briefly:
          1. The setting (i.e., is this about a current or future project)
          2. A bit more detail about what you're thinking"

          ### Workflow
          From there, draw on the specific modules from the problem choice framework most appropriate to the question:
          - **Skills 1-4** for future project planning (ideation, risk, optimization, parameters)
          - **Skills 5-7** for current project navigation (decision trees, adversity, inversion)
          - **Skill 8** for communication and synthesis
          - **Skill 9** for comprehensive workflow orchestration

          See the complete reference materials in the `references/` folder.

          ---

          ## Core Framework Concepts

          ### The Central Insight
          **Problem Choice >> Execution Quality**

          Even brilliant execution of a mediocre problem yields incremental impact. Good execution of an important problem yields substantial impact.

          ### The Time Paradox
          Scientists typically spend:
          - **Days** choosing a problem
          - **Years** solving it

          This imbalance limits impact. These skills help invest more time choosing wisely.

          ### Evaluation Axes
          **For Evaluating Ideas:**
          - **X-axis:** Likelihood of success
          - **Y-axis:** Impact if successful

          Skills help move ideas rightward (more feasible) and upward (more impactful).

          ### The Risk Paradox
          - Don't avoid risk—befriend it
          - No risk = incremental work
          - But: Multiple miracles = avoid or refine
          - **Balance:** Understood, quantified, manageable risk

          ### The Parameter Paradox
          - Too many fixed = brittleness
          - Too few fixed = paralysis
          - **Sweet spot:** Fix ONE meaningful constraint

          ### The Adversity Principle
          - Crises are inevitable (don't be surprised)
          - Crises are opportune (don't waste them)
          - **Strategy:** Fix problem AND upgrade project simultaneously

          ---

          ## The 9 Skills Overview

          | Skill | Purpose | Output | Time |
          |-------|---------|--------|------|
          | 1. Intuition Pumps | Generate high-quality research ideas | Problem Ideation Document | ~1 week |
          | 2. Risk Assessment | Identify and manage project risks | Risk Assessment Matrix | 3-5 days |
          | 3. Optimization Function | Define success metrics | Impact Assessment Document | 2-3 days |
          | 4. Parameter Strategy | Decide what to fix vs. keep flexible | Parameter Strategy Document | 2-3 days |
          | 5. Decision Tree Navigation | Plan decision points and altitude dance | Decision Tree Map | 2 days |
          | 6. Adversity Response | Prepare for crises as opportunities | Adversity Playbook | 2 days |
          | 7. Problem Inversion | Navigate around obstacles | Problem Inversion Analysis | 1 day |
          | 8. Integration & Synthesis | Synthesize into coherent plan | Project Communication Package | 3-5 days |
          | 9. Meta-Framework | Orchestrate complete workflow | Complete Project Package | 1-6 weeks |

          ---

          ## Skill Workflow

          ```
          SKILL 1: Intuition Pumps
                   | (generates idea)
                   v
          SKILL 2: Risk Assessment
                   | (evaluates feasibility)
                   v
          SKILL 3: Optimization Function
                   | (defines success metrics)
                   v
          SKILL 4: Parameter Strategy
                   | (determines flexibility)
                   v
          SKILL 5: Decision Tree
                   | (plans execution and evaluation)
                   v
          SKILL 6: Adversity Planning
                   | (prepares for failure modes)
                   v
          SKILL 7: Problem Inversion
                   | (provides pivot strategies)
                   v
          SKILL 8: Integration & Communication
                   | (synthesizes into coherent plan)
                   v
          SKILL 9: Meta-Skill
                   (orchestrates complete workflow)
          ```

          ---

          ## Key Design Principles

          1. **Conversational Entry** - Meet users where they are with three clear starting points
          2. **Thoughtful Interaction** - Ask clarifying questions; low confidence prompts additional input
          3. **Literature Integration** - Use PubMed searches at strategic points for validation
          4. **Concrete Outputs** - Every skill produces tangible 1-2 page documents
          5. **Building Specificity** - Progressive detail emerges through targeted questions
          6. **Flexibility** - Skills work independently, sequentially, or iteratively
          7. **Scientific Rigor** - Claims about generality and feasibility should be evidence-based

          ---

          ## Who Should Use These Skills

          ### Graduate Students (Primary Audience)
          - **When:** Choosing thesis projects, qualifying exams, committee meetings
          - **Focus:** Skills 1-3 (ideation, risk, impact) + Skill 9 (complete workflow)
          - **Timeline:** 2-4 weeks for comprehensive planning

          ### Postdocs
          - **When:** Starting new position, planning independent projects, fellowship applications
          - **Focus:** All skills, emphasizing independence and risk management
          - **Timeline:** 1-2 weeks intensive planning

          ### Principal Investigators
          - **When:** New lab, new direction, mentoring trainees, grant cycles
          - **Focus:** Skills 1, 3, 4, 6 (ideation, impact, parameters, adversity)
          - **Timeline:** Ongoing, integrate into lab culture

          ### Startup Founders
          - **When:** Company inception, pivot decisions, investor pitches
          - **Focus:** Skills 1-4 (ideation through parameters) + Skill 8 (communication)
          - **Timeline:** 1-2 weeks for initial planning, revisit quarterly

          ---

          ## Reference Materials

          Detailed skill documentation is available in the `references/` folder:

          | File | Content | Search Patterns |
          |------|---------|-----------------|
          | `01-intuition-pumps.md` | Generate research ideas | `Intuition Pump #`, `Trap #`, `Phase [0-9]` |
          | `02-risk-assessment.md` | Risk identification | `Risk.*1-5`, `go/no-go`, `assumption` |
          | `03-optimization-function.md` | Success metrics | `Generality.*Learning`, `optimization`, `impact` |
          | `04-parameter-strategy.md` | Parameter fixation | `fixed.*float`, `constraint`, `parameter` |
          | `05-decision-tree.md` | Decision tree navigation | `altitude`, `Level [0-9]`, `decision` |
          | `06-adversity-planning.md` | Adversity response | `adversity`, `crisis`, `ensemble` |
          | `07-problem-inversion.md` | Problem inversion strategies | `Strategy [0-9]`, `inversion`, `goal` |
          | `08-integration-synthesis.md` | Integration and synthesis | `narrative`, `communication`, `story` |
          | `09-meta-framework.md` | Complete workflow | `Phase`, `workflow`, `orchestrat` |

          ---

          ## Expected Outcomes

          ### Immediate (After Completing Workflow)
          - Clear project vision
          - Honest risk assessment
          - Contingency plans
          - Communication materials ready
          - Confidence in problem choice

          ### 6-Month
          - Faster decisions (have framework)
          - Productive adversity handling
          - No existential crises (risks mitigated)

          ### 2-Year
          - Published results or strong progress
          - Avoided dead-end projects
          - Career aligned with goals
          - **Time well-spent** (ultimate measure)

          ---

          ## Foundational Reference

          **Fischbach, M.A., & Walsh, C.T. (2024).** "Problem choice and decision trees in science and engineering." *Cell*, 187, 1828-1833.

          Based on course BIOE 395 taught at Stanford University.
    - path: /knowledge-work-plugins/skills/scientific-problem-selection/references/01-intuition-pumps.md
      text: |
          # SKILL: Intuition Pumps for Scientific Problem Ideation

          ## Overview
          This skill helps scientists generate high-quality research ideas by providing systematic prompts ("intuition pumps") and identifying common ideation traps. Based on the framework that most biological and chemical science projects involve **perturbing a system, measuring it, and analyzing the data**, this skill guides users through structured ideation that can significantly impact how they spend years of their career.

          ## Core Framework

          ### The Three Pillars of Scientific Work
          Research advances generally fall into one of these categories, each with two dimensions:

          **PERTURBATION**
          - *Logic*: Novel ways to manipulate biological systems (e.g., using CRISPR for deep mutational scanning)
          - *Technology*: New tools for manipulation (e.g., developing base editors, creating whole-genome CRISPR libraries)

          **MEASUREMENT**  
          - *Logic*: Novel applications of existing measurement tools (e.g., using tissue clearing to study liver fibrosis)
          - *Technology*: New measurement capabilities (e.g., developing tissue-clearing techniques, super-resolution microscopy)

          **THEORY/COMPUTATION**
          - *Logic*: Using computational tools to make discoveries (e.g., applying AlphaFold to identify protein functions)
          - *Technology*: Building new algorithms or models (e.g., developing machine learning architectures for biological data)

          Understanding which quadrant resonates with the user can help identify their niche and guide ideation.

          ## The Skill Workflow

          ### Phase 1: Initial Discovery Questions (5-10 minutes)

          Before diving into intuition pumps, Claude should gather context by asking the user:

          1. **What is the user's general research area or field?** (e.g., immunology, synthetic biology, neuroscience, protein engineering)

          2. **What excites the user most about science?**
             - Building new tools/technologies?
             - Discovering fundamental principles?
             - Solving practical problems?
             - Understanding dynamic processes?

          3. **What are the user's existing strengths?** (Select all that apply)
             - Specific techniques (please list)
             - Computational skills
             - Access to unique systems/models
             - Domain expertise in a particular area

          4. **Current constraints:**
             - Time horizon for this project? (months/years)
             - Resources available?
             - Must it connect to existing work, or can the user start fresh?

          5. **On a scale of 1-5, how would the user rate their current idea?**
             - Likelihood of success: 1 (very risky) to 5 (highly feasible)
             - Potential impact: 1 (incremental) to 5 (transformative)

          ### Phase 2: Applying Intuition Pumps

          Based on the user's responses, Claude should guide them through relevant intuition pumps from this list:

          #### Intuition Pump #1: Make It Systematic
          **Prompt:** Take any one-off perturbation or measurement and make it systematic.

          **Examples:**
          - Instead of mutating one enzyme, measure kinetic parameters across an entire enzyme family
          - Instead of one CRISPR mutant → genome-wide screen with transcriptomic readout
          - Instead of imaging one condition → high-throughput imaging across thousands of conditions

          **Prompt for User:** What one-off experiment in your field could become a systematic survey?

          #### Intuition Pump #2: Identify Technology Limitations
          **Prompt:** What are the fundamental limitations of technologies you use? These limitations are opportunities.

          **Examples:**
          - Microscopy can't resolve beyond diffraction limit → super-resolution microscopy
          - DNA synthesis can't make complete genomes → develop assembly methods
          - Genetic screens have precise input but imprecise output → develop high-dimensional readouts
          - We do single gene KOs but networks are complex → develop combinatorial perturbation methods

          **Prompt for User:** What technology limitation frustrates you most? How might you turn that limitation into an opportunity?

          #### Intuition Pump #3: The "I Can't Imagine" Test
          **Prompt:** I can't imagine a future in which we don't have ____, but it doesn't exist yet.

          **Examples:**
          - The ability to design highly efficient enzymes like we design other proteins
          - The ability to deliver genome editing payloads to any cell type in vivo
          - 3D tomographic imaging of live cells at molecular resolution
          - Proteome-scale sequencing with the throughput of RNA-seq

          **Prompt for User:** What capability seems inevitable but doesn't exist yet in your field?

          #### Intuition Pump #4: Static vs. Dynamic Understanding
          **Prompt:** We understand biological "parts lists" but rarely understand dynamic processes.

          **Key Insight:** Most observations are single-timepoint, single-perturbation format. But biological systems are dynamic—like humans flowing through Grand Central Station or money through financial systems.

          **Examples:**
          - Understanding growth factor signaling like we understand turning a key in a car engine
          - Time-resolved cell atlases with lineage tracing through entire development
          - Following metabolite flux through pathways in real-time

          **Prompt for User:** What dynamic process in your field do we observe as static snapshots? How might you capture the full temporal or spatial dynamics?

          #### Intuition Pump #5: Pick a New Axis
          **Prompt:** We almost always use time as the x-axis for dynamic processes. What other coordinate could you use?

          **Example:** Instead of time, use "infection progression" markers to enable monitoring asynchronous cells

          **Prompt for User:** What non-temporal coordinate could reveal new biology in your system?

          #### Intuition Pump #6: Create a Technology Platform
          **Prompt:** Instead of answering one question, could you build a platform that enables many questions?

          **Examples:**
          - Antibodies for intracellular targets (not just extracellular)
          - AI that predicts perturbations needed to reach desired cell states
          - Universal genome delivery vehicles

          **Prompt for User:** What platform would transform how your field asks questions?

          #### Intuition Pump #7: Dogs That Don't Bark
          **Prompt:** Why doesn't something exist or occur? Absence can be as informative as presence.

          **Examples:**
          - Why are there no Gram-negative bacteria on human skin?
          - Why do some catalytically inactive enzymes persist through evolution?
          - Why don't certain cell types exist in certain tissues?

          **Prompt for User:** What absence puzzles you in your field?

          ### Phase 3: Avoiding Common Traps

          After generating ideas, we must evaluate them critically. Here are the most common traps:

          #### Trap #1: The Truffle Hound
          **Warning:** Don't become so good at one system or technique that you fail to ask questions of biological import.

          **Bad:** "What is the role of p190 RhoGAP in wing development?"  
          **Better:** "How do signaling pathways and cytoskeleton coordinate to control wing development?"

          **Self-Check:** Is the question driven by biological curiosity or by what the user is technically capable of?

          #### Trap #2: Applying Existing Tool to New System
          **Warning:** "Let's use CRISPR in my organism" can be valuable but risks crowding and incrementalism.

          **When It Works:** The user is enabling a field that truly needs this capability
          **When It Fails:** The tool is already widely applied; the contribution will be incremental

          **Self-Check:** Will this tool application open new biological questions, or just extend existing observations? Claude should help the user evaluate this honestly.

          #### Trap #3: Jumping on the First Idea
          **Warning:** Treating ideas with reverence instead of skepticism. Confirmation bias sets in quickly.

          **Better Approach:** Users should treat new ideas like leeches trying to steal their time. Look for the warts. Develop several ideas in parallel and comparison shop.

          **Self-Check:** Has the user critically evaluated at least 3-5 alternative approaches?

          #### Trap #4: Too Many Fixed Parameters
          **Warning:** Fixing too many parameters at the outset creates a poor technique-application match.

          **Example of Over-Constraining:** "I will use spatial transcriptomics to study antigen-presenting cell and T cell interactions in the tumor microenvironment."
          - This fixes: technique (spatial transcriptomics), cell types, and context
          - If any assumption fails, the project fails

          **Self-Check:** Has the user fixed more than 2 parameters before starting?

          #### Trap #5: Too Few Fixed Parameters
          **Warning:** "I want to do impactful work in cell engineering" → paralysis

          **Resolution:** Constraints engender creativity. Fix ONE parameter at a time and let creativity flow.

          **Self-Check:** Does the user have at least one concrete constraint to work with?

          ### Phase 4: Literature Integration

          To ensure the idea has appropriate scope and hasn't been thoroughly explored, Claude should ask:

          1. **What are 2-3 key questions or gaps the idea addresses?**

          2. **What should be searched in PubMed to:**
             - Understand the current state of the field?
             - Identify related approaches?
             - Find empirical knowledge from adjacent domains that could inform the approach?

          Claude should use PubMed to:
          - Assess how general/specific the problem is
          - Identify relevant methodological advances
          - Find analogous systems or approaches in other fields
          - Determine the degree of competition

          ### Phase 5: Idea Refinement and Output

          After working through intuition pumps, avoiding traps, and reviewing literature, Claude should help the user:

          1. **Crystallize the Idea:**
             - Biological question
             - Technical approach (perturbation/measurement/theory: logic vs. technology)
             - What's novel about this angle?

          2. **Articulate Fixed vs. Floating Parameters:**
             - What MUST remain constant in the approach?
             - What can be flexible if obstacles arise?

          3. **Identify Key Assumptions:**
             - What must be true for this to work?
             - Which assumptions are about biology vs. technology capabilities?

          4. **Sketch Alternative Paths:**
             - If the primary approach fails, what's Plan B?
             - Can the project be designed to succeed regardless of outcome?

          ## Output Deliverable

          At the end of this skill, Claude should produce a **2-page Problem Ideation Document** containing:

          ### Page 1: Core Idea
          - **Title:** Concise project name
          - **The Question:** What biological question is being asked?
          - **The Approach:** How will it be answered? (Specify perturbation/measurement/computation: logic vs. technology)
          - **What's Novel:** The unique angle
          - **Why It Matters:** Potential impact (generality × learning, or technology development)
          - **Intuition Pump(s) Used:** Which prompted this idea

          ### Page 2: Critical Analysis
          - **Fixed vs. Floating Parameters:**
            - Fixed: What must stay constant
            - Floating: What can adapt

          - **Key Assumptions & Risk Assessment:**
            - Biological assumptions (risk level 1-5)
            - Technical assumptions (risk level 1-5)

          - **Traps Avoided:** Which pitfalls were navigated around?

          - **Alternative Approaches:** Plan B and Plan C

          - **Literature Context:**
            - 3-5 key papers that inform or relate to this work
            - Degree of competition (low/medium/high)
            - The user's edge/advantage

          - **Next Steps:** First 3 concrete experiments or analyses

          ## Key Principles to Remember

          1. **Reversal of Polarity:** Treat ideas with skepticism, not reverence. Look for flaws before falling in love.

          2. **Comparison Shopping:** Develop multiple ideas in parallel. The act of comparison improves decision-making.

          3. **Fix One Parameter at a Time:** Constraints engender creativity, but too many constraints prevent it.

          4. **Think in Ensembles:** The user is picking a family of possible projects, not a singular path. Flexibility is essential.

          5. **Balance Logic and Technology:** Novel biology can come from new tools OR clever application of existing tools.

          6. **Systematic Over One-Off:** High-throughput and systematic approaches often reveal more than single observations.

          7. **Dynamic Over Static:** Biological systems are dynamic. How can process be captured rather than snapshot?

          ## Getting Started

          When the user is ready, Claude should guide them through the Phase 1 questions to begin the systematic ideation process. The key message: spending extra time on problem choice is the highest-leverage activity in science. A well-chosen problem executed reasonably well will have more impact than a mediocre problem executed brilliantly.

          ---

          *This skill is based on the problem choice framework developed by Michael A. Fischbach and Christopher T. Walsh, as described in "Problem choice and decision trees in science and engineering" (Cell, 2024).*
    - path: /knowledge-work-plugins/skills/scientific-problem-selection/references/02-risk-assessment.md
      text: |
          # SKILL 2: Risk Assessment and Assumption Analysis

          ## Overview
          This skill helps scientists systematically identify, quantify, and manage project risk through rigorous assumption analysis. The goal is not to eliminate risk—risk-free projects tend to be incremental—but to name it, quantify it, and work steadily to chip away at it. This skill builds directly on the Problem Ideation Document from Skill 1.

          ## Core Principle

          **"Don't avoid risk; befriend it."**

          The most important concept in problem choice is the two-axis evaluation:
          - **X-axis:** Likelihood of success
          - **Y-axis:** Impact if successful

          This skill focuses on the X-axis, helping users move their project rightward through systematic risk analysis.

          ## Why This Matters

          A project with a high-risk assumption that won't read out for >2 years is problematic. One that requires multiple miracles to succeed should be avoided or refined. The human tendency is to stay in a safe local space, work laterally, and put off facing existential risks—like an ostrich burying its head in the sand. This skill helps users face risk head-on.

          ## The Skill Workflow

          ### Phase 1: Extract Project Assumptions (10-15 minutes)

          First, Claude should gather information about the user's project from Skill 1:

          1. **Project Summary** (from Skill 1):
             - The biological question
             - The technical approach
             - What's novel about it

          2. **Project Horizon:**
             - How long is this project expected to take? (months/years)
             - What is the user's role? (graduate student, postdoc, PI, startup founder)

          3. **Initial Risk Sense:**
             - What keeps the user up at night about this project?
             - What's the scariest assumption?

          ### Phase 2: Comprehensive Assumption Listing

          Claude should work with the user to list EVERY assumption the project makes from inception through conclusion. Assumptions fall into two categories:

          #### Type A: Assumptions About Biological Reality
          These are facts about the world that either are or aren't true. They won't change during the project.

          **Examples:**
          - New cell types exist beyond what's currently known
          - A particular gene regulates the process being studied
          - Two proteins physically interact
          - A pathway functions in the organism of interest
          - The biological effect size is detectable

          #### Type B: Assumptions About Technical Capability
          These are about whether technology can do what's needed. These CAN change during the project as methods improve.

          **Examples:**
          - A specific cell type can be isolated
          - Sequencing will generate high-quality data
          - An assay has sufficient throughput
          - Computational analysis can distinguish signal from noise
          - Gene editing will work in the system

          **Claude should ask:**
          1. What must be true about the biology for this to work?
          2. What must the technology be able to do?
          3. What about the experimental design—what assumptions are built in?
          4. What about the analysis—can it deliver what's needed?
          5. If everything works, can the findings be validated?
          6. Will the findings be interpretable and meaningful?

          ### Phase 3: Risk Scoring (The Assumption Analysis Table)

          For each assumption, Claude should help the user assign two scores:

          #### Risk Level (1-5 scale):
          - **1** = Very likely to be true/work (>90% confidence)
          - **2** = Likely (70-90% confidence)
          - **3** = Uncertain (40-70% confidence)
          - **4** = Unlikely (10-40% confidence)
          - **5** = Very unlikely (<10% confidence)

          #### Time to Test (months):
          How long before the user will know if this assumption is valid?

          **Critical Rules:**
          1. Be brutally honest—try to convince oneself of being WRONG, not right
          2. Distinguish between biological vs. technical assumptions
          3. Consider whether technical assumptions might improve over time
          4. Note which assumptions depend on earlier assumptions succeeding

          ### Phase 4: Risk Profile Evaluation

          Once the complete table is ready, Claude should analyze the risk profile:

          #### Red Flags to Identify:
          1. **The Late High-Risk Problem:** Risk level 4-5 assumption that won't read out until >18 months
          2. **The Multiple Miracles:** More than 2-3 assumptions with risk level 4-5
          3. **The Dependency Chain:** High-risk assumptions stacked in sequence
          4. **The Ostrich Pattern:** Starting with low-risk work while avoiding the high-risk tests

          #### Green Lights:
          1. **Early Go/No-Go:** Highest-risk assumption testable in <6 months
          2. **Multiple Candidates:** Project can succeed with several different outcomes
          3. **Graceful Degradation:** If assumption X fails, assumption Y provides alternative path
          4. **Risk Distribution:** High-risk assumptions balanced across timeline

          **Rule of Thumb:** If you have a risk level 5 assumption three years out, pick another project.

          ### Phase 5: Risk Mitigation Strategies

          For each high-risk assumption (level 4-5), Claude should help develop mitigation strategies:

          #### Strategy 1: Move High-Risk Tests Earlier
          **Question:** Can a quicker, cruder test be designed that answers most of what's needed?

          **Example:** Instead of waiting 2 years to validate a new cell type exists, consider:
          - Using existing markers as a proxy
          - Testing in a simpler model system first
          - Using computational predictions to increase confidence

          #### Strategy 2: Multiple Candidates Approach
          **Question:** Can multiple candidates be tested in parallel to increase likelihood of success?

          **Example:** Instead of:
          - Testing one kinase → Test a panel of 10 kinases
          - Building one engineered organism → Build and test a library
          - Pursuing one therapeutic target → Pursue 3 related targets

          #### Strategy 3: Reframe the Question
          **Question:** Can the project scope be adjusted to reduce critical assumptions while maintaining impact?

          **Example from lecture:**
          - **Original:** Identify NEW enteroendocrine cell types (high risk: they may not exist)
          - **Reframed:** Better characterize KNOWN but incompletely understood cell types (lower risk)

          #### Strategy 4: Change the System
          **Question:** Is there a different biological system with similar scientific value but lower technical risk?

          **Example from lecture:**
          - **Original:** Intestinal epithelium (hard to manipulate genetically)
          - **Alternative:** Liver (easier genetic manipulation options exist)

          #### Strategy 5: Add Complementary Approaches
          **Question:** Can a parallel approach be added that de-risks the main assumption?

          **Example from lecture:**
          - Add spatial transcriptomics to scRNA-seq
          - This provides biogeographic context and validates cell type existence earlier

          ### Phase 6: Go/No-Go Experiment Design

          For the top 3 highest-risk assumptions, Claude should help design the critical go/no-go experiments:

          **For each, specify:**
          1. **The Question:** Exactly what is being tested?
          2. **The Experiment:** Most direct test possible (even if crude)
          3. **Success Criteria:** What result means "go"?
          4. **Failure Response:** What result means "pivot" or "stop"?
          5. **Timeline:** How soon can this be run?
          6. **Resources:** What is needed?

          **Key Principle:** Cut right to the critical go/no-go experiment. Don't just start with easy stuff—the risk points aren't going away.

          ### Phase 7: Literature Validation

          Claude should search PubMed to help calibrate risk assessments:

          **Search for:**
          1. **Precedents:** Has anyone done something similar? (Reduces technical risk)
          2. **Biological Evidence:** What's known about the system? (Informs biological risk)
          3. **Technical Benchmarks:** How well do the methods work in practice?
          4. **Adjacent Successes:** Has anyone solved related problems?

          **Questions to ask the user:**
          - What specific searches would help calibrate risk?
          - Are there particular papers that informed the assumptions?
          - Are there technical benchmarks to look up?

          ### Phase 8: Revised Project Plan

          Based on the risk analysis, Claude should help create a revised plan:

          #### Option A: De-Risk the Current Plan
          - Reorder experiments to test high-risk assumptions early
          - Add complementary approaches
          - Design multiple-candidate strategies

          #### Option B: Reframe the Project
          - Adjust scope while maintaining impact
          - Change biological system
          - Modify technical approach

          #### Option C: Pick a Different Project
          Sometimes the honest answer is: "This has too many miracles." That's valuable to know BEFORE investing years.

          ## Output Deliverable

          Claude should produce a **2-page Risk Assessment Document**:

          ### Page 1: Assumption Analysis Table

          | Assumption | Type* | Risk† | Time‡ | Notes |
          |------------|-------|-------|-------|-------|
          | [Assumption 1] | Bio/Tech | 1-5 | X mo | [Rationale for score] |
          | [Assumption 2] | Bio/Tech | 1-5 | X mo | [Rationale for score] |
          | ... | ... | ... | ... | ... |

          *Bio = Biological reality, Tech = Technical capability  
          †Risk: 1=very likely to 5=very unlikely  
          ‡Time to test in months

          #### Risk Profile Summary:
          - **Total Assumptions:** X
          - **High Risk (4-5):** X assumptions
          - **Late High Risk (>18mo):** X assumptions
          - **Critical Path:** [Identify the chain of dependent assumptions]
          - **Overall Assessment:** [Green/Yellow/Red light with explanation]

          ### Page 2: Risk Mitigation Plan

          #### Top 3 High-Risk Assumptions:
          For each:
          1. **The Assumption:** [Stated clearly]
          2. **Current Risk Level & Timeline:** X (risk) at Y months
          3. **Why This Risk Exists:** [Explanation]
          4. **Mitigation Strategy:** [From Strategies 1-5 above]
          5. **Go/No-Go Experiment:**
             - Experiment design
             - Success criteria
             - Timeline
             - What you'll do if it fails

          #### Revised Project Timeline:
          ```
          Month 0-6:   [Early go/no-go experiments]
          Month 6-12:  [Based on go/no-go results]
          Month 12-18: [...]
          Month 18+:   [...]
          ```

          #### Contingency Plans:
          - **If assumption X fails:** [Plan B]
          - **If assumption Y fails:** [Plan C]
          - **Multiple success paths:** [How project can succeed different ways]

          #### Decision Points:
          - **Month X:** Evaluate [assumptions A, B] → Go/Pivot/Stop decision
          - **Month Y:** Evaluate [assumptions C, D] → Go/Pivot/Stop decision

          ## Practical Examples

          ### Example 1: ScRNA-Seq for Enteroendocrine Cells

          **High-Risk Assumptions Identified:**
          1. "New cell types can be validated experimentally" (Risk 5, 24 months)
          2. "Knockout will yield biologically relevant phenotype" (Risk 5, 30 months)

          **Problem:** Two risk-5 assumptions at 24+ months = RED FLAG

          **Mitigation Applied:**
          - Reframe to study known but poorly characterized cells (reduces Risk 5→3)
          - Switch to liver instead of intestine (improves validation timeline: 30→18 months)
          - Add spatial transcriptomics (provides earlier validation checkpoint at 16 months)

          ### Example 2: Bacterial Therapy for Chronic Kidney Disease

          **High-Risk Assumption Identified:**
          "Key uremic toxins leading to effects can be determined" (Risk 4, unknown timeline)

          **Problem:** Critical assumption with unclear path to resolution

          **Mitigation Applied:**
          - Focus on known lead toxins (IS and PCS) rather than discovering new ones
          - Add parallel track: test multiple toxin candidates
          - Design study where learning toxin identity IS the outcome (multiple success paths)

          ## Key Principles to Remember

          1. **Try to Convince Yourself You're Wrong:** The goal is critical evaluation, not confirmation bias.

          2. **Ignore Everything But Key Risk Points:** Don't get distracted by easy tasks. The high-risk assumptions aren't going away.

          3. **Early and Often:** Design go/no-go experiments at the earliest feasible moment.

          4. **Be Candid About Risk:** When presenting ideas, acknowledging risk makes your case MORE convincing, not less.

          5. **No Risk, No Interest:** The goal isn't zero risk—it's understood, quantified, manageable risk.

          6. **Risk Can Change:** Technical assumptions may improve as methods advance. Build this into your planning.

          7. **Compare Risk Profiles:** Evaluate multiple projects in parallel to compare risk profiles and make better choices.

          8. **Watch for the Ostrich Pattern:** Are you avoiding the scary experiment? That's human nature, but a critical failure mode.

          ## Warning Signs

          **Warning signs include:**
          - Risk level 5 assumptions >2 years out
          - More than 3 assumptions at risk level 4-5
          - Highest-risk assumptions at the END of the timeline
          - Rationalizing why high-risk assumptions will "probably work out"
          - Planning to "start with the easy stuff" while avoiding risk tests
          - Inability to articulate clear go/no-go criteria

          **Good shape indicators:**
          - Highest-risk tests happen in first 6 months
          - Multiple paths to success exist
          - Clear plans for what to do if key assumptions fail
          - Risk is distributed across the timeline
          - Testing assumptions, not confirming hopes

          ## Getting Started

          Claude should begin with Phase 1 by asking for:
          1. The project summary from Skill 1
          2. Project timeline expectations
          3. What concerns the user most about this project

          Together, Claude and the user will build a rigorous risk assessment that dramatically improves the likelihood of success by helping avoid years of work on projects with insurmountable obstacles.

          ---

          *Remember: Spending time on risk analysis is the most valuable investment a scientist can make. A well-understood risk profile enables moving forward with confidence or pivoting with clarity—both are valuable outcomes.*
    - path: /knowledge-work-plugins/skills/scientific-problem-selection/references/03-optimization-function.md
      text: |
          # SKILL 3: Optimization Function Selection

          ## Overview
          This skill helps scientists articulate HOW their project should be evaluated and define what success means. While Skill 2 focused on likelihood of success (the X-axis), this skill focuses on impact if successful (the Y-axis). The key insight: **value is in the eye of a belief system**—the value creation framework must be explicitly stated and led with.

          ## Core Principle

          **"Pick the right optimization function."**

          Different types of projects should be evaluated by different metrics. A common source of conflict between trainees and PIs, or authors and referees, is a misunderstanding about which category a project falls under. The root cause is often failure to articulate evaluation criteria clearly.

          ## The Fundamental Truth

          The default state of:
          1. Every new discovery is **irrelevance**
          2. Every new technology is **non-use**
          3. Every company is **death**

          Scientists must actively work against these defaults by choosing the right metrics and scoring well on at least one axis.

          ## The Skill Workflow

          ### Phase 1: Project Categorization (5 minutes)

          First, Claude should determine what type of project the user is pursuing:

          **Question 1: What is the primary goal?**
          A. Understand how biology works (fundamental knowledge)
          B. Enable new experiments or capabilities (tool/technology)
          C. Solve a practical problem (invention/application)
          D. Something else (please describe)

          **Question 2: What would "success" look like in 3-5 years?**
          - 1-2 sentences describing the ideal outcome

          **Question 3: Who cares if this succeeds?**
          - Academic researchers in the subfield?
          - Broader scientific community across fields?
          - Clinicians or practitioners?
          - Industry partners or companies?
          - General public or specific communities?
          - All of the above?

          Based on the answers, Claude should help identify the right optimization function.

          ### Phase 2: Understanding the Three Main Frameworks

          #### Framework 1: Basic Science
          **Axes:** How much did we learn? × How general/fundamental is the object of study?

          **Philosophy:** A high score on EITHER axis yields substantial impact. You don't need both.

          **Examples:**
          - **High Generality, Medium Learning:** Ribosome stalling complex
            - Updates understanding of translation (fundamental process)
            - Scores well because translation is universal
            
          - **Medium Generality, High Learning:** Oxytricha germ-line nucleus
            - Genomic acrobatics may not be common to other organisms
            - BUT elegant mapping scores highly on how much we learned
            - May yield tools for genome editing (bonus)
            
          - **High on Both Axes (Landmark):** RNA interference, biomolecular condensates
            - These are rare—don't expect every project to be here
            - But aim to score well on at least one axis

          **Key Questions:**
          - How many systems/organisms does this apply to?
          - Does it update understanding of a fundamental process?
          - Will textbooks need to be rewritten?
          - What new questions does this open?

          #### Framework 2: Technology Development
          **Axes:** How widely will it be used? × How critical is it for the application?

          **Philosophy:** Again, high score on EITHER axis is sufficient.

          **Examples:**
          - **Widely Used, Not Critical:** BLAST
            - Used in countless projects
            - Rarely THE critical tool, but enormous cumulative impact
            
          - **Not Widely Used, Highly Critical:** Cryo-electron tomography
            - Too complicated for broad adoption
            - But generates stunning data that's impossible to get otherwise
            - When you need it, nothing else works
            
          - **High on Both Axes (Game-Changing):** 
            - GFP, CRISPR, AlphaFold (the famous ones)
            - But also: lentiviral delivery, cell sorting, massively parallel sequencing
            - Technologies we cannot imagine living without

          **Key Questions:**
          - How many labs would adopt this?
          - For what fraction of experiments is this THE enabling technology?
          - What becomes possible that wasn't before?
          - How hard is it to implement?

          **Critical Rule:** A tool that won't be widely used AND isn't critical for an application probably isn't worth building.

          #### Framework 3: Typical Invention/Application
          **Axes:** How much good? × For how many people?

          **Philosophy:** Useful for translational work, frugal science, global health.

          **Examples:**
          - Foldscope: Paper microscope accessible to millions of students globally
          - Neglected tropical disease intervention: Quality-adjusted life years per $100
          - Medical device: Number of patients who can access treatment

          **Key Questions:**
          - What problem does this solve?
          - How many people have this problem?
          - How much better is their life if you solve it?
          - What's the cost per person helped?

          ### Phase 3: Selecting and Articulating Your Framework

          Based on your Phase 1 responses, let me help you choose:

          **If you selected A (fundamental knowledge):** → Basic Science Framework
          **If you selected B (enable experiments):** → Technology Development Framework  
          **If you selected C (solve practical problem):** → Invention Framework

          **Now, let's be explicit:**

          1. **State Your Framework:** "This project should be evaluated as [basic science/technology development/invention]."

          2. **Define Your Axes:** 
             - X-axis measures: [specific metric]
             - Y-axis measures: [specific metric]

          3. **Make Your Case:**
             - X-axis score (Low/Medium/High): [Your assessment + reasoning]
             - Y-axis score (Low/Medium/High): [Your assessment + reasoning]

          4. **Threshold Check:** 
             - Do you score at least MEDIUM-HIGH on one axis?
             - If both are LOW-MEDIUM, you have a problem

          ### Phase 4: Alternative or Custom Metrics

          Sometimes standard frameworks don't fit. Examples where custom metrics work:

          **Alternative Metric Examples:**
          - **Frugal Science:** How many children in low/middle-income countries gain access to microscopy?
          - **Neglected Disease:** Quality-adjusted life years saved per $100 invested
          - **Sustainability:** Tons of CO₂ equivalent prevented × cost-effectiveness
          - **Equity:** Reduction in disparity metric × number of people affected

          **When to propose alternative metrics:**
          - Your work addresses a specific underserved need
          - Standard metrics miss your core value proposition
          - You're working in an emerging area without established norms
          - Your work crosses traditional boundaries

          **How to propose alternative metrics:**
          1. Explain why standard metrics are insufficient
          2. Define your proposed metric clearly
          3. Provide a value creation index (two axes)
          4. Show how your project scores on these axes

          ### Phase 5: Comparative Assessment

          Even if absolute impact is hard to estimate, comparative assessment is valuable:

          **Exercise: Compare 3 Related Projects**

          For your project and two alternatives (either from literature or hypothetical):

          | Project | Framework | X-Axis Score | Y-Axis Score | Overall |
          |---------|-----------|--------------|--------------|---------|
          | Yours | [Type] | [L/M/H] + reasoning | [L/M/H] + reasoning | [Assessment] |
          | Alt 1 | [Type] | [L/M/H] + reasoning | [L/M/H] + reasoning | [Assessment] |
          | Alt 2 | [Type] | [L/M/H] + reasoning | [L/M/H] + reasoning | [Assessment] |

          **Comparative Questions:**
          - Which would be most impactful if they all work?
          - Which has the best risk-adjusted impact?
          - Are you pursuing the best option?
          - If not, why? (Sometimes there are good reasons: resources, expertise, timing)

          ### Phase 6: Avoiding Metric Mismatch

          **Common Mismatches:**

          #### Mismatch 1: Basic Science vs. Technology Evaluation
          **Scenario:** You're doing fundamental biology, but reviewers ask "How widely will this be used?"

          **Problem:** They're evaluating basic science with technology metrics

          **Solution:** Explicitly frame as basic science. Lead with: "This updates our understanding of [fundamental process], which is conserved across [many systems]."

          #### Mismatch 2: Technology vs. Basic Science Evaluation
          **Scenario:** You're building a tool, but reviewers ask "How much did we learn about biology?"

          **Problem:** They're evaluating technology with basic science metrics

          **Solution:** Explicitly frame as technology development. Lead with: "This enables experiments that are currently impossible, which [X] labs need for [Y] applications."

          #### Mismatch 3: Within-Category Confusion
          **Scenario:** Your basic science is specific but deep, but reviewers want broad generality

          **Problem:** They think both axes are required, rather than either/or

          **Solution:** Explicitly acknowledge: "While this may not be universal, the depth of mechanistic insight scores highly on the learning axis."

          #### Mismatch 4: Time Horizon Mismatch
          **Scenario:** You're working on long-term fundamental research, but reviewers want immediate impact

          **Problem:** Different value systems about when impact should materialize

          **Solution:** Articulate your time horizon explicitly and provide historical examples of similar timelines

          ### Phase 7: Value System Discussion

          This is where Claude explicitly discusses the user's belief system about what matters:

          **Questions for Reflection:**

          1. **What drives the user?**
             - Discovery and understanding?
             - Enabling others?
             - Solving problems?
             - Building things?

          2. **What would make the user proud?**
             - Paper in Cell/Nature/Science?
             - Tool used by hundreds of labs?
             - Treatment reaching patients?
             - Opening a new field?

          3. **How does the user want to be remembered?**
             - "Discovered X"
             - "Built Y that enabled Z"
             - "Solved problem W"
             - "Trained students who went on to..."

          4. **Whose approval matters?**
             - Specific senior scientists in the field?
             - Broader community across fields?
             - Practitioners who use tools?
             - People whose lives are improved?

          **There are no wrong answers—but alignment matters:**
          - The project should match the user's value system
          - The evaluation framework should match the project type
          - Communication should lead with the framework

          ### Phase 8: Literature Benchmarking

          Claude should use PubMed to benchmark impact in the user's area:

          **Searches should include:**

          1. **Impact Exemplars:** Papers the user considers high-impact in the field
             - What framework did they use (implicitly or explicitly)?
             - How did they score on the axes?
             - What made them successful?

          2. **Analogous Projects:** Similar approaches or systems
             - How were they evaluated?
             - What impact did they achieve?
             - What can be learned from their framing?

          3. **Field Expectations:** What's typical for the area?
             - Are basic science papers common?
             - Is technology development valued?
             - What level of impact is "good enough"?

          **Questions to ask the user:**
          - What papers should be analyzed as benchmarks?
          - What search terms capture the field's impact exemplars?
          - Are there specific journals or authors whose framing to emulate?

          ### Phase 9: Communication Strategy

          Once the framework is selected, here's how to lead with it:

          #### In Talks:
          **Opening Frame (within first 2 slides):**
          - "The goal of this work is to understand [fundamental process X] in [general system Y]" → Basic science
          - "We're developing a technology that will enable [critical experiment X] for [community Y]" → Technology
          - "This invention addresses [problem X] affecting [N] people" → Application

          #### In Papers:
          **Abstract Structure:**
          - State your framework implicitly through word choice
          - Basic science: "reveals," "demonstrates," "shows that"
          - Technology: "enables," "provides," "makes it possible to"
          - Application: "solves," "addresses," "improves"

          #### In Grants:
          **Broader Impact Section:**
          - Explicitly name your evaluation framework
          - Provide the two-axis assessment
          - Score yourself on each axis with evidence

          #### With Your PI/Committee:
          **Alignment Conversation:**
          - "I want to make sure we're aligned on how this should be evaluated"
          - "I see this as [framework], scoring [X] on [axis 1] and [Y] on [axis 2]"
          - "Do you agree, or do you see it differently?"
          - "This matters because..." [explain downstream implications]

          ## Output Deliverable

          Claude should produce a **2-page Impact Assessment Document**:

          ### Page 1: Framework and Scoring

          #### Project Categorization:
          - **Type:** Basic Science / Technology Development / Invention / Custom
          - **Rationale:** [Why this categorization fits]

          #### Optimization Function:
          - **X-Axis:** [Metric name and definition]
          - **Y-Axis:** [Metric name and definition]
          - **Custom Rationale (if applicable):** [Why standard metrics don't fit]

          #### Self-Assessment:

          **X-Axis Score: [Low/Medium/High]**
          - Evidence: [Specific reasons for this score]
          - Examples: [Comparable projects or benchmarks]
          - PubMed Support: [Key papers that inform assessment]

          **Y-Axis Score: [Low/Medium/High]**
          - Evidence: [Specific reasons for this score]
          - Examples: [Comparable projects or benchmarks]
          - PubMed Support: [Key papers that inform assessment]

          **Overall Assessment:**
          - Score on at least one axis: ☑ Yes / ☐ No
          - Strong justification: ☑ Yes / ☐ No
          - Aligned with your values: ☑ Yes / ☐ No

          #### Visual Framework:
          ```
                   [Your Project Type]
                        
          Y-Axis    |           ★ Your Project
          [Metric]  |         /
                    |       /
                    |     /
                    |   /
                    |_________________
                        X-Axis [Metric]
                        
          ★ = Your project
          Reference projects plotted for context
          ```

          ### Page 2: Communication and Alignment

          #### Value System Alignment:
          - **What Drives You:** [Discovery/Enabling/Problem-solving/Building]
          - **Success Definition:** [What would make this worthwhile]
          - **Approval Sources:** [Whose opinion matters and why]
          - **Framework Fit:** [How project aligns with values]

          #### Potential Mismatches to Avoid:
          1. [Specific mismatch type]
             - Scenario: [When this might happen]
             - Prevention: [How to frame to avoid it]

          2. [Another mismatch]
             - Scenario: [When this might happen]
             - Prevention: [How to frame to avoid it]

          #### Communication Strategy:

          **For Talks:**
          - Opening frame: [Exact language to use in first 2 slides]
          - Key phrases: [Vocabulary that signals your framework]

          **For Papers:**
          - Abstract structure: [Framework-appropriate language]
          - Impact statement: [How to articulate in discussion]

          **For Grants:**
          - Broader impact: [How to score yourself explicitly]
          - Justification: [Evidence for scores]

          **For Mentors:**
          - Alignment question: [Exact question to ask]
          - Your perspective: [How you see it]
          - Discussion points: [What matters for alignment]

          #### Comparative Analysis:

          | Project | Type | X-Score | Y-Score | Notes |
          |---------|------|---------|---------|-------|
          | Yours | [Type] | [L/M/H] | [L/M/H] | [Key strengths] |
          | Benchmark 1 | [Type] | [L/M/H] | [L/M/H] | [What you can learn] |
          | Benchmark 2 | [Type] | [L/M/H] | [L/M/H] | [What you can learn] |
          | Alternative | [Type] | [L/M/H] | [L/M/H] | [Why not pursuing] |

          #### Action Items:
          1. [Specific step to strengthen X-axis score or argument]
          2. [Specific step to strengthen Y-axis score or argument]
          3. [Communication alignment with key stakeholders]

          ## Practical Examples

          ### Example 1: Ribosome Stalling (Basic Science)
          - **Framework:** Basic science
          - **X-Axis (Generality):** HIGH—translation is universal
          - **Y-Axis (Learning):** MEDIUM—mechanism of one quality control system
          - **Assessment:** High on generality alone = substantial impact
          - **Communication:** "Updates our understanding of translation quality control"

          ### Example 2: BLAST (Technology)
          - **Framework:** Technology development
          - **X-Axis (Widely Used):** VERY HIGH—used by virtually all molecular biologists
          - **Y-Axis (Critical):** LOW-MEDIUM—helpful but rarely essential
          - **Assessment:** Extreme breadth of use = enormous cumulative impact
          - **Communication:** "Enables rapid sequence comparison across all biological databases"

          ### Example 3: Cryo-EM Tomography (Technology)
          - **Framework:** Technology development
          - **X-Axis (Widely Used):** LOW—complex, expensive, specialized
          - **Y-Axis (Critical):** VERY HIGH—generates impossible-to-get-otherwise data
          - **Assessment:** Extreme criticality for niche = high impact
          - **Communication:** "Enables 3D visualization of molecular machines in native cellular context"

          ### Example 4: Foldscope (Invention)
          - **Framework:** Invention (custom metric)
          - **X-Axis (Good):** MEDIUM—functional microscopy
          - **Y-Axis (People):** VERY HIGH—millions of students globally
          - **Assessment:** Massive reach × modest utility = transformative for education
          - **Communication:** "Democratizes microscopy for global education"

          ## Key Principles to Remember

          1. **Value Is in the Eye of a Belief System:** Make yours explicit.

          2. **Lead with Your Metric:** Don't assume others share your framework.

          3. **Either Axis Suffices:** You don't need both—just score well on one.

          4. **Articulate Early:** Discuss with mentors before you're 2 years in.

          5. **Avoid Default State:** Work actively against irrelevance/non-use.

          6. **Compare, Don't Absolute:** Even rough comparison beats ignoring impact.

          7. **Align Communication:** Your words should signal your framework.

          8. **Match Project to Values:** Life is too short for misaligned work.

          ## Warning Signs

          **Warning signs include:**
          - Inability to articulate which framework applies
          - Scoring LOW on both axes
          - Project type and evaluation framework don't match
          - User and PI have different frameworks but haven't discussed it
          - Using basic science metrics for a tool or vice versa
          - Never explicitly discussing impact assessment

          **Good shape indicators:**
          - Clear statement of optimization function
          - MEDIUM-HIGH score on at least one axis
          - Framework matches project type
          - Alignment with key stakeholders
          - Communication signals framework clearly
          - Benchmarking against comparable work

          ## Getting Started

          Claude should begin Phase 1 by asking:
          1. What is the primary goal? (A/B/C/D)
          2. What would success look like in 3-5 years?
          3. Who cares if this succeeds?

          Together, Claude and the user will select the right optimization function and position the work for maximum impact.

          ---

          *Remember: Impact assessment isn't about ego—it's about ensuring work matters in the way the scientist wants it to matter. Explicit framing prevents years of misalignment.*
    - path: /knowledge-work-plugins/skills/scientific-problem-selection/references/04-parameter-strategy.md
      text: |
          # SKILL 4: Parameter Fixation Strategy

          ## Overview
          This skill helps scientists strategically decide which parameters to fix and which to keep flexible in their project. The paradox: too many fixed parameters creates brittleness, but too few causes paralysis. The key is fixing ONE parameter thoughtfully and letting others float—constraints engender creativity.

          ## Core Principle

          **"Fix one parameter; let the others float."**

          Most failure modes in ideation involve fixing too many parameters at the outset (system + method + application). Conversely, statements like "I want to do impactful work in cell engineering" are so broad they cause paralysis. The sweet spot: fix one meaningful constraint and let creativity flow within that boundary.

          ## What Are Project Parameters?

          Parameters are the choices that define your project:

          **Common Parameters:**
          - **System:** Which organism/cell type/tissue/molecule?
          - **Question:** What biological phenomenon to study?
          - **Tool/Method:** Which experimental approach?
          - **Application:** What practical use or goal?
          - **Output:** What form will results take?
          - **Collaborators:** Who will you work with?
          - **Timeline:** How fast must you move?
          - **Resources:** What's available/necessary?

          ## The Skill Workflow

          ### Phase 1: Parameter Inventory (10 minutes)

          First, let's identify what's already fixed in your current project idea:

          **Question 1: List your project parameters**

          For each category, indicate if it's **FIXED** (must stay) or **FLOATING** (could change):

          | Parameter Type | Your Choice | Status (F/FL) | Why Fixed? |
          |----------------|-------------|---------------|------------|
          | **System** | [organism/cell/tissue] | F / FL | [reason] |
          | **Question** | [biological phenomenon] | F / FL | [reason] |
          | **Tool/Method** | [techniques] | F / FL | [reason] |
          | **Application** | [use case/goal] | F / FL | [reason] |
          | **Timeline** | [duration] | F / FL | [reason] |
          | **Resources** | [equipment/funding] | F / FL | [reason] |

          **Question 2: Count your fixed parameters**
          - How many did you mark as FIXED? _____
          - If >2, you may have over-constrained the problem

          **Question 3: Why are they fixed?**
          For each fixed parameter, is it because:
          A. Your expertise/passion
          B. Lab resources/capabilities  
          C. Advisor requirements
          D. You think it's the "best" solution
          E. Historical accident (you started this way)

          ### Phase 2: The GLP-1 Example (Case Study)

          Let's learn from a concrete example:

          **Proposed Project:** Engineer a T cell to produce GLP-1 (glucagon-like peptide-1) for continuous supply.

          **Analysis: What's Fixed?**
          1. Improving GLP-1 receptor agonist delivery characteristics (the problem)
          2. Using an engineered T cell (the solution)

          **Problem:** Two parameters fixed = poor technique-application match

          **Alternative Framings:**

          **If you fix Parameter 1 (GLP-1 delivery):**
          - Let the solution float
          - Better options: peptide engineering for extended half-life, oral peptides, small molecules, B cells (better protein secretion)
          - Why T cell is suboptimal: Not designed for protein secretion
          - **Best for:** Trainee in metabolism lab who cares about GLP-1

          **If you fix Parameter 2 (Engineered T cell):**
          - Let the application float
          - Better options: local-acting peptides (cytokines, chemokines, growth factors) for oncology/autoimmunity/regeneration
          - Why GLP-1 is suboptimal: Doesn't leverage T cell's natural capabilities
          - **Best for:** Trainee in immunology/cell engineering lab

          **Key Insight:** Which parameter you fix depends on YOUR interests and your lab's expertise. Both can lead to great projects, but they're DIFFERENT projects.

          ### Phase 3: Diagnostic Questions

          **The Goldilocks Test:**

          **Too Many Fixed Parameters (>2):**
          - Are you forcing a technique-application match?
          - If one assumption fails, does everything fail?
          - Are you more attached to HOW than WHAT?
          - Does your project sound like: "Use X to do Y in Z"?

          **Too Few Fixed Parameters (0-1 very broad):**
          - Do you feel paralyzed where to start?
          - Is your statement super generic? ("Do impactful work in...")
          - Are you avoiding commitment?
          - Do you have decision fatigue?

          **Just Right (1-2 well-chosen):**
          - Do you have creative constraints?
          - Can you articulate why THIS constraint matters?
          - If one approach fails, do alternatives exist?
          - Does the constraint energize you?

          ### Phase 4: The Illumina Example (Constraints Drive Innovation)

          **Historical Context:** Next-generation sequencing wasn't designed; we got Illumina's approach (many short reads).

          **Initial Constraint:** Short reads seemed like a limitation
          - Not what we would have "asked for"
          - Seemed inferior to long reads

          **Innovation Unleashed:**
          - Computational methods (assembly algorithms)
          - Novel applications (RNA-seq, ChIP-seq, ATAC-seq)
          - Unexpected uses (protein folding via sequencing)
          - Biochemical creativity to work within constraints

          **Lesson:** Constraints don't limit creativity—they focus it. If you feel stuck, fix ONE parameter and watch resourcefulness emerge.

          ### Phase 5: Which Parameter Should You Fix?

          **Strategic Questions to Identify the Right Fixed Parameter:**

          1. **What can you prototype quickly?**
             - What test article could you build rapidly?
             - Which experimental conditions enable early go/no-go?
             - What gives you fastest feedback?

          2. **What are people around you unusually good at?**
             - Lab expertise?
             - Core facility capabilities?
             - Collaborator strengths?
             - Your unique skill combination?

          3. **What do you enjoy so much you don't think of it as work?**
             - System you're passionate about?
             - Technique you love?
             - Type of question that excites you?

          4. **What's your competitive advantage?**
             - Unique resource access?
             - Rare skill combination?
             - Proprietary data/reagents?
             - First-mover opportunity?

          **Common Strategic Choices:**

          **Fix the System (Let question & tool float):**
          - Good if: You're an expert in the organism/tissue/cell type
          - Enables: Asking multiple questions, trying various tools
          - Example: "I study *Drosophila* neural development; I'll let the specific questions and methods emerge"

          **Fix the Question (Let system & tool float):**
          - Good if: You care deeply about a biological phenomenon
          - Enables: Testing across systems, using best tool for each
          - Example: "I want to understand phase separation; I'll study it wherever it's clearest"

          **Fix the Tool (Let system & question float):**
          - Good if: You're developing or mastering a technology
          - Enables: Finding best applications, comparing across systems
          - Example: "I'm building a new microscopy method; I'll find the most impactful uses"

          **Fix the Application (Let system & tool float):**
          - Good if: You have a specific translational goal
          - Enables: Trying multiple approaches, testing in different models
          - Example: "I want to treat disease X; I'm open to any validated approach"

          ### Phase 6: Parameter Flexibility Matrix

          For your project, let's create a flexibility assessment:

          | Parameter | Currently | Should Be? | If Problem Arises, Could This Float? |
          |-----------|-----------|------------|--------------------------------------|
          | System | [F/FL] | [F/FL] | Yes / No / Maybe |
          | Question | [F/FL] | [F/FL] | Yes / No / Maybe |
          | Tool | [F/FL] | [F/FL] | Yes / No / Maybe |
          | Application | [F/FL] | [F/FL] | Yes / No / Maybe |
          | Timeline | [F/FL] | [F/FL] | Yes / No / Maybe |
          | Resources | [F/FL] | [F/FL] | Yes / No / Maybe |

          **Analysis:**
          - **Flexibility Score:** How many "Yes" or "Maybe"? _____
          - **Risk Assessment:** If <3 can float, you're brittle
          - **Pivot Potential:** Which parameters provide escape routes?

          ### Phase 7: Scenario Planning

          For each fixed parameter, let's plan what happens if it becomes untenable:

          **Fixed Parameter 1: [Name it]**
          - **Why it's fixed:** [Your reason]
          - **Risk if this fails:** [What breaks]
          - **Contingency:** [What could you float instead]
          - **Alternative project:** [If you fixed something else]

          **Fixed Parameter 2: [Name it]**
          - **Why it's fixed:** [Your reason]  
          - **Risk if this fails:** [What breaks]
          - **Contingency:** [What could you float instead]
          - **Alternative project:** [If you fixed something else]

          ### Phase 8: The Unfixing Exercise

          Sometimes you need to unfix parameters to escape a rut:

          **Current State:** [Describe your over-constrained project]

          **Unfixing Experiment:**

          **Try 1: Unfix the System**
          - Keep question & tool
          - What other systems could you study?
          - Which would be easier/faster/more informative?

          **Try 2: Unfix the Tool**
          - Keep system & question
          - What other methods exist?
          - Which are more mature/accessible/powerful?

          **Try 3: Unfix the Question**
          - Keep system & tool
          - What other questions could you ask?
          - Which would be more impactful/feasible?

          **Evaluation:** Does any "unfixed" version seem better than your original? If yes, you over-constrained.

          ### Phase 9: Literature Reality Check

          Let's use PubMed to see how others handled parameter fixation:

          **Search 1: Successful projects in your area**
          - What did they fix?
          - What did they let float?
          - Did they pivot from their initial parameter choices?

          **Search 2: Failed or stalled projects**
          - (Often in discussion sections or preprints)
          - Did they over-constrain?
          - What parameters trapped them?

          **Search 3: Method papers**
          - How did technology developers choose applications?
          - Did they fix the tool and let applications emerge?

          **Your Searches:**
          What specific papers should we analyze for parameter lessons?

          ## Output Deliverable

          **2-Page Parameter Strategy Document**

          ### Page 1: Current State and Analysis

          #### Parameter Inventory:
          | Parameter | Current Status | Strategic Rationale | Flexibility |
          |-----------|----------------|---------------------|-------------|
          | System | Fixed: [X] | [Why] | Can float if: [condition] |
          | Question | Floating: [Y,Z] | [Why] | Constrained by: [X] |
          | Tool | [Status] | [Why] | [Contingency] |
          | Application | [Status] | [Why] | [Contingency] |

          #### Diagnostic Summary:
          - **Fixed Parameters:** [Count and list]
          - **Assessment:** ☐ Too Many (>2) / ☐ Just Right (1-2) / ☐ Too Few (0, too broad)
          - **Primary Fixed Parameter:** [The one that matters most]
          - **Reason for Fixation:** [Expertise/Passion/Resources/Other]

          #### Goldilocks Test Results:
          - Over-constrained indicators: [Yes/No to each test]
          - Under-constrained indicators: [Yes/No to each test]
          - Verdict: [Analysis]

          ### Page 2: Strategy and Contingencies

          #### Recommended Parameter Strategy:

          **Core Fixed Parameter:** [The one to keep]
          - **Rationale:** [Why this one]
          - **Your advantage:** [Expertise/access/passion]
          - **Enables:** [What becomes possible]

          **Parameters That Should Float:** [List]
          - [Parameter 1]: [How to explore alternatives]
          - [Parameter 2]: [How to explore alternatives]

          #### If Core Assumptions Fail:

          **Scenario 1: [Specific failure mode]**
          - **Unfix:** [Which parameter to let float]
          - **Alternative 1:** [New configuration]
          - **Alternative 2:** [Another option]

          **Scenario 2: [Another failure mode]**
          - **Unfix:** [Which parameter]
          - **Alternative 1:** [New configuration]
          - **Alternative 2:** [Another option]

          #### Project Ensemble:
          ```
          Core Fixed: [X]

          Possible Projects:
          1. [X] + [A] + [B1] → [Outcome]
          2. [X] + [A] + [B2] → [Outcome]
          3. [X] + [C] + [B1] → [Outcome]

          All share [X], but float other parameters
          ```

          #### Strategic Questions Answered:
          1. **Quick prototype:** [How to test quickly]
          2. **Team strengths:** [Who's good at what]
          3. **Your passion:** [What energizes you]
          4. **Competitive advantage:** [Your edge]

          #### Historical Parallel:
          [Example like Illumina where constraints drove innovation in your field]
          - The constraint: [What seemed limiting]
          - The innovation: [How people worked within it]
          - Your application: [How this applies to your project]

          ## Practical Examples

          ### Example 1: GLP-1 T Cell Project (Over-Constrained)
          - **Fixed:** GLP-1 delivery + T cell engineering
          - **Problem:** Poor technique-application match
          - **Solution:** Unfix one parameter
            - Fix delivery, float cell type → Better options emerge
            - Fix T cell, float payload → Better applications emerge

          ### Example 2: Drosophila Neurobiologist (Well-Constrained)
          - **Fixed:** *Drosophila* nervous system
          - **Floating:** Specific questions, methods
          - **Works because:** Deep system expertise, many tools available
          - **Enables:** Pursuing most impactful questions as field evolves

          ### Example 3: "Impactful Cell Engineering" (Under-Constrained)
          - **Fixed:** Nothing specific
          - **Problem:** Paralysis from too many options
          - **Solution:** Fix one meaningful constraint
            - Option A: Fix CAR-T platform → Find best applications
            - Option B: Fix autoimmune disease → Find best cell engineering approach
            - Option C: Fix specific rare disease → Let methods emerge

          ## Key Principles to Remember

          1. **Constraints Engender Creativity:** Limitations focus resourcefulness

          2. **One Parameter Rule:** Fix one meaningful constraint, let others float

          3. **Match to Your Strengths:** Fix the parameter you have advantage in

          4. **Technique-Application Match:** Don't force tools into wrong problems

          5. **Flexibility = Resilience:** Floating parameters provide pivot options

          6. **Historical Lesson:** Best technologies emerged from working within constraints (Illumina)

          7. **Not Forever:** Parameters can unfix mid-project when stuck

          ## Warning Signs

          **Over-Constrained (Too Many Fixed):**
          - Project sounds like: "Use X to study Y in Z"
          - When one assumption fails, everything fails
          - You're attached to HOW more than WHAT
          - Forcing a technique-application match

          **Under-Constrained (Too Few/Vague):**
          - Statement is incredibly broad ("impactful work in...")
          - Feeling paralyzed about where to start
          - Avoiding commitment due to infinite options
          - No clear next experimental step

          **Well-Constrained:**
          - One clear fixed parameter with good rationale
          - Multiple paths within that constraint
          - Energized by the focused challenge
          - If one approach fails, alternatives exist

          ## Ready to Begin?

          Let's start with Phase 1. Please provide:
          1. Your current project description
          2. List of what you think is fixed vs. floating
          3. Your lab's core expertise
          4. What aspect excites you most

          Together we'll optimize your parameter strategy for maximum creativity and resilience.

          ---

          *Remember: The right constraint is liberating, not limiting. It channels creativity into productive directions while maintaining flexibility for pivots.*
    - path: /knowledge-work-plugins/skills/scientific-problem-selection/references/05-decision-tree.md
      text: |
          # SKILL 5: Decision Tree Navigation ("The Altitude Dance")

          ## Overview
          This skill teaches you to move fluidly between execution (Level 1: getting stuff done) and strategic evaluation (Level 2: critical thinking). Projects rarely unfold linearly—they require frequent course correction. Most trainees should spend MORE time on their project's decision tree.

          ## Core Principle
          **"Learn the altitude dance"**

          Move back and forth frequently between:
          - **Level 1:** Full immersion in experimental details or coding
          - **Level 2:** Step back, clear your head, evaluate as if someone else did the work

          These cannot be done simultaneously. The key to navigating a project's decision tree is alternating between these levels deliberately.

          ## Key Concepts

          **Why Decision Trees Matter:**
          Once you're in a project, the landscape changes:
          - You've learned from initial experiments
          - New papers have been published
          - Technology has advanced
          - Your assumptions have been tested

          At any decision point, you should rarely follow your plan from 2 years ago—there will be a better alternative.

          **The Altitude Levels:**
          - **Level 1 (Ground Level):** Doing the work, troubleshooting, optimizing
          - **Level 2 (Strategic Altitude):** What did we learn? What should we do next?
          - **Level 3 (Field Altitude):** How does this fit in the broader landscape?
          - **Level 4 (Career Altitude):** Is this the right use of my finite time?

          **Common Failure Modes:**
          1. **Stuck in Level 1:** Troubleshooting endlessly without reassessing the plan
          2. **Only Level 2:** Brilliant strategist but never rolls up sleeves
          3. **No rhythm:** Switching randomly instead of deliberately

          ## Workflow

          ### Phase 1: Map Your Decision Tree

          For your project, identify:
          1. **Initial plan:** What was the intended path?
          2. **Branch points:** Where might alternative paths emerge?
          3. **Decision criteria:** What determines which branch to take?
          4. **New information:** What could change the landscape?

          ### Phase 2: Establish Your Rhythm

          **Recommended Schedule:**
          - **Daily:** Level 1 work (experiments, coding, analysis)
          - **Weekly:** Level 2 evaluation (1-2 hours, ideally Friday afternoon)
          - **Monthly:** Level 3 field review (read new papers, attend seminars)
          - **Quarterly:** Level 4 career check-in (with mentor)

          **Level 2 Weekly Protocol:**
          1. Clear your head (walk, coffee, change of scene)
          2. Review what happened this week
          3. Ask: What did we learn?
          4. Ask: What should happen next?
          5. Update decision tree
          6. Plan next week's Level 1 work

          ### Phase 3: Decision Points

          At each major branch point:

          **Example: Genetic Screen Hits Wall**

          Instead of endless troubleshooting:
          - **Alternative 1:** Redo computational analysis with larger genome dataset
          - **Alternative 2:** Use AlphaFold models to search for similar folds
          - **Alternative 3:** Print and test larger candidate set (DNA synthesis cheaper now)

          **Framework:**
          1. **Acknowledge the stuck point**
          2. **Step to Level 2:** Evaluate with fresh eyes
          3. **Consider: What's newly possible?** (technology, knowledge)
          4. **Generate 3 alternatives**
          5. **Decide:** Troubleshoot more vs. pursue alternative

          ## Output: Decision Tree Map
          - Visual map of your project's decision points
          - Update frequency schedule
          - Criteria for each branch point
          - Protocol for getting unstuck
    - path: /knowledge-work-plugins/skills/scientific-problem-selection/references/06-adversity-planning.md
      text: |
          # SKILL 6: Adversity Response Planning ("The Adversity Feature")

          ## Overview
          This skill helps you prepare for inevitable crises and reframe them as opportunities. The term "adversity feature" (like a "rock garden" on a mountain bike trail) captures the mindset: adversity is not an obstacle—it's an opportunity to develop skill and improve your project.

          ## Core Principle
          **"Capitalize on the 'adversity feature'"**

          Adversity in a project is inevitable AND opportune:
          - **Inevitable:** Almost every project suffers existential crisis or sharp turn
          - **Opportune:** Two valuable outcomes possible:
            1. Fix the problem AND upgrade the project simultaneously
            2. Develop reasoning-your-way-out skills (best growth opportunity)

          ## Key Concepts

          **Why Adversity Is Inevitable:**
          - Technology doesn't work as advertised
          - Biological assumptions prove false
          - You get scooped
          - Key collaborator leaves
          - Funding runs out
          - Results don't support hypothesis

          **Why Adversity Is Opportune:**
          - Forces you to think deeply about alternatives
          - Removes sunk-cost bias (path is blocked anyway)
          - Often leads to better projects than original plan
          - Develops critical problem-solving skills
          - Makes you resourceful

          **The Crisis Mindset:**
          - **Wrong:** "This is a disaster that delays me"
          - **Right:** "This is the crisis I've been waiting for—don't waste it"

          ## Workflow

          ### Phase 1: Anticipate Failure Modes

          For your project, list likely adversity scenarios:
          1. **Technical failures:** Method doesn't work, signal too low, etc.
          2. **Biological surprises:** System behaves unexpectedly
          3. **Competition:** Someone scoops you
          4. **Resource issues:** Funding, equipment, access
          5. **Timeline pressures:** Takes longer than expected

          For each, rate:
          - Likelihood (Low/Medium/High)
          - Impact if it happens (Low/Medium/High)
          - When it might surface (early/mid/late)

          ### Phase 2: Upgrade Opportunities

          For each high-likelihood or high-impact failure mode:

          **Question 1: How could you fix this AND make the project better?**
          Not just: "Get it working"
          Instead: "Use this as opportunity to improve the approach"

          **Example: Your Cell Type Can't Be Isolated**
          - Fix: Develop new isolation method
          - Upgrade: Make method work for whole class of cell types
          - Result: Better project (technology paper) + original biology

          **Question 2: What skill would you develop by solving this?**
          - Computational: Learn new analysis method
          - Technical: Master challenging technique
          - Conceptual: Reason through biological complexity

          ### Phase 3: The Ensemble View

          **Critical Insight:** You're not picking ONE project path—you're picking an ENSEMBLE of possible projects that share core elements.

          **Your Project Ensemble:**
          ```
          Core Theme: [What stays constant]

          Path 1: [Original plan]
          Path 2: [If assumption A fails]
          Path 3: [If technical barrier B encountered]
          Path 4: [If scooped on C]

          All paths lead to impactful results, just different ones
          ```

          This reframing is liberating: when adversity strikes, you're not failing—you're discovering which path in the ensemble you're actually on.

          ### Phase 4: Historical Examples

          **Example 1: PROTAC Discovery**
          - **Original Plan:** Create molecules to degrade specific kinase
          - **Crisis:** Didn't work for intended target
          - **Upgrade:** Test across kinome systematically
          - **Result:** Better project (mapped degradable kinome, discovered that target engagement ≠ degradation)
          - **Impact:** More influential than if original plan succeeded

          **Example 2: Steroid Receptor Study**
          - **Original Plan:** Identify THE receptor for a steroid
          - **Crisis:** Binds multiple receptors at different affinities
          - **Upgrade:** Reframe question: How does finite receptor pool sense infinite lipids?
          - **Result:** Combinatorial sensing model (like piano chords)
          - **Impact:** More interesting than "receptor X binds steroid Y"

          ## Output: Adversity Playbook

          **Page 1: Anticipated Crises**
          | Crisis | Likelihood | Impact | Timeline | Growth Opportunity |
          |--------|-----------|--------|----------|-------------------|
          | [Crisis 1] | H/M/L | H/M/L | Early/Mid/Late | [Skill developed] |

          **Page 2: Upgrade Strategies**
          For each high-priority crisis:
          - **The Crisis:** [Description]
          - **Fix Strategy:** [How to solve it]
          - **Upgrade Strategy:** [How to make project better while fixing]
          - **Alternative Path:** [New direction if fix doesn't work]
          - **Ensemble Position:** [How this fits in project family]

          **Page 3: Resilience Rituals**
          - **Weekly check-in:** Review what went wrong, what was learned
          - **Monthly ensemble review:** Update the family of possible projects
          - **Crisis protocol:** When major setback hits, take 2 days to think before acting
          - **Growth tracking:** Document skills developed through adversity
    - path: /knowledge-work-plugins/skills/scientific-problem-selection/references/07-problem-inversion.md
      text: |
          # SKILL 7: Problem Inversion Strategies ("Turn It On Its Head")

          ## Overview
          This skill provides three concrete strategies for navigating around obstacles by reframing problems. When stuck, instead of pushing harder on the current approach, try inverting the problem.

          ## Core Principle
          **"Turn a problem on its head"**

          Three powerful strategies:
          1. **Unfix parameters** (covered in Skill 4, applied here in crisis)
          2. **Don't achieve goal A? Achieve comparable goal B**
          3. **"I have the answer; what is the question?"**

          ## Strategy 1: Unfix Parameters (In Crisis Mode)

          **When to Use:** Run-of-the-mill issues in project execution

          **Approach:** Let a "sacred" fixed parameter float

          **Example from Lecture:**
          - **Stuck:** Spatial transcriptomics of APC-T cell interactions in tumor microenvironment
          - **All fixed:** Technique, cell types, context
          - **Inversion:**
            - Unfix technique → What else could measure these interactions?
            - Unfix cell types → What other interactions matter in tumors?
            - Unfix context → Where else do APC-T interactions matter?

          **Your Application:**
          For each fixed parameter in your project:
          - What if this floated?
          - What alternatives exist?
          - Which would be easier/faster/more informative?

          ## Strategy 2: Comparable Goal Substitution

          **When to Use:** Existential threats to project (can't achieve original goal)

          **Approach:** Achieve a different but equally valuable goal

          **Mindset Shift:**
          - **Wrong:** "I failed to do X"
          - **Right:** "The world needs Y instead, which I CAN do"

          **Example from Lectures: PROTAC Story**
          - **Goal A (Failed):** Degrade specific therapeutic target
          - **Goal B (Achieved):** Map which kinases ARE degradable
          - **Value:** B is more impactful (general principle + method validation)
          - **Learning:** Target engagement ≠ degradation (important discovery)

          **Framework:**
          1. **Original goal:** [What you wanted]
          2. **Why it failed:** [Specific reason]
          3. **What CAN you do with current data/tools:** [Capabilities]
          4. **Comparable goals:**
             - Option 1: [Different but related goal]
             - Option 2: [Another alternative]
             - Option 3: [Yet another]
          5. **Which is most valuable:** [Analysis]
          6. **How to frame it:** [Communication strategy]

          ## Strategy 3: Answer Seeking Question

          **When to Use:** End-of-project challenges (interpretation, framing, application)

          **Approach:** You got an answer, but not to your original question. What question DOES your data answer?

          **Mindset Shift:**
          - **Wrong:** "This doesn't answer my question"
          - **Right:** "What interesting question does this answer?"

          **Example from Lectures: Steroid Receptor**
          - **Original Question:** What is THE receptor for this steroid?
          - **Answer Obtained:** Binds multiple receptors at different affinities
          - **Problem:** Can't answer original question (no single receptor)
          - **Inversion:** "What question does this answer?"
          - **New Question:** How does finite receptor pool sense infinite lipids?
          - **Answer:** Combinatorial sensing (pattern = unique "chord")
          - **Impact:** More interesting than intended finding

          **Framework:**
          1. **Original question:** [What you asked]
          2. **Data obtained:** [What you actually found]
          3. **Why it doesn't answer:** [The mismatch]
          4. **What DOES the data show clearly:** [Solid findings]
          5. **What questions could these answer:**
             - Question 1: [Option]
             - Question 2: [Option]
             - Question 3: [Option]
          6. **Which is most interesting:** [Assessment]
          7. **How to reframe paper/project:** [New framing]

          ## Workflow

          ### Phase 1: Identify Your Obstacle
          - **Type:** Technical / Biological / Competitive / Interpretive
          - **Severity:** Run-of-mill / Existential / End-stage
          - **Description:** [What's blocking you]

          ### Phase 2: Select Strategy

          | Obstacle Type | Recommended Strategy |
          |--------------|---------------------|
          | Technical barrier, mid-project | Strategy 1 (Unfix parameters) |
          | Can't achieve original goal | Strategy 2 (Comparable goal) |
          | Have data, unclear what it means | Strategy 3 (Answer seeking question) |

          ### Phase 3: Apply Strategy

          Work through the relevant framework above with your specific situation.

          ### Phase 4: Evaluate Alternatives

          For each alternative generated:
          - **Scientific value:** How interesting is this?
          - **Feasibility:** How hard to execute?
          - **Timeline:** How long will it take?
          - **Impact:** How does this compare to original plan?
          - **Your advantage:** Do you still have edge here?

          ## Output: Problem Inversion Analysis

          **Page 1: Current Situation**
          - **Obstacle:** [Clear description]
          - **Why you're stuck:** [Root cause]
          - **Original plan:** [What you intended]
          - **Current capability:** [What you CAN do]

          **Page 2: Strategy Applications**

          **Strategy 1 (Unfix Parameters):**
          | Fixed Parameter | If This Floated | Alternative Approaches | Assessment |
          |----------------|-----------------|----------------------|------------|
          | [Param 1] | [Consequences] | [Options] | [Value] |

          **Strategy 2 (Comparable Goals):**
          | Original Goal | Why It Failed | Comparable Goal | Value Assessment |
          |--------------|---------------|----------------|------------------|
          | [Goal A] | [Reason] | [Goal B] | [Compare impact] |

          **Strategy 3 (Answer → Question):**
          - **Data obtained:** [What you have]
          - **Question 1 it could answer:** [Option 1]
          - **Question 2 it could answer:** [Option 2]
          - **Question 3 it could answer:** [Option 3]
          - **Most interesting:** [Selection + reasoning]

          **Page 3: Recommended Path**
          - **Selected strategy:** [1, 2, or 3]
          - **New direction:** [Specific plan]
          - **Why this is better:** [Not just "it works" but "it's more interesting"]
          - **Communication approach:** [How to frame this pivot]
          - **Timeline:** [New schedule]
    - path: /knowledge-work-plugins/skills/scientific-problem-selection/references/08-integration-synthesis.md
      text: |
          # SKILL 8: Integration and Synthesis

          ## Overview
          This final individual skill synthesizes all previous skills into a coherent project plan and communication strategy. You'll create a complete package that demonstrates thoughtful problem selection and rigorous planning.

          ## Core Principle
          **"Tell a compelling story with your choices"**

          Humans love stories. Your project should have:
          - **Setting:** Background and problem framing
          - **Problem statement:** Clear, general enough to be interesting, specific enough to be distinctive
          - **New idea/approach:** Your angle (perturbation/measurement/theory: logic vs. technology)
          - **Iteration:** Loop of "we wondered X → did Y → found Z → interpreted as W"
          - **Conclusion:** What we learned and/or what's now possible
          - **Passion:** Authentic enthusiasm

          ## Workflow

          ### Phase 1: Gather Your Skill Outputs

          Collect your completed documents:
          - ☐ Skill 1: Problem Ideation Document
          - ☐ Skill 2: Risk Assessment Matrix
          - ☐ Skill 3: Impact Assessment Document
          - ☐ Skill 4: Parameter Strategy Document
          - ☐ Skill 5: Decision Tree Map
          - ☐ Skill 6: Adversity Playbook
          - ☐ Skill 7: Problem Inversion Analysis (if applicable)

          ### Phase 2: Create Narrative Arc

          **Story Structure for Your Project:**

          **1. Setting (Background)**
          - What's known in the field?
          - What's the gap or opportunity?
          - Why does this matter?

          **2. Problem Statement**
          - General enough: connects to broad principle
          - Specific enough: distinctive and tractable
          - Your framing from Skill 1

          **3. Your Approach**
          - Perturbation/Measurement/Theory
          - Logic vs. Technology
          - What's novel about your angle (from Skill 1)
          - How your optimization function shapes approach (from Skill 3)

          **4. Strategy**
          - Fixed vs. floating parameters (from Skill 4)
          - Decision points mapped out (from Skill 5)
          - Risk mitigation built in (from Skill 2)
          - Adversity contingencies (from Skill 6)

          **5. Why You**
          - Your competitive advantage
          - Lab expertise
          - Your passion and alignment
          - Timeline and resources

          ### Phase 3: Communication Formats

          **Format 1: 3-Slide, 5-Minute Presentation**

          **Slide 1: The Opportunity**
          - Setting + Problem statement
          - One key figure or schematic
          - Why this matters (optimization function)

          **Slide 2: Your Approach**
          - New idea/angle
          - Key experiments or analyses
          - What makes this feasible
          - Decision tree highlights

          **Slide 3: Impact and Timeline**
          - What you'll learn or enable
          - Success metrics
          - Timeline with milestones
          - Your advantage

          **Slide Design Tips:**
          - Minimal text (bullets are fine here)
          - Strong visuals
          - Tell story, don't catalog facts
          - Passion shows through

          **Format 2: 1-Page Written Summary**

          **Paragraph 1:** Setting and problem (2-3 sentences)
          **Paragraph 2:** Your approach and novelty (3-4 sentences)
          **Paragraph 3:** Why it will work (risk mitigation, your advantage) (2-3 sentences)
          **Paragraph 4:** Impact and timeline (2-3 sentences)

          **Total:** ~250-300 words that could be abstract or summary

          **Format 3: 1-Minute Elevator Pitch**

          **Structure:**
          - "I'm working on [problem] because [why it matters]"
          - "Current approaches are limited by [gap]"
          - "My angle is [approach] which is novel because [what's new]"
          - "This will [impact] and I have [advantage]"

          **Practice until:** Natural, passionate, memorable

          ### Phase 4: Integration Document

          **Complete Project Plan Integrating All Skills:**

          **Section 1: Problem Selection Rationale**
          - How you generated this idea (Skill 1 intuition pumps)
          - Why this problem matters (Skill 3 optimization function)
          - Your competitive advantage

          **Section 2: Risk Management**
          - Assumption analysis table (Skill 2)
          - Go/no-go experiments
          - Timeline with checkpoints
          - Mitigation strategies

          **Section 3: Execution Strategy**
          - Fixed vs. floating parameters (Skill 4)
          - Decision tree navigation plan (Skill 5)
          - Adversity response protocols (Skill 6)
          - Project ensemble (alternative paths)

          **Section 4: Communication Plan**
          - Presentations (3-slide deck)
          - Written summary (1-page)
          - Elevator pitch (1-minute)
          - Key messages for different audiences

          **Section 5: Career Alignment**
          - How this fits your trajectory
          - Skills you'll develop
          - Network you'll build
          - Next steps after this project

          ## Output: Complete Project Package

          **Document 1: Integrated Project Plan (4-6 pages)**
          - All sections above
          - References to individual skill outputs
          - Timeline and milestones
          - Resource requirements

          **Document 2: Communication Materials**
          - 3-slide presentation
          - 1-page summary
          - Elevator pitch script
          - Talking points for different audiences

          **Document 3: Living Documents**
          - Decision tree (to update regularly)
          - Risk assessment (to review quarterly)
          - Adversity playbook (to consult in crisis)
          - Parameter strategy (to revisit if stuck)

          ## Key Principles

          1. **Integration, Not Duplication:** Each skill output serves a purpose in the whole
          2. **Story Over Catalog:** Communicate choices, not just facts
          3. **Passion Matters:** Authentic enthusiasm is persuasive
          4. **Living Plan:** This evolves; revisit quarterly
          5. **Alignment:** Project, values, and career fit together
          6. **Preparation:** You've thought through contingencies
          7. **Communication:** You can pitch this clearly to anyone

          ## Ready to Synthesize

          With all skills complete, you now have a comprehensive, thoughtful, rigorous approach to problem selection and project planning. This is the highest-leverage work you can do in science.
    - path: /knowledge-work-plugins/skills/scientific-problem-selection/references/09-meta-framework.md
      text: |
          # SKILL 9: Meta-Framework - Complete Problem Selection Workflow

          ## Overview
          This meta-skill orchestrates the complete problem selection process, guiding users through Skills 1-8 in a systematic, iterative way. This skill should be used when comprehensive support is needed from ideation through execution planning, with integrated literature searches and coherent documentation.

          ## When to Use This Skill

          **Use Skill 9 (Complete Workflow) when:**
          - Starting a new project from scratch
          - Major project pivot or reframe needed
          - Grant/fellowship application requiring systematic planning
          - Thesis committee meeting preparation
          - Startup company planning
          - Want comprehensive, documented problem selection process

          **Use Individual Skills when:**
          - You're at a specific stage (e.g., just need risk assessment)
          - Quick consultation on one aspect
          - Updating one component of existing plan
          - Teaching/learning one concept

          ## The Complete Workflow

          ### Overview of the Journey

          ```
          START: Vague idea or area of interest
              ↓
          [SKILL 1] → Problem Ideation Document
              ↓
          [SKILL 2] → Risk Assessment Matrix
              ↓
          [SKILL 3] → Impact Assessment Document
              ↓
          [SKILL 4] → Parameter Strategy Document
              ↓
          [SKILL 5] → Decision Tree Map
              ↓
          [SKILL 6] → Adversity Playbook
              ↓
          [SKILL 7] → Problem Inversion Analysis (if needed)
              ↓
          [SKILL 8] → Integrated Project Plan + Communication Materials
              ↓
          END: Comprehensive, rigorous project ready to execute
          ```

          **Estimated Time:**
          - **Intensive:** 1 week of focused work (full-time)
          - **Distributed:** 4-6 weeks with other commitments
          - **With iterations:** Add 50% more time

          **You'll invest time once to save years of potential missteps.**

          ## Phase-by-Phase Workflow

          ### Phase 1: Preparation (Before Starting)

          **Gather Your Context:**
          1. **Your background:**
             - Research area/field
             - Current position (grad student, postdoc, PI, etc.)
             - Lab expertise and resources
             - Timeline constraints

          2. **Your starting point:**
             - Vague area of interest?
             - Specific problem in mind?
             - Must build on existing work?
             - Starting completely fresh?

          3. **Your goals:**
             - Publication target (journal tier, timeline)?
             - Degree requirement (thesis chapter)?
             - Funding application?
             - Startup foundation?
             - Career development?

          **Set Expectations:**
          - This process will challenge your assumptions
          - You may discover your initial idea needs major revision
          - That's the point—better to know now than after 2 years
          - Intellectual honesty is required; this only works if you're rigorous

          ### Phase 2: Ideation (Skill 1) - ~1 week

          **What We'll Do:**
          1. Understand your context and constraints
          2. Work through relevant intuition pumps
          3. Avoid common ideation traps
          4. Generate 2-3 project ideas
          5. Preliminary literature search to calibrate scope
          6. Select most promising idea
          7. Create Problem Ideation Document (2 pages)

          **Literature Integration Point 1:**
          - Search PubMed for precedents and adjacent work
          - Assess generality of problem
          - Identify methodological advances
          - Determine competition level

          **Deliverable:**
          - Problem Ideation Document with core idea and initial analysis
          - List of 10-15 key papers
          - Preliminary assessment of novelty and feasibility

          **Checkpoint:** Do you have a clear, specific idea that excites you? If not, iterate on intuition pumps.

          ### Phase 3: Risk Analysis (Skill 2) - ~3-5 days

          **What We'll Do:**
          1. Extract ALL assumptions from your idea
          2. Categorize (biological vs. technical)
          3. Score each assumption (risk 1-5, time to test)
          4. Identify high-risk late-reading assumptions
          5. Design go/no-go experiments
          6. Develop mitigation strategies
          7. Create Risk Assessment Matrix (2 pages)

          **Literature Integration Point 2:**
          - Search for technical precedents (has method worked before?)
          - Find biological evidence (what's known about your system?)
          - Identify benchmarks (success rates, effect sizes)
          - Assess timeline realism

          **Deliverable:**
          - Complete assumption analysis table
          - Top 3 high-risk assumptions with mitigation plans
          - Go/no-go experiment designs
          - Revised timeline with decision points

          **Checkpoint:** Is your risk profile acceptable? If risk-5 assumptions are >2 years out, return to Skill 1 to reframe.

          ### Phase 4: Impact Assessment (Skill 3) - ~2-3 days

          **What We'll Do:**
          1. Categorize your project type
          2. Select appropriate optimization function
          3. Score yourself on both axes
          4. Compare to benchmarks
          5. Articulate value system alignment
          6. Develop communication strategy
          7. Create Impact Assessment Document (2 pages)

          **Literature Integration Point 3:**
          - Identify high-impact exemplars in your field
          - Analyze their framing and evaluation
          - Benchmark your potential impact
          - Understand field expectations

          **Deliverable:**
          - Clear optimization function selection
          - Self-assessment on both axes with justification
          - Comparative analysis vs. alternatives
          - Communication strategy for different audiences

          **Checkpoint:** Do you score MEDIUM-HIGH on at least one axis? If not, return to Skill 1 to find higher-impact angle.

          ### Phase 5: Parameter Strategy (Skill 4) - ~2-3 days

          **What We'll Do:**
          1. Inventory all project parameters
          2. Identify which are fixed vs. floating
          3. Assess if you're over/under-constrained
          4. Select strategic fixed parameter
          5. Plan flexibility for contingencies
          6. Create Parameter Strategy Document (2 pages)

          **Literature Integration Point 4:**
          - How did successful projects handle parameters?
          - What parameter choices led to breakthroughs?
          - What over-constraints caused failures?

          **Deliverable:**
          - Complete parameter inventory
          - Strategic rationale for fixed/floating decisions
          - Flexibility matrix for contingencies
          - Project ensemble (family of related projects)

          **Checkpoint:** Have you fixed 1-2 meaningful parameters while maintaining flexibility? If too rigid, adjust.

          ### Phase 6: Decision Tree Planning (Skill 5) - ~2 days

          **What We'll Do:**
          1. Map your project's decision tree
          2. Identify major branch points
          3. Set criteria for each decision
          4. Establish Level 1 / Level 2 rhythm
          5. Create protocols for getting unstuck
          6. Create Decision Tree Map (1-2 pages)

          **No major literature search here** (unless you identify specific decision points needing technical information)

          **Deliverable:**
          - Visual decision tree
          - Decision criteria at each branch
          - Schedule for Level 2 evaluations
          - Protocol for course correction

          **Checkpoint:** Have you planned for regular strategic evaluation, not just execution?

          ### Phase 7: Adversity Preparation (Skill 6) - ~2 days

          **What We'll Do:**
          1. Anticipate likely failure modes
          2. For each, identify upgrade opportunity
          3. Map your project ensemble
          4. Create crisis response protocols
          5. Create Adversity Playbook (2-3 pages)

          **Literature Integration Point 5:**
          - Historical examples of productive pivots
          - How did others capitalize on adversity?
          - What second-generation projects emerged from failures?

          **Deliverable:**
          - Anticipated crisis catalog
          - Upgrade strategies for each
          - Project ensemble map
          - Resilience rituals and protocols

          **Checkpoint:** Are you prepared to see adversity as opportunity? Have you planned how to upgrade, not just fix?

          ### Phase 8: Problem Inversion Toolkit (Skill 7) - ~1 day

          **What We'll Do:**
          1. Review three inversion strategies
          2. Pre-plan applications for your likely obstacles
          3. Create Problem Inversion Analysis (1-2 pages)

          **This is preparatory** - you may not need it now, but when crisis hits, you'll have framework ready.

          **Deliverable:**
          - Strategy 1 application planned
          - Strategy 2 options identified
          - Strategy 3 alternative questions brainstormed
          - Quick-reference guide for crisis

          **Checkpoint:** Do you have concrete strategies for inverting problems when stuck?

          ### Phase 9: Integration and Synthesis (Skill 8) - ~3-5 days

          **What We'll Do:**
          1. Review all outputs from Skills 1-7
          2. Create cohesive narrative
          3. Develop communication materials:
             - 3-slide presentation
             - 1-page summary
             - 1-minute elevator pitch
          4. Write integrated project plan (4-6 pages)
          5. Create living documents for ongoing use

          **Literature Integration Point 6:**
          - Final references for integrated plan
          - Key papers for each section
          - Communication examples from field leaders

          **Deliverable:**
          - Complete Integrated Project Plan (4-6 pages)
          - 3-slide presentation deck
          - 1-page written summary
          - Elevator pitch script
          - Living documents (decision tree, risk matrix, etc.)

          **Checkpoint:** Can you communicate your project compellingly in 1 minute, 5 minutes, and 1 page? Do all pieces fit together coherently?

          ## Iteration and Refinement

          ### When to Iterate

          **Red Flags That Require Going Back:**

          **From Skill 2 (Risk):**
          - Risk-5 assumptions >2 years out → Return to Skill 1 (reframe problem)
          - >3 risk-4-5 assumptions → Return to Skill 1 (simplify or change approach)

          **From Skill 3 (Impact):**
          - Score LOW on both axes → Return to Skill 1 (find higher-impact angle)
          - Optimization function mismatch → Return to Skill 1 (reframe problem)

          **From Skill 4 (Parameters):**
          - >2 fixed parameters → Return to Skill 1 (over-constrained)
          - Zero fixed parameters → Return to Skill 1 (under-constrained)

          **From Skills 5-6:**
          - No clear decision points → Return to Skill 4 (need more flexibility)
          - Every failure mode is existential → Return to Skill 2 (too risky)

          ### Iteration Protocol

          **Major Revision Needed:**
          1. **Pause and acknowledge:** The process is working—it caught a problem
          2. **Return to indicated skill:** Usually Skill 1 or 2
          3. **Bring forward what you learned:** Don't start from scratch
          4. **Revised idea → Run through workflow again:** Faster the second time
          5. **Multiple iterations OK:** Better than years on wrong project

          **Minor Refinement:**
          1. **Update specific document:** E.g., adjust parameter strategy
          2. **Check downstream effects:** Does this change anything else?
          3. **Update integration document:** Keep everything coherent

          ## Literature Integration Strategy

          ### Overall PubMed Approach

          **Throughout the workflow, use PubMed strategically:**

          1. **Skill 1 (Ideation):** Assess generality, find precedents, gauge competition
          2. **Skill 2 (Risk):** Technical feasibility, biological evidence, benchmarks
          3. **Skill 3 (Impact):** Field exemplars, evaluation frameworks, benchmarks
          4. **Skill 4 (Parameters):** Successful parameter choices, cautionary tales
          5. **Skill 6 (Adversity):** Productive pivots, upgrade examples
          6. **Skill 8 (Integration):** Communication models, comprehensive references

          **Search Strategy:**
          - Start broad (field overview)
          - Get specific (your exact approach)
          - Look adjacent (related systems/methods)
          - Find benchmarks (what's state-of-art?)
          - Identify competition (who else is doing this?)

          **Papers to Track:**
          - ~10-15 key papers from Skill 1
          - ~5-10 technical papers from Skill 2
          - ~5-10 impact exemplars from Skill 3
          - ~5 parameter lessons from Skill 4
          - ~3-5 pivot examples from Skill 6
          - **Total: ~30-50 papers** (your foundation)

          ## Final Deliverable Package

          ### What You'll Have at the End

          **Core Documents (Organized Folder):**
          1. `01_Problem_Ideation.pdf` (2 pages, Skill 1)
          2. `02_Risk_Assessment.pdf` (2 pages, Skill 2)
          3. `03_Impact_Assessment.pdf` (2 pages, Skill 3)
          4. `04_Parameter_Strategy.pdf` (2 pages, Skill 4)
          5. `05_Decision_Tree.pdf` (1-2 pages, Skill 5)
          6. `06_Adversity_Playbook.pdf` (2-3 pages, Skill 6)
          7. `07_Problem_Inversion.pdf` (1-2 pages, Skill 7)
          8. `08_Integrated_Plan.pdf` (4-6 pages, Skill 8)

          **Communication Materials:**
          - `Presentation_3slides.pptx`
          - `Summary_1page.pdf`
          - `Elevator_Pitch.txt`

          **Living Documents (for ongoing use):**
          - `Decision_Tree.pdf` (update monthly)
          - `Risk_Matrix.xlsx` (update quarterly)
          - `Adversity_Playbook.pdf` (consult in crisis)
          - `Parameter_Strategy.pdf` (revisit if stuck)

          **Reference Library:**
          - `Key_Papers.pdf` (annotated bibliography, 30-50 papers)
          - Organized by: Ideation / Technical / Impact / Pivots

          **Total: ~20-25 pages of documentation + supporting materials**

          ## Using Your Outputs

          ### For Different Purposes

          **Grant/Fellowship Applications:**
          - Start with Integrated Plan (Skill 8)
          - Include specific aims from Ideation (Skill 1)
          - Show risk mitigation from Risk Assessment (Skill 2)
          - Demonstrate impact from Impact Assessment (Skill 3)
          - Timeline from Decision Tree (Skill 5)

          **Thesis Committee Meetings:**
          - Present 3-slide deck (Skill 8)
          - Walk through decision tree (Skill 5)
          - Discuss risk mitigation (Skill 2)
          - Show parameter flexibility (Skill 4)
          - Demonstrate thoughtful planning

          **Lab Meetings:**
          - Use elevator pitch (Skill 8)
          - Show decision tree updates (Skill 5)
          - Discuss latest adversity and response (Skill 6)
          - Get input on parameter strategy (Skill 4)

          **Collaborator Conversations:**
          - Share 1-page summary (Skill 8)
          - Highlight where their expertise fits (Skill 4)
          - Show risk mitigation plan (Skill 2)
          - Discuss impact potential (Skill 3)

          **Personal Reflection:**
          - Quarterly: Review Decision Tree (Skill 5), update milestones
          - After setbacks: Consult Adversity Playbook (Skill 6)
          - When stuck: Use Problem Inversion (Skill 7)
          - Annual: Full workflow review, consider new projects

          ## Maintenance and Updates

          ### Living Documents Protocol

          **Monthly:**
          - Update Decision Tree (Skill 5)
          - Log adversities and responses (Skill 6)
          - Note new papers or competition
          - Adjust timeline if needed

          **Quarterly:**
          - Review Risk Matrix (Skill 2) - mark assumptions tested
          - Reassess Impact (Skill 3) - has evaluation changed?
          - Check Parameter Strategy (Skill 4) - still optimal?
          - Update Integrated Plan (Skill 8) - keep current

          **Annually:**
          - Complete workflow review
          - Consider new projects with fresh Skill 1 ideation
          - Archive old project docs
          - Extract lessons learned

          ## Success Metrics

          ### How Do You Know This Worked?

          **Immediate Indicators:**
          - Clearer project vision than before
          - Honest assessment of risks
          - Contingency plans for failures
          - Compelling communication materials
          - Alignment between project and values
          - Confidence in problem choice

          **6-Month Indicators:**
          - Major decisions made faster (have framework)
          - Adversity handled productively (used playbook)
          - No existential crises (risks were mitigated)
          - Regular Level 2 evaluation happening
          - Project staying on-track or pivoting smartly

          **2-Year Indicators:**
          - Published results or strong progress
          - Avoided dead-end projects
          - Multiple high-quality options at decision points
          - Skills developed as planned
          - Career trajectory aligned with goals
          - Time well-spent (the ultimate measure)

          ## Key Principles of the Meta-Framework

          1. **Systematic > Ad Hoc:** Process ensures nothing forgotten
          2. **Iterative > Linear:** Expect to loop back, that's good
          3. **Documented > Mental:** Writing forces clarity
          4. **Integrated > Fragmented:** All skills connect
          5. **Living > Static:** Update as you learn
          6. **Thoughtful > Fast:** Time invested now saves years later
          7. **Honest > Optimistic:** Rigor protects against wishful thinking
          8. **Prepared > Surprised:** Anticipate adversity
          9. **Flexible > Rigid:** Parameters float when needed
          10. **Passionate > Obligatory:** Alignment matters

          ## Getting Started

          ### First Steps

          **This Week:**
          1. Block time in calendar (1-2 hours to start)
          2. Gather your context (background, goals, constraints)
          3. Begin Skill 1 (Intuition Pumps)
          4. Let me know your starting point

          **This Month:**
          1. Work through Skills 1-4 (foundation)
          2. Share with mentor for alignment check
          3. Iterate if major changes needed
          4. Complete Skills 5-8 (execution planning)

          **This Quarter:**
          1. Begin project execution with living documents
          2. Monthly decision tree updates
          3. Quarterly risk assessment reviews
          4. Log adversities and responses

          **This Year:**
          1. Execute planned project
          2. Use frameworks when stuck
          3. Update living documents
          4. Evaluate process and refine

          ## Ready to Begin?

          The complete meta-framework is substantial, but each step builds on the last. You'll move through:
          - ~2 weeks of intensive planning
          - Comprehensive documentation
          - Clear decision criteria
          - Communication materials
          - Living documents for ongoing guidance

          **Most importantly:** You'll KNOW you're working on a well-chosen problem with rigorous planning. That confidence is priceless.

          Let's start with Skill 1. Are you ready to begin?

          ---

          *Remember: The highest-leverage work in science is choosing the right problem. This meta-framework ensures you spend your finite time wisely. The investment in systematic planning pays dividends for years.*

    - path: /knowledge-work-plugins/skills/scvi-tools/LICENSE.txt
      text: |
          Apache License
          Version 2.0, January 2004
          http://www.apache.org/licenses/

          TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

          1. Definitions.

          "License" shall mean the terms and conditions for use, reproduction,
          and distribution as defined by Sections 1 through 9 of this document.

          "Licensor" shall mean the copyright owner or entity authorized by
          the copyright owner that is granting the License.

          "Legal Entity" shall mean the union of the acting entity and all
          other entities that control, are controlled by, or are under common
          control with that entity. For the purposes of this definition,
          "control" means (i) the power, direct or indirect, to cause the
          direction or management of such entity, whether by contract or
          otherwise, or (ii) ownership of fifty percent (50%) or more of the
          outstanding shares, or (iii) beneficial ownership of such entity.

          "You" (or "Your") shall mean an individual or Legal Entity
          exercising permissions granted by this License.

          "Source" form shall mean the preferred form for making modifications,
          including but not limited to software source code, documentation
          source, and configuration files.

          "Object" form shall mean any form resulting from mechanical
          transformation or translation of a Source form, including but
          not limited to compiled object code, generated documentation,
          and conversions to other media types.

          "Work" shall mean the work of authorship, whether in Source or
          Object form, made available under the License, as indicated by a
          copyright notice that is included in or attached to the work
          (an example is provided in the Appendix below).

          "Derivative Works" shall mean any work, whether in Source or Object
          form, that is based on (or derived from) the Work and for which the
          editorial revisions, annotations, elaborations, or other modifications
          represent, as a whole, an original work of authorship. For the purposes
          of this License, Derivative Works shall not include works that remain
          separable from, or merely link (or bind by name) to the interfaces of,
          the Work and Derivative Works thereof.

          "Contribution" shall mean any work of authorship, including
          the original version of the Work and any modifications or additions
          to that Work or Derivative Works thereof, that is intentionally
          submitted to Licensor for inclusion in the Work by the copyright owner
          or by an individual or Legal Entity authorized to submit on behalf of
          the copyright owner. For the purposes of this definition, "submitted"
          means any form of electronic, verbal, or written communication sent
          to the Licensor or its representatives, including but not limited to
          communication on electronic mailing lists, source code control systems,
          and issue tracking systems that are managed by, or on behalf of, the
          Licensor for the purpose of discussing and improving the Work, but
          excluding communication that is conspicuously marked or otherwise
          designated in writing by the copyright owner as "Not a Contribution."

          "Contributor" shall mean Licensor and any individual or Legal Entity
          on behalf of whom a Contribution has been received by Licensor and
          subsequently incorporated within the Work.

          2. Grant of Copyright License. Subject to the terms and conditions of
          this License, each Contributor hereby grants to You a perpetual,
          worldwide, non-exclusive, no-charge, royalty-free, irrevocable
          copyright license to reproduce, prepare Derivative Works of,
          publicly display, publicly perform, sublicense, and distribute the
          Work and such Derivative Works in Source or Object form.

          3. Grant of Patent License. Subject to the terms and conditions of
          this License, each Contributor hereby grants to You a perpetual,
          worldwide, non-exclusive, no-charge, royalty-free, irrevocable
          (except as stated in this section) patent license to make, have made,
          use, offer to sell, sell, import, and otherwise transfer the Work,
          where such license applies only to those patent claims licensable
          by such Contributor that are necessarily infringed by their
          Contribution(s) alone or by combination of their Contribution(s)
          with the Work to which such Contribution(s) was submitted. If You
          institute patent litigation against any entity (including a
          cross-claim or counterclaim in a lawsuit) alleging that the Work
          or a Contribution incorporated within the Work constitutes direct
          or contributory patent infringement, then any patent licenses
          granted to You under this License for that Work shall terminate
          as of the date such litigation is filed.

          4. Redistribution. You may reproduce and distribute copies of the
          Work or Derivative Works thereof in any medium, with or without
          modifications, and in Source or Object form, provided that You
          meet the following conditions:

          (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

          (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

          (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

          (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

          You may add Your own copyright statement to Your modifications and
          may provide additional or different license terms and conditions
          for use, reproduction, or distribution of Your modifications, or
          for any such Derivative Works as a whole, provided Your use,
          reproduction, and distribution of the Work otherwise complies with
          the conditions stated in this License.

          5. Submission of Contributions. Unless You explicitly state otherwise,
          any Contribution intentionally submitted for inclusion in the Work
          by You to the Licensor shall be under the terms and conditions of
          this License, without any additional terms or conditions.
          Notwithstanding the above, nothing herein shall supersede or modify
          the terms of any separate license agreement you may have executed
          with Licensor regarding such Contributions.

          6. Trademarks. This License does not grant permission to use the trade
          names, trademarks, service marks, or product names of the Licensor,
          except as required for reasonable and customary use in describing the
          origin of the Work and reproducing the content of the NOTICE file.

          7. Disclaimer of Warranty. Unless required by applicable law or
          agreed to in writing, Licensor provides the Work (and each
          Contributor provides its Contributions) on an "AS IS" BASIS,
          WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
          implied, including, without limitation, any warranties or conditions
          of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
          PARTICULAR PURPOSE. You are solely responsible for determining the
          appropriateness of using or redistributing the Work and assume any
          risks associated with Your exercise of permissions under this License.

          8. Limitation of Liability. In no event and under no legal theory,
          whether in tort (including negligence), contract, or otherwise,
          unless required by applicable law (such as deliberate and grossly
          negligent acts) or agreed to in writing, shall any Contributor be
          liable to You for damages, including any direct, indirect, special,
          incidental, or consequential damages of any character arising as a
          result of this License or out of the use or inability to use the
          Work (including but not limited to damages for loss of goodwill,
          work stoppage, computer failure or malfunction, or any and all
          other commercial damages or losses), even if such Contributor
          has been advised of the possibility of such damages.

          9. Accepting Warranty or Additional Liability. While redistributing
          the Work or Derivative Works thereof, You may choose to offer,
          and charge a fee for, acceptance of support, warranty, indemnity,
          or other liability obligations and/or rights consistent with this
          License. However, in accepting such obligations, You may act only
          on Your own behalf and on Your sole responsibility, not on behalf
          of any other Contributor, and only if You agree to indemnify,
          defend, and hold each Contributor harmless for any liability
          incurred by, or claims asserted against, such Contributor by reason
          of your accepting any such warranty or additional liability.

          END OF TERMS AND CONDITIONS

          APPENDIX: How to apply the Apache License to your work.

          To apply the Apache License to your work, attach the following
          boilerplate notice, with the fields enclosed by brackets "[]"
          replaced with your own identifying information. (Don't include
          the brackets!) The text should be enclosed in the appropriate
          comment syntax for the file format. We also recommend that a
          file or class name and description of purpose be included on the
          same "printed page" as the copyright notice for easier
          identification within third-party archives.

          Copyright [yyyy] [name of copyright owner]

          Licensed under the Apache License, Version 2.0 (the "License");
          you may not use this file except in compliance with the License.
          You may obtain a copy of the License at

          http://www.apache.org/licenses/LICENSE-2.0

          Unless required by applicable law or agreed to in writing, software
          distributed under the License is distributed on an "AS IS" BASIS,
          WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
          See the License for the specific language governing permissions and
          limitations under the License.
    - path: /knowledge-work-plugins/skills/scvi-tools/SKILL.md
      text: |
          ---
          name: scvi-tools
          description: Deep learning for single-cell analysis using scvi-tools. This skill should be used when users need (1) data integration and batch correction with scVI/scANVI, (2) ATAC-seq analysis with PeakVI, (3) CITE-seq multi-modal analysis with totalVI, (4) multiome RNA+ATAC analysis with MultiVI, (5) spatial transcriptomics deconvolution with DestVI, (6) label transfer and reference mapping with scANVI/scArches, (7) RNA velocity with veloVI, or (8) any deep learning-based single-cell method. Triggers include mentions of scVI, scANVI, totalVI, PeakVI, MultiVI, DestVI, veloVI, sysVI, scArches, variational autoencoder, VAE, batch correction, data integration, multi-modal, CITE-seq, multiome, reference mapping, latent space.
          ---

          # scvi-tools Deep Learning Skill

          This skill provides guidance for deep learning-based single-cell analysis using scvi-tools, the leading framework for probabilistic models in single-cell genomics.

          ## How to Use This Skill

          1. Identify the appropriate workflow from the model/workflow tables below
          2. Read the corresponding reference file for detailed steps and code
          3. Use scripts in `scripts/` to avoid rewriting common code
          4. For installation or GPU issues, consult `references/environment_setup.md`
          5. For debugging, consult `references/troubleshooting.md`

          ## When to Use This Skill

          - When scvi-tools, scVI, scANVI, or related models are mentioned
          - When deep learning-based batch correction or integration is needed
          - When working with multi-modal data (CITE-seq, multiome)
          - When reference mapping or label transfer is required
          - When analyzing ATAC-seq or spatial transcriptomics data
          - When learning latent representations of single-cell data

          ## Model Selection Guide

          | Data Type | Model | Primary Use Case |
          |-----------|-------|------------------|
          | scRNA-seq | **scVI** | Unsupervised integration, DE, imputation |
          | scRNA-seq + labels | **scANVI** | Label transfer, semi-supervised integration |
          | CITE-seq (RNA+protein) | **totalVI** | Multi-modal integration, protein denoising |
          | scATAC-seq | **PeakVI** | Chromatin accessibility analysis |
          | Multiome (RNA+ATAC) | **MultiVI** | Joint modality analysis |
          | Spatial + scRNA reference | **DestVI** | Cell type deconvolution |
          | RNA velocity | **veloVI** | Transcriptional dynamics |
          | Cross-technology | **sysVI** | System-level batch correction |

          ## Workflow Reference Files

          | Workflow | Reference File | Description |
          |----------|---------------|-------------|
          | Environment Setup | `references/environment_setup.md` | Installation, GPU, version info |
          | Data Preparation | `references/data_preparation.md` | Formatting data for any model |
          | scRNA Integration | `references/scrna_integration.md` | scVI/scANVI batch correction |
          | ATAC-seq Analysis | `references/atac_peakvi.md` | PeakVI for accessibility |
          | CITE-seq Analysis | `references/citeseq_totalvi.md` | totalVI for protein+RNA |
          | Multiome Analysis | `references/multiome_multivi.md` | MultiVI for RNA+ATAC |
          | Spatial Deconvolution | `references/spatial_deconvolution.md` | DestVI spatial analysis |
          | Label Transfer | `references/label_transfer.md` | scANVI reference mapping |
          | scArches Mapping | `references/scarches_mapping.md` | Query-to-reference mapping |
          | Batch Correction | `references/batch_correction_sysvi.md` | Advanced batch methods |
          | RNA Velocity | `references/rna_velocity_velovi.md` | veloVI dynamics |
          | Troubleshooting | `references/troubleshooting.md` | Common issues and solutions |

          ## CLI Scripts

          Modular scripts for common workflows. Chain together or modify as needed.

          ### Pipeline Scripts

          | Script | Purpose | Usage |
          |--------|---------|-------|
          | `prepare_data.py` | QC, filter, HVG selection | `python scripts/prepare_data.py raw.h5ad prepared.h5ad --batch-key batch` |
          | `train_model.py` | Train any scvi-tools model | `python scripts/train_model.py prepared.h5ad results/ --model scvi` |
          | `cluster_embed.py` | Neighbors, UMAP, Leiden | `python scripts/cluster_embed.py adata.h5ad results/` |
          | `differential_expression.py` | DE analysis | `python scripts/differential_expression.py model/ adata.h5ad de.csv --groupby leiden` |
          | `transfer_labels.py` | Label transfer with scANVI | `python scripts/transfer_labels.py ref_model/ query.h5ad results/` |
          | `integrate_datasets.py` | Multi-dataset integration | `python scripts/integrate_datasets.py results/ data1.h5ad data2.h5ad` |
          | `validate_adata.py` | Check data compatibility | `python scripts/validate_adata.py data.h5ad --batch-key batch` |

          ### Example Workflow

          ```bash
          # 1. Validate input data
          python scripts/validate_adata.py raw.h5ad --batch-key batch --suggest

          # 2. Prepare data (QC, HVG selection)
          python scripts/prepare_data.py raw.h5ad prepared.h5ad --batch-key batch --n-hvgs 2000

          # 3. Train model
          python scripts/train_model.py prepared.h5ad results/ --model scvi --batch-key batch

          # 4. Cluster and visualize
          python scripts/cluster_embed.py results/adata_trained.h5ad results/ --resolution 0.8

          # 5. Differential expression
          python scripts/differential_expression.py results/model results/adata_clustered.h5ad results/de.csv --groupby leiden
          ```

          ### Python Utilities

          The `scripts/model_utils.py` provides importable functions for custom workflows:

          | Function | Purpose |
          |----------|---------|
          | `prepare_adata()` | Data preparation (QC, HVG, layer setup) |
          | `train_scvi()` | Train scVI or scANVI |
          | `evaluate_integration()` | Compute integration metrics |
          | `get_marker_genes()` | Extract DE markers |
          | `save_results()` | Save model, data, plots |
          | `auto_select_model()` | Suggest best model |
          | `quick_clustering()` | Neighbors + UMAP + Leiden |

          ## Critical Requirements

          1. **Raw counts required**: scvi-tools models require integer count data
             ```python
             adata.layers["counts"] = adata.X.copy()  # Before normalization
             scvi.model.SCVI.setup_anndata(adata, layer="counts")
             ```

          2. **HVG selection**: Use 2000-4000 highly variable genes
             ```python
             sc.pp.highly_variable_genes(adata, n_top_genes=2000, batch_key="batch", layer="counts", flavor="seurat_v3")
             adata = adata[:, adata.var['highly_variable']].copy()
             ```

          3. **Batch information**: Specify batch_key for integration
             ```python
             scvi.model.SCVI.setup_anndata(adata, layer="counts", batch_key="batch")
             ```

          ## Quick Decision Tree

          ```
          Need to integrate scRNA-seq data?
          ├── Have cell type labels? → scANVI (references/label_transfer.md)
          └── No labels? → scVI (references/scrna_integration.md)

          Have multi-modal data?
          ├── CITE-seq (RNA + protein)? → totalVI (references/citeseq_totalvi.md)
          ├── Multiome (RNA + ATAC)? → MultiVI (references/multiome_multivi.md)
          └── scATAC-seq only? → PeakVI (references/atac_peakvi.md)

          Have spatial data?
          └── Need cell type deconvolution? → DestVI (references/spatial_deconvolution.md)

          Have pre-trained reference model?
          └── Map query to reference? → scArches (references/scarches_mapping.md)

          Need RNA velocity?
          └── veloVI (references/rna_velocity_velovi.md)

          Strong cross-technology batch effects?
          └── sysVI (references/batch_correction_sysvi.md)
          ```

          ## Key Resources

          - [scvi-tools Documentation](https://docs.scvi-tools.org/)
          - [scvi-tools Tutorials](https://docs.scvi-tools.org/en/stable/tutorials/index.html)
          - [Model Hub](https://huggingface.co/scvi-tools)
          - [GitHub Issues](https://github.com/scverse/scvi-tools/issues)
    - path: /knowledge-work-plugins/skills/scvi-tools/references/atac_peakvi.md
      text: |
          # scATAC-seq Analysis with PeakVI

          This reference covers single-cell ATAC-seq analysis using PeakVI for dimensionality reduction, batch correction, and differential accessibility.

          ## Overview

          PeakVI is a deep generative model for scATAC-seq data that:
          - Models binary accessibility (peak open/closed)
          - Handles batch effects
          - Provides latent representation for clustering
          - Enables differential accessibility analysis

          ## Prerequisites

          ```python
          import scvi
          import scanpy as sc
          import numpy as np
          import anndata as ad

          print(f"scvi-tools version: {scvi.__version__}")
          ```

          ## Step 1: Load and Prepare ATAC Data

          ### From 10x Genomics (Cell Ranger ATAC)

          ```python
          # Peak-cell matrix from fragments
          # Usually in filtered_peak_bc_matrix format

          adata = sc.read_10x_h5("filtered_peak_bc_matrix.h5")

          # Or from mtx format
          adata = sc.read_10x_mtx("filtered_peak_bc_matrix/")

          # Check structure
          print(f"Cells: {adata.n_obs}, Peaks: {adata.n_vars}")
          print(f"Sparsity: {1 - adata.X.nnz / (adata.n_obs * adata.n_vars):.2%}")
          ```

          ### From ArchR/Signac

          ```python
          # Export from ArchR (in R)
          # saveArchRProject(proj, outputDirectory="atac_export", load=FALSE)
          # Then read the exported files in Python

          # From Signac:
          # Export peak matrix and metadata
          ```

          ## Step 2: Quality Control

          ```python
          # Calculate QC metrics
          sc.pp.calculate_qc_metrics(adata, inplace=True)

          # Key metrics for ATAC:
          # - n_genes_by_counts: peaks per cell (should rename)
          # - total_counts: fragments per cell
          adata.obs['n_peaks'] = adata.obs['n_genes_by_counts']
          adata.obs['total_fragments'] = adata.obs['total_counts']

          # Filter cells
          adata = adata[adata.obs['n_peaks'] > 500].copy()
          adata = adata[adata.obs['n_peaks'] < 50000].copy()  # Remove potential doublets

          # Filter peaks (accessible in at least n cells)
          sc.pp.filter_genes(adata, min_cells=10)

          print(f"After QC: {adata.shape}")
          ```

          ### Binarize Data

          ```python
          # PeakVI works with binary accessibility
          # Binarize if not already binary
          adata.X = (adata.X > 0).astype(np.float32)

          # Verify
          print(f"Unique values: {np.unique(adata.X.data)}")
          ```

          ## Step 3: Feature Selection

          Unlike RNA-seq, peak selection for ATAC is less established. Options:

          ### Option A: Most Accessible Peaks

          ```python
          # Select top peaks by accessibility frequency
          peak_accessibility = np.array(adata.X.sum(axis=0)).flatten()
          top_peaks = np.argsort(peak_accessibility)[-50000:]  # Top 50k peaks

          adata = adata[:, top_peaks].copy()
          ```

          ### Option B: Variable Peaks

          ```python
          # Select peaks with high variance
          # (Most informative for clustering)
          from sklearn.feature_selection import VarianceThreshold

          selector = VarianceThreshold(threshold=0.05)
          selector.fit(adata.X)
          adata = adata[:, selector.get_support()].copy()
          ```

          ### Option C: Peaks Near Genes

          ```python
          # Keep peaks within promoter regions or gene bodies
          # Requires peak annotation
          # gene_peaks = peaks with gene annotation
          # adata = adata[:, adata.var['near_gene']].copy()
          ```

          ## Step 4: Add Batch Information

          ```python
          # Add batch annotation if multiple samples
          adata.obs['batch'] = adata.obs['sample_id']  # Or appropriate column

          print(adata.obs['batch'].value_counts())
          ```

          ## Step 5: Setup and Train PeakVI

          ```python
          # Setup AnnData
          scvi.model.PEAKVI.setup_anndata(
              adata,
              batch_key="batch"  # Optional, omit for single batch
          )

          # Create model
          model = scvi.model.PEAKVI(
              adata,
              n_latent=20,      # Latent dimensions
              n_layers_encoder=2,
              n_layers_decoder=2
          )

          # Train
          model.train(
              max_epochs=200,
              early_stopping=True,
              batch_size=128
          )

          # Check training
          model.history['elbo_train'].plot()
          ```

          ## Step 6: Get Latent Representation

          ```python
          # Latent space for downstream analysis
          adata.obsm["X_PeakVI"] = model.get_latent_representation()

          # Clustering and visualization
          sc.pp.neighbors(adata, use_rep="X_PeakVI", n_neighbors=15)
          sc.tl.umap(adata)
          sc.tl.leiden(adata, resolution=0.5)

          # Visualize
          sc.pl.umap(adata, color=['leiden', 'batch'], ncols=2)
          ```

          ## Step 7: Differential Accessibility

          ```python
          # Differential accessibility between clusters
          da_results = model.differential_accessibility(
              groupby='leiden',
              group1='0',
              group2='1'
          )

          # Filter significant peaks
          da_sig = da_results[
              (da_results['is_da_fdr_0.05']) &
              (abs(da_results['lfc_mean']) > 1)
          ]

          print(f"Significant DA peaks: {len(da_sig)}")
          print(da_sig.head())
          ```

          ### DA Between Conditions

          ```python
          # Compare conditions within cell type
          adata_subset = adata[adata.obs['cell_type'] == 'CD4 T cells'].copy()

          da_condition = model.differential_accessibility(
              groupby='condition',
              group1='treated',
              group2='control'
          )
          ```

          ## Step 8: Peak Annotation

          ```python
          # Annotate peaks with nearest genes
          # Using pybedtools or similar

          # Example peak name format: chr1:1000-2000
          # Parse into bed format for annotation

          import pandas as pd

          def parse_peak_names(peak_names):
              """Parse peak names into bed format."""
              records = []
              for peak in peak_names:
                  chrom, coords = peak.split(':')
                  start, end = coords.split('-')
                  records.append({
                      'chrom': chrom,
                      'start': int(start),
                      'end': int(end),
                      'peak': peak
                  })
              return pd.DataFrame(records)

          peak_bed = parse_peak_names(adata.var_names)
          ```

          ## Step 9: Motif Analysis

          ```python
          # Export significant peaks for motif analysis
          # Use HOMER, MEME, or chromVAR

          # Export peak sequences
          sig_peaks = da_sig.index.tolist()
          peak_bed_sig = peak_bed[peak_bed['peak'].isin(sig_peaks)]
          peak_bed_sig.to_csv("significant_peaks.bed", sep='\t', index=False, header=False)

          # Then run HOMER:
          # findMotifsGenome.pl significant_peaks.bed hg38 motif_output/ -size 200
          ```

          ## Step 10: Gene Activity Scores

          ```python
          # Compute gene activity from peak accessibility
          # (Requires peak-gene annotations)

          def compute_gene_activity(adata, peak_gene_map):
              """
              Compute gene activity scores from peak accessibility.
              
              Parameters
              ----------
              adata : AnnData
                  ATAC data with peaks
              peak_gene_map : dict
                  Mapping of peaks to genes
                  
              Returns
              -------
              AnnData with gene activity scores
              """
              from scipy.sparse import csr_matrix
              
              genes = list(set(peak_gene_map.values()))
              gene_matrix = np.zeros((adata.n_obs, len(genes)))
              
              for i, gene in enumerate(genes):
                  gene_peaks = [p for p, g in peak_gene_map.items() if g == gene]
                  if gene_peaks:
                      peak_idx = [list(adata.var_names).index(p) for p in gene_peaks if p in adata.var_names]
                      if peak_idx:
                          gene_matrix[:, i] = np.array(adata.X[:, peak_idx].sum(axis=1)).flatten()
              
              adata_gene = ad.AnnData(
                  X=csr_matrix(gene_matrix),
                  obs=adata.obs.copy(),
                  var=pd.DataFrame(index=genes)
              )
              
              return adata_gene
          ```

          ## Complete Pipeline

          ```python
          def analyze_scatac(
              adata,
              batch_key=None,
              n_top_peaks=50000,
              n_latent=20,
              resolution=0.5
          ):
              """
              Complete scATAC-seq analysis with PeakVI.
              
              Parameters
              ----------
              adata : AnnData
                  Raw peak-cell matrix
              batch_key : str, optional
                  Batch annotation column
              n_top_peaks : int
                  Number of top peaks to use
              n_latent : int
                  Latent dimensions
              resolution : float
                  Leiden clustering resolution
                  
              Returns
              -------
              Tuple of (processed AnnData, trained model)
              """
              import scvi
              import scanpy as sc
              import numpy as np
              
              adata = adata.copy()
              
              # QC
              sc.pp.calculate_qc_metrics(adata, inplace=True)
              adata = adata[adata.obs['n_genes_by_counts'] > 500].copy()
              sc.pp.filter_genes(adata, min_cells=10)
              
              # Binarize
              adata.X = (adata.X > 0).astype(np.float32)
              
              # Select top peaks
              if adata.n_vars > n_top_peaks:
                  peak_accessibility = np.array(adata.X.sum(axis=0)).flatten()
                  top_peaks = np.argsort(peak_accessibility)[-n_top_peaks:]
                  adata = adata[:, top_peaks].copy()
              
              # Setup PeakVI
              scvi.model.PEAKVI.setup_anndata(adata, batch_key=batch_key)
              
              # Train
              model = scvi.model.PEAKVI(adata, n_latent=n_latent)
              model.train(max_epochs=200, early_stopping=True)
              
              # Latent representation
              adata.obsm["X_PeakVI"] = model.get_latent_representation()
              
              # Clustering
              sc.pp.neighbors(adata, use_rep="X_PeakVI")
              sc.tl.umap(adata)
              sc.tl.leiden(adata, resolution=resolution)
              
              return adata, model

          # Usage
          adata, model = analyze_scatac(
              adata,
              batch_key="sample",
              n_top_peaks=50000
          )

          # Visualize
          sc.pl.umap(adata, color=['leiden', 'sample'])

          # Differential accessibility
          da_results = model.differential_accessibility(
              groupby='leiden',
              group1='0',
              group2='1'
          )
          ```

          ## Integration with scRNA-seq

          For multiome data or separate RNA/ATAC from same cells:

          ```python
          # See MultiVI for joint RNA+ATAC analysis
          # Or use WNN (weighted nearest neighbors) approach

          # Transfer labels from RNA to ATAC using shared latent space
          ```

          ## Troubleshooting

          | Issue | Cause | Solution |
          |-------|-------|----------|
          | Training slow | Too many peaks | Subset to top 50k peaks |
          | Poor clustering | Too few informative peaks | Use variable peaks |
          | Batch dominates | Strong technical effects | Ensure batch_key is set |
          | Memory error | Large peak matrix | Use sparse format, reduce peaks |

          ## Key References

          - Ashuach et al. (2022) "PeakVI: A deep generative model for single-cell chromatin accessibility analysis"
    - path: /knowledge-work-plugins/skills/scvi-tools/references/batch_correction_sysvi.md
      text: |
          # Advanced Batch Correction with sysVI

          This reference covers system-level batch correction using sysVI, designed for integrating data across major technological or study differences.

          ## Overview

          sysVI (System Variational Inference) extends scVI for scenarios where:
          - Batch effects are very strong (different technologies)
          - Standard scVI over-corrects biological signal
          - You need to separate "system" effects from biological variation

          ## When to Use sysVI vs scVI

          | Scenario | Recommended Model |
          |----------|-------------------|
          | Same technology, different samples | scVI |
          | 10x v2 vs 10x v3 | scVI (usually) |
          | 10x vs Smart-seq2 | sysVI |
          | Different sequencing depths | scVI with covariates |
          | Cross-study integration | sysVI |
          | Atlas-scale integration | sysVI |

          ## Prerequisites

          ```python
          import scvi
          import scanpy as sc
          import numpy as np

          print(f"scvi-tools version: {scvi.__version__}")
          ```

          ## Understanding sysVI Architecture

          sysVI separates variation into:
          1. **Biological variation**: Cell type, state, trajectory
          2. **System variation**: Technology, study, lab effects

          ```
                              ┌─────────────────┐
          Input counts ──────►│    Encoder      │
                              │                 │
          System info ───────►│  (conditioned)  │
                              └────────┬────────┘
                                       │
                              ┌────────▼────────┐
                              │   Latent z      │
                              │  (biological)   │
                              └────────┬────────┘
                                       │
                              ┌────────▼────────┐
          System info ───────►│    Decoder      │
                              │  (conditioned)  │
                              └────────┬────────┘
                                       │
                              Reconstructed counts
          ```

          ## Basic sysVI Workflow

          ### Step 1: Prepare Data

          ```python
          # Load datasets from different systems
          adata1 = sc.read_h5ad("10x_data.h5ad")
          adata2 = sc.read_h5ad("smartseq_data.h5ad")

          # Add system labels
          adata1.obs["system"] = "10x"
          adata2.obs["system"] = "Smart-seq2"

          # Add batch labels (within system)
          # e.g., different samples within each technology

          # Concatenate
          adata = sc.concat([adata1, adata2])

          # Store raw counts
          adata.layers["counts"] = adata.X.copy()
          ```

          ### Step 2: HVG Selection

          ```python
          # Select HVGs considering both batch and system
          sc.pp.highly_variable_genes(
              adata,
              n_top_genes=4000,  # More genes for cross-system
              flavor="seurat_v3",
              batch_key="system",  # Consider system for HVG
              layer="counts"
          )

          # Optionally: ensure overlap between systems
          # Check HVGs are expressed in both systems
          adata = adata[:, adata.var["highly_variable"]].copy()
          ```

          ### Step 3: Setup and Train sysVI

          ```python
          # Setup AnnData
          # Note: sysVI may be accessed differently depending on version
          # Check scvi-tools documentation for current API

          scvi.model.SCVI.setup_anndata(
              adata,
              layer="counts",
              batch_key="sample",           # Within-system batches
              categorical_covariate_keys=["system"]  # System-level covariate
          )

          # For true sysVI (if available in your version)
          # scvi.model.SysVI.setup_anndata(...)

          # Create model with system awareness
          model = scvi.model.SCVI(
              adata,
              n_latent=30,
              n_layers=2,
              gene_likelihood="nb"
          )

          # Train
          model.train(max_epochs=300)
          ```

          ### Step 4: Extract Representations

          ```python
          # Get latent representation
          adata.obsm["X_integrated"] = model.get_latent_representation()

          # Clustering and visualization
          sc.pp.neighbors(adata, use_rep="X_integrated")
          sc.tl.umap(adata)
          sc.tl.leiden(adata)

          # Check integration
          sc.pl.umap(adata, color=["system", "leiden", "cell_type"])
          ```

          ## Alternative: Harmony + scVI

          For cross-system integration, combining methods can work well:

          ```python
          import scanpy.external as sce

          # First run PCA
          sc.pp.pca(adata)

          # Apply Harmony for initial alignment
          sce.pp.harmony_integrate(adata, key="system")

          # Then train scVI on Harmony-corrected embedding
          # Or use Harmony representation directly
          ```

          ## Alternative: Using Covariates in scVI

          For moderate system effects:

          ```python
          # Include system as categorical covariate
          scvi.model.SCVI.setup_anndata(
              adata,
              layer="counts",
              batch_key="sample",
              categorical_covariate_keys=["system", "technology_version"]
          )

          model = scvi.model.SCVI(adata, n_latent=30)
          model.train()
          ```

          ## Alternative: Separate Models + Integration

          For very different systems:

          ```python
          # Train separate models
          scvi.model.SCVI.setup_anndata(adata1, layer="counts", batch_key="sample")
          model1 = scvi.model.SCVI(adata1)
          model1.train()

          scvi.model.SCVI.setup_anndata(adata2, layer="counts", batch_key="sample")
          model2 = scvi.model.SCVI(adata2)
          model2.train()

          # Get latent spaces
          adata1.obsm["X_scVI"] = model1.get_latent_representation()
          adata2.obsm["X_scVI"] = model2.get_latent_representation()

          # Align with CCA or Harmony
          # ... additional alignment step
          ```

          ## Evaluating Cross-System Integration

          ### Visual Assessment

          ```python
          import matplotlib.pyplot as plt

          fig, axes = plt.subplots(1, 3, figsize=(15, 4))

          # Color by system
          sc.pl.umap(adata, color="system", ax=axes[0], show=False, title="By System")

          # Color by cell type
          sc.pl.umap(adata, color="cell_type", ax=axes[1], show=False, title="By Cell Type")

          # Color by expression of marker
          sc.pl.umap(adata, color="CD3D", ax=axes[2], show=False, title="CD3D Expression")

          plt.tight_layout()
          ```

          ### Quantitative Metrics

          ```python
          # Using scib-metrics
          from scib_metrics.benchmark import Benchmarker

          bm = Benchmarker(
              adata,
              batch_key="system",
              label_key="cell_type",
              embedding_obsm_keys=["X_integrated"]
          )

          bm.benchmark()

          # Key metrics:
          # - Batch mixing (ASW_batch, Graph connectivity)
          # - Bio conservation (NMI, ARI, ASW_label)
          ```

          ### LISI Scores

          ```python
          # Local Inverse Simpson's Index
          from scib_metrics import lisi

          # Batch LISI (higher = better mixing)
          batch_lisi = lisi.ilisi_graph(
              adata,
              batch_key="system",
              use_rep="X_integrated"
          )

          # Cell type LISI (lower = better preservation)
          ct_lisi = lisi.clisi_graph(
              adata,
              label_key="cell_type", 
              use_rep="X_integrated"
          )

          print(f"Batch LISI: {batch_lisi.mean():.3f}")
          print(f"Cell type LISI: {ct_lisi.mean():.3f}")
          ```

          ## Handling Specific Challenges

          ### Different Gene Sets

          ```python
          # Find common genes
          common_genes = adata1.var_names.intersection(adata2.var_names)
          print(f"Common genes: {len(common_genes)}")

          # If too few, use gene mapping
          # Or impute missing genes
          ```

          ### Different Sequencing Depths

          ```python
          # Add depth as continuous covariate
          adata.obs["log_counts"] = np.log1p(adata.obs["total_counts"])

          scvi.model.SCVI.setup_anndata(
              adata,
              layer="counts",
              batch_key="sample",
              continuous_covariate_keys=["log_counts"]
          )
          ```

          ### Unbalanced Cell Types

          ```python
          # Check cell type distribution per system
          import pandas as pd

          ct_dist = pd.crosstab(adata.obs["system"], adata.obs["cell_type"], normalize="index")
          print(ct_dist)

          # If very unbalanced, consider:
          # 1. Subsample to balance
          # 2. Use scANVI with labels to preserve rare types
          ```

          ## Complete Pipeline

          ```python
          def integrate_cross_system(
              adatas: dict,
              system_key: str = "system",
              batch_key: str = "batch",
              cell_type_key: str = "cell_type",
              n_top_genes: int = 4000,
              n_latent: int = 30
          ):
              """
              Integrate datasets from different technological systems.
              
              Parameters
              ----------
              adatas : dict
                  Dictionary of {system_name: AnnData}
              system_key : str
                  Key for system annotation
              batch_key : str
                  Key for within-system batch
              cell_type_key : str
                  Key for cell type labels (optional)
              n_top_genes : int
                  Number of HVGs
              n_latent : int
                  Latent dimensions
                  
              Returns
              -------
              Integrated AnnData with model
              """
              import scvi
              import scanpy as sc
              
              # Add system labels and concatenate
              for system_name, adata in adatas.items():
                  adata.obs[system_key] = system_name
              
              adata = sc.concat(list(adatas.values()))
              
              # Find common genes
              for name, ad in adatas.items():
                  if name == list(adatas.keys())[0]:
                      common_genes = set(ad.var_names)
                  else:
                      common_genes = common_genes.intersection(ad.var_names)
              
              adata = adata[:, list(common_genes)].copy()
              print(f"Common genes: {len(common_genes)}")
              
              # Store counts
              adata.layers["counts"] = adata.X.copy()
              
              # HVG selection
              sc.pp.highly_variable_genes(
                  adata,
                  n_top_genes=n_top_genes,
                  flavor="seurat_v3",
                  batch_key=system_key,
                  layer="counts"
              )
              adata = adata[:, adata.var["highly_variable"]].copy()
              
              # Setup with system as covariate
              scvi.model.SCVI.setup_anndata(
                  adata,
                  layer="counts",
                  batch_key=batch_key if batch_key in adata.obs else None,
                  categorical_covariate_keys=[system_key]
              )
              
              # Train
              model = scvi.model.SCVI(adata, n_latent=n_latent, n_layers=2)
              model.train(max_epochs=300, early_stopping=True)
              
              # Get representation
              adata.obsm["X_integrated"] = model.get_latent_representation()
              
              # Clustering
              sc.pp.neighbors(adata, use_rep="X_integrated")
              sc.tl.umap(adata)
              sc.tl.leiden(adata)
              
              return adata, model

          # Usage
          adatas = {
              "10x_v3": sc.read_h5ad("10x_v3_data.h5ad"),
              "Smart-seq2": sc.read_h5ad("smartseq_data.h5ad"),
              "Drop-seq": sc.read_h5ad("dropseq_data.h5ad")
          }

          adata_integrated, model = integrate_cross_system(adatas)

          # Visualize
          sc.pl.umap(adata_integrated, color=["system", "leiden"])
          ```

          ## Troubleshooting

          | Issue | Cause | Solution |
          |-------|-------|----------|
          | Systems don't mix | Effects too strong | Use more genes, increase n_latent |
          | Over-correction | Model too aggressive | Reduce n_layers, use scANVI |
          | Few common genes | Different platforms | Use gene name mapping |
          | One system dominates | Unbalanced sizes | Subsample larger dataset |

          ## Key References

          - Lopez et al. (2018) "Deep generative modeling for single-cell transcriptomics"
          - Luecken et al. (2022) "Benchmarking atlas-level data integration in single-cell genomics"
    - path: /knowledge-work-plugins/skills/scvi-tools/references/citeseq_totalvi.md
      text: |
          # CITE-seq Analysis with totalVI

          This reference covers multi-modal analysis of CITE-seq data (RNA + surface proteins) using totalVI.

          ## Overview

          CITE-seq combines:
          - scRNA-seq (transcriptome)
          - Protein surface markers (antibody-derived tags, ADT)

          totalVI jointly models both modalities to:
          - Integrate across batches
          - Denoise protein signal
          - Learn joint latent representation
          - Enable cross-modal imputation

          ## Prerequisites

          ```python
          import scvi
          import scanpy as sc
          import mudata as md
          import numpy as np
          import pandas as pd

          print(f"scvi-tools version: {scvi.__version__}")
          ```

          ## Step 1: Load CITE-seq Data

          ### From 10x Genomics (Cell Ranger)

          ```python
          # 10x outputs separate gene expression and feature barcoding
          adata_rna = sc.read_10x_h5("filtered_feature_bc_matrix.h5", gex_only=False)

          # Separate RNA and protein
          adata_protein = adata_rna[:, adata_rna.var['feature_types'] == 'Antibody Capture'].copy()
          adata_rna = adata_rna[:, adata_rna.var['feature_types'] == 'Gene Expression'].copy()

          print(f"RNA: {adata_rna.shape}")
          print(f"Protein: {adata_protein.shape}")
          ```

          ### From MuData

          ```python
          # If data is in MuData format
          mdata = md.read_h5mu("cite_seq.h5mu")

          adata_rna = mdata['rna'].copy()
          adata_protein = mdata['protein'].copy()
          ```

          ### Combine into Single AnnData

          ```python
          # totalVI expects protein data in obsm
          adata = adata_rna.copy()

          # Add protein expression to obsm
          adata.obsm["protein_expression"] = adata_protein.X.toarray() if hasattr(adata_protein.X, 'toarray') else adata_protein.X

          # Store protein names
          adata.uns["protein_names"] = list(adata_protein.var_names)
          ```

          ## Step 2: Quality Control

          ### RNA QC

          ```python
          # Standard RNA QC
          # Handle both human (MT-) and mouse (mt-, Mt-) mitochondrial genes
              adata.var['mt'] = (
                  adata.var_names.str.startswith('MT-') |
                  adata.var_names.str.startswith('mt-') |
                  adata.var_names.str.startswith('Mt-')
              )
          sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], inplace=True)

          # Filter cells
          adata = adata[adata.obs['n_genes_by_counts'] > 200].copy()
          adata = adata[adata.obs['pct_counts_mt'] < 20].copy()

          # Filter genes
          sc.pp.filter_genes(adata, min_cells=3)
          ```

          ### Protein QC

          ```python
          # Protein QC
          protein_counts = adata.obsm["protein_expression"]
          print(f"Protein counts per cell: min={protein_counts.sum(1).min():.0f}, max={protein_counts.sum(1).max():.0f}")

          # Check for isotype controls
          # Isotype controls should have low counts
          protein_names = adata.uns["protein_names"]
          for i, name in enumerate(protein_names):
              if 'isotype' in name.lower() or 'control' in name.lower():
                  print(f"{name}: mean={protein_counts[:, i].mean():.1f}")
          ```

          ## Step 3: Data Preparation

          ### Store Raw Counts

          ```python
          # Store RNA counts
          adata.layers["counts"] = adata.X.copy()

          # Protein must be raw ADT counts (NOT CLR-normalized)
          # WARNING: If importing from Seurat, ensure you use raw counts, not CLR-normalized data
          # Seurat's NormalizeData(normalization.method = "CLR") transforms counts - use the original assay
          ```

          ### HVG Selection for RNA

          ```python
          # Select HVGs for RNA
          # Note: totalVI uses all proteins regardless of HVG

          sc.pp.highly_variable_genes(
              adata,
              n_top_genes=4000,  # Use more for CITE-seq
              flavor="seurat_v3",
              batch_key="batch" if "batch" in adata.obs else None,
              layer="counts"
          )

          # Subset to HVGs
          adata = adata[:, adata.var["highly_variable"]].copy()
          ```

          ## Step 4: Setup and Train totalVI

          ```python
          # Setup AnnData for totalVI
          scvi.model.TOTALVI.setup_anndata(
              adata,
              layer="counts",
              protein_expression_obsm_key="protein_expression",
              batch_key="batch"  # Optional
          )

          # Create model
          model = scvi.model.TOTALVI(
              adata,
              n_latent=20,
              latent_distribution="normal"  # or "ln" for log-normal
          )

          # Train
          model.train(
              max_epochs=200,
              early_stopping=True,
              batch_size=128
          )

          # Check training
          model.history['elbo_train'].plot()
          ```

          ## Step 5: Get Latent Representation

          ```python
          # Joint latent space
          adata.obsm["X_totalVI"] = model.get_latent_representation()

          # Clustering and visualization
          sc.pp.neighbors(adata, use_rep="X_totalVI")
          sc.tl.umap(adata)
          sc.tl.leiden(adata, resolution=1.0)

          sc.pl.umap(adata, color=['leiden', 'batch'])
          ```

          ## Step 6: Denoised Protein Expression

          ```python
          # Get denoised protein values
          # This removes background noise from protein measurements

          _, protein_denoised = model.get_normalized_expression(
              return_mean=True,
              transform_batch="batch1"  # Optional: normalize to specific batch
          )

          # Add to adata
          adata.obsm["protein_denoised"] = protein_denoised

          # Visualize denoised proteins
          protein_names = adata.uns["protein_names"]
          for i, protein in enumerate(protein_names[:5]):
              adata.obs[f"denoised_{protein}"] = protein_denoised[:, i]

          sc.pl.umap(adata, color=[f"denoised_{p}" for p in protein_names[:5]])
          ```

          ## Step 7: Normalized RNA Expression

          ```python
          # Get normalized RNA expression
          rna_normalized, _ = model.get_normalized_expression(
              return_mean=True
          )

          # Store
          adata.layers["totalVI_normalized"] = rna_normalized
          ```

          ## Step 8: Differential Expression

          ### RNA Differential Expression

          ```python
          # DE between clusters
          de_rna = model.differential_expression(
              groupby="leiden",
              group1="0",
              group2="1"
          )

          # Filter significant genes
          de_sig = de_rna[
              (de_rna['is_de_fdr_0.05']) &
              (abs(de_rna['lfc_mean']) > 1)
          ]

          print(f"Significant DE genes: {len(de_sig)}")
          ```

          ### Protein Differential Expression

          ```python
          # Protein DE
          de_protein = model.differential_expression(
              groupby="leiden",
              group1="0",
              group2="1",
              mode="protein"
          )

          print(de_protein.head(20))
          ```

          ## Step 9: Visualization

          ### Protein Expression on UMAP

          ```python
          # Denoised protein on UMAP
          import matplotlib.pyplot as plt

          proteins_to_plot = ["CD3", "CD4", "CD8", "CD19", "CD14"]

          fig, axes = plt.subplots(1, len(proteins_to_plot), figsize=(4*len(proteins_to_plot), 4))
          for ax, protein in zip(axes, proteins_to_plot):
              idx = adata.uns["protein_names"].index(protein)
              sc.pl.umap(
                  adata,
                  color=adata.obsm["protein_denoised"][:, idx],
                  ax=ax,
                  title=protein,
                  show=False
              )
          plt.tight_layout()
          ```

          ### Joint Heatmap

          ```python
          # Heatmap of top genes and proteins per cluster
          sc.pl.dotplot(
              adata,
              var_names=de_sig.index[:20].tolist(),
              groupby="leiden",
              layer="totalVI_normalized"
          )
          ```

          ## Step 10: Cell Type Annotation

          ```python
          # Use both RNA and protein markers for annotation

          # RNA markers
          rna_markers = {
              'T cells': ['CD3D', 'CD3E'],
              'CD4 T': ['CD4'],
              'CD8 T': ['CD8A', 'CD8B'],
              'B cells': ['CD19', 'MS4A1'],
              'Monocytes': ['CD14', 'LYZ']
          }

          # Check denoised protein expression
          for i, protein in enumerate(adata.uns["protein_names"]):
              if any(m in protein for m in ['CD3', 'CD4', 'CD8', 'CD19', 'CD14']):
                  print(f"{protein}: cluster means")
                  for cluster in adata.obs['leiden'].unique():
                      mask = adata.obs['leiden'] == cluster
                      mean_expr = adata.obsm["protein_denoised"][mask, i].mean()
                      print(f"  Cluster {cluster}: {mean_expr:.2f}")
          ```

          ## Complete Pipeline

          ```python
          def analyze_citeseq(
              adata_rna,
              adata_protein,
              batch_key=None,
              n_top_genes=4000,
              n_latent=20
          ):
              """
              Complete CITE-seq analysis with totalVI.
              
              Parameters
              ----------
              adata_rna : AnnData
                  RNA expression (raw counts)
              adata_protein : AnnData
                  Protein expression (raw counts)
              batch_key : str, optional
                  Batch column in obs
              n_top_genes : int
                  Number of HVGs
              n_latent : int
                  Latent dimensions
                  
              Returns
              -------
              Tuple of (processed AnnData, trained model)
              """
              import scvi
              import scanpy as sc
              
              # Ensure same cells
              common_cells = adata_rna.obs_names.intersection(adata_protein.obs_names)
              adata = adata_rna[common_cells].copy()
              adata_protein = adata_protein[common_cells].copy()
              
              # Add protein to obsm
              adata.obsm["protein_expression"] = adata_protein.X.toarray() if hasattr(adata_protein.X, 'toarray') else adata_protein.X
              adata.uns["protein_names"] = list(adata_protein.var_names)
              
              # RNA QC
              # Handle both human (MT-) and mouse (mt-, Mt-) mitochondrial genes
              adata.var['mt'] = (
                  adata.var_names.str.startswith('MT-') |
                  adata.var_names.str.startswith('mt-') |
                  adata.var_names.str.startswith('Mt-')
              )
              sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], inplace=True)
              adata = adata[adata.obs['pct_counts_mt'] < 20].copy()
              sc.pp.filter_genes(adata, min_cells=3)
              
              # Store counts
              adata.layers["counts"] = adata.X.copy()
              
              # HVG selection
              sc.pp.highly_variable_genes(
                  adata,
                  n_top_genes=n_top_genes,
                  flavor="seurat_v3",
                  batch_key=batch_key,
                  layer="counts"
              )
              adata = adata[:, adata.var["highly_variable"]].copy()
              
              # Setup totalVI
              scvi.model.TOTALVI.setup_anndata(
                  adata,
                  layer="counts",
                  protein_expression_obsm_key="protein_expression",
                  batch_key=batch_key
              )
              
              # Train
              model = scvi.model.TOTALVI(adata, n_latent=n_latent)
              model.train(max_epochs=200, early_stopping=True)
              
              # Get representations
              adata.obsm["X_totalVI"] = model.get_latent_representation()
              rna_norm, protein_denoised = model.get_normalized_expression(return_mean=True)
              adata.layers["totalVI_normalized"] = rna_norm
              adata.obsm["protein_denoised"] = protein_denoised
              
              # Clustering
              sc.pp.neighbors(adata, use_rep="X_totalVI")
              sc.tl.umap(adata)
              sc.tl.leiden(adata)
              
              return adata, model

          # Usage
          adata, model = analyze_citeseq(
              adata_rna,
              adata_protein,
              batch_key="batch"
          )

          # Visualize
          sc.pl.umap(adata, color=['leiden', 'batch'])
          ```

          ## Troubleshooting

          | Issue | Cause | Solution |
          |-------|-------|----------|
          | Protein signal noisy | Background not removed | Use get_normalized_expression with denoising |
          | Batch effects persist | Need batch_key | Ensure batch_key is specified |
          | Memory error | Too many genes | Reduce n_top_genes |
          | Poor protein clustering | Few proteins | Normal - totalVI uses RNA for structure |

          ## Key References

          - Gayoso et al. (2021) "Joint probabilistic modeling of single-cell multi-omic data with totalVI"
    - path: /knowledge-work-plugins/skills/scvi-tools/references/data_preparation.md
      text: |
          # Data Preparation for scvi-tools

          This reference covers how to properly prepare AnnData objects for use with scvi-tools models.

          ## Overview

          Proper data preparation is critical for scvi-tools. Key requirements:
          1. **Raw counts** (not normalized)
          2. **Highly variable gene selection**
          3. **Proper setup_anndata() call**

          ## Step 1: Load and Inspect Data

          ```python
          import scanpy as sc
          import scvi
          import numpy as np

          # Load data
          adata = sc.read_h5ad("data.h5ad")

          # Check what's in adata.X
          print(f"Shape: {adata.shape}")
          print(f"X dtype: {adata.X.dtype}")
          print(f"X contains integers: {np.allclose(adata.X.data, adata.X.data.astype(int))}")
          print(f"X min: {adata.X.min()}, max: {adata.X.max()}")
          ```

          ### Verify Raw Counts

          ```python
          # scvi-tools needs INTEGER counts
          # If X appears normalized, check for raw counts

          if hasattr(adata, 'raw') and adata.raw is not None:
              print("Found adata.raw")
              # Use raw counts
              adata = adata.raw.to_adata()
              
          # Or check layers
          if 'counts' in adata.layers:
              print("Found counts layer")
              # Will specify layer in setup_anndata
          ```

          ## Step 2: Basic Filtering

          ```python
          # Filter cells (standard QC)
          sc.pp.filter_cells(adata, min_genes=200)
          sc.pp.filter_cells(adata, max_genes=5000)

          # Calculate mito percent if not present
          # Handle both human (MT-) and mouse (mt-, Mt-) mitochondrial genes
          adata.var['mt'] = (
              adata.var_names.str.startswith('MT-') |
              adata.var_names.str.startswith('mt-') |
              adata.var_names.str.startswith('Mt-')
          )
          sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], inplace=True)
          adata = adata[adata.obs['pct_counts_mt'] < 20].copy()

          # Filter genes
          sc.pp.filter_genes(adata, min_cells=3)

          print(f"After filtering: {adata.shape}")
          ```

          ## Step 3: Store Raw Counts

          **Critical**: Always preserve raw counts before any normalization.

          ```python
          # Store raw counts in a layer
          adata.layers["counts"] = adata.X.copy()

          # Now you can normalize for other purposes (HVG selection)
          # But scvi will use the counts layer
          ```

          ## Step 4: Highly Variable Gene Selection

          scvi-tools works best with 1,500-5,000 HVGs.

          ### For Single-Batch Data

          ```python
          # Normalize for HVG selection only
          adata_hvg = adata.copy()
          sc.pp.normalize_total(adata_hvg, target_sum=1e4)
          sc.pp.log1p(adata_hvg)

          # Select HVGs
          sc.pp.highly_variable_genes(
              adata_hvg,
              n_top_genes=2000,
              flavor="seurat"  # or "cell_ranger"
          )

          # Transfer HVG annotation
          adata.var['highly_variable'] = adata_hvg.var['highly_variable']
          ```

          ### For Multi-Batch Data (Recommended)

          ```python
          # Use seurat_v3 flavor with batch_key
          # This selects genes variable across batches
          sc.pp.highly_variable_genes(
              adata,
              n_top_genes=2000,
              flavor="seurat_v3",
              batch_key="batch",  # Your batch column
              layer="counts"      # Use raw counts
          )
          ```

          ### Subset to HVGs

          ```python
          # Subset to highly variable genes
          adata = adata[:, adata.var['highly_variable']].copy()
          print(f"After HVG selection: {adata.shape}")
          ```

          ## Step 5: Setup AnnData

          The `setup_anndata()` function registers data for the model.

          ### Basic Setup

          ```python
          scvi.model.SCVI.setup_anndata(
              adata,
              layer="counts"  # Specify layer with raw counts
          )
          ```

          ### With Batch Information

          ```python
          scvi.model.SCVI.setup_anndata(
              adata,
              layer="counts",
              batch_key="batch"  # Column in adata.obs
          )
          ```

          ### With Cell Type Labels (for scANVI)

          ```python
          scvi.model.SCANVI.setup_anndata(
              adata,
              layer="counts",
              batch_key="batch",
              labels_key="cell_type"  # Column with cell type labels
          )
          ```

          ### With Continuous Covariates

          ```python
          scvi.model.SCVI.setup_anndata(
              adata,
              layer="counts",
              batch_key="batch",
              continuous_covariate_keys=["percent_mito", "n_genes"]
          )
          ```

          ### With Categorical Covariates

          ```python
          scvi.model.SCVI.setup_anndata(
              adata,
              layer="counts",
              batch_key="batch",
              categorical_covariate_keys=["donor", "technology"]
          )
          ```

          ## Multi-Modal Data Setup

          ### CITE-seq (for totalVI)

          ```python
          # Protein data in adata.obsm
          # RNA in adata.X, protein in separate matrix

          # Add protein data
          adata.obsm["protein_expression"] = protein_counts  # numpy array

          # Setup for totalVI
          scvi.model.TOTALVI.setup_anndata(
              adata,
              layer="counts",
              batch_key="batch",
              protein_expression_obsm_key="protein_expression"
          )
          ```

          ### Multiome RNA+ATAC (for MultiVI)

          ```python
          # RNA and ATAC in separate AnnData objects or MuData

          import mudata as md

          # If using MuData
          mdata = md.read("multiome.h5mu")

          scvi.model.MULTIVI.setup_mudata(
              mdata,
              rna_layer="counts",
              protein_layer=None,
              batch_key="batch",
              modalities={"rna": "rna", "accessibility": "atac"}
          )
          ```

          ## Complete Preparation Pipeline

          For a complete preparation function, use `prepare_adata()` from `scripts/model_utils.py`:

          ```python
          from model_utils import prepare_adata

          # Prepare data with QC, HVG selection, and layer setup
          adata = prepare_adata(
              adata,
              batch_key="batch",
              n_top_genes=2000,
              min_genes=200,
              max_mito_pct=20
          )

          # Then setup for your model
          import scvi
          scvi.model.SCVI.setup_anndata(adata, layer="counts", batch_key="batch")
          ```

          This function handles:
          - Mitochondrial QC filtering
          - Cell and gene filtering
          - Storing counts in layer
          - HVG selection (batch-aware if batch_key provided)
          - Subsetting to HVGs

          ## Checking Setup

          ```python
          # View registered data
          print(adata.uns['_scvi_manager_uuid'])
          print(adata.uns['_scvi_adata_minify_type'])

          # For scVI
          scvi.model.SCVI.view_anndata_setup(adata)
          ```

          ## Common Issues and Solutions

          | Issue | Cause | Solution |
          |-------|-------|----------|
          | "X should contain integers" | Normalized data in X | Use layer="counts" |
          | "batch_key not found" | Wrong column name | Check adata.obs.columns |
          | Sparse matrix errors | Incompatible format | Convert: adata.X = adata.X.toarray() |
          | Memory error | Too many genes | Subset to HVGs first |
          | NaN in data | Missing values | Filter or impute |

          ## Data Format Reference

          ### Required

          - `adata.X` or `adata.layers["counts"]`: Raw integer counts (sparse OK)
          - `adata.obs`: Cell metadata DataFrame
          - `adata.var`: Gene metadata DataFrame

          ### Recommended

          - `adata.obs["batch"]`: Batch/sample identifiers
          - `adata.var["highly_variable"]`: HVG boolean mask

          ### For scANVI

          - `adata.obs["labels"]`: Cell type annotations
          - Can include "Unknown" for unlabeled cells
    - path: /knowledge-work-plugins/skills/scvi-tools/references/environment_setup.md
      text: |
          # Environment Setup for scvi-tools

          This reference covers installation and environment configuration for scvi-tools.

          ## Installation Options

          ### Option 1: Conda Environment (Recommended)

          ```bash
          # Create environment with GPU support
          conda create -n scvi-env python=3.10
          conda activate scvi-env

          # Install scvi-tools
          pip install scvi-tools

          # For GPU acceleration (recommended for large datasets)
          pip install torch --index-url https://download.pytorch.org/whl/cu118

          # Common dependencies
          pip install scanpy leidenalg
          ```

          ### Option 2: Pip Only

          ```bash
          # Create virtual environment
          python -m venv scvi-env
          source scvi-env/bin/activate  # Linux/Mac
          # scvi-env\Scripts\activate   # Windows

          # Install
          pip install scvi-tools scanpy
          ```

          ### Option 3: With Spatial Analysis Support

          ```bash
          conda create -n scvi-spatial python=3.10
          conda activate scvi-spatial

          pip install scvi-tools scanpy squidpy
          ```

          ### Option 4: With MuData Support (Multiome)

          ```bash
          pip install scvi-tools mudata muon
          ```

          ## Verify Installation

          ```python
          import scvi
          import torch
          import scanpy as sc

          print(f"scvi-tools version: {scvi.__version__}")
          print(f"scanpy version: {sc.__version__}")
          print(f"PyTorch version: {torch.__version__}")
          print(f"GPU available: {torch.cuda.is_available()}")

          if torch.cuda.is_available():
              print(f"GPU device: {torch.cuda.get_device_name(0)}")
              print(f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
          ```

          ## GPU Configuration

          ### Check CUDA Version

          ```bash
          nvidia-smi
          nvcc --version
          ```

          ### PyTorch CUDA Versions

          | CUDA Version | PyTorch Install Command |
          |--------------|------------------------|
          | CUDA 11.8 | `pip install torch --index-url https://download.pytorch.org/whl/cu118` |
          | CUDA 12.1 | `pip install torch --index-url https://download.pytorch.org/whl/cu121` |
          | CPU only | `pip install torch --index-url https://download.pytorch.org/whl/cpu` |

          ### Memory Management

          ```python
          import torch

          # Clear GPU cache between models
          torch.cuda.empty_cache()

          # Monitor memory usage
          print(f"Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
          print(f"Cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB")
          ```

          ## Common Issues

          | Issue | Cause | Solution |
          |-------|-------|----------|
          | `CUDA out of memory` | GPU memory exhausted | Reduce batch_size, use smaller model |
          | `No GPU detected` | CUDA not installed | Install CUDA toolkit matching PyTorch |
          | `Version mismatch` | PyTorch/CUDA incompatibility | Reinstall PyTorch with correct CUDA version |
          | `Import error scvi` | Missing dependencies | `pip install scvi-tools[all]` |

          ## Jupyter Setup

          ```bash
          # Install Jupyter kernel
          pip install ipykernel
          python -m ipykernel install --user --name scvi-env --display-name "scvi-tools"

          # For interactive plots
          pip install matplotlib seaborn
          ```

          ## Recommended Package Versions

          For reproducibility, pin versions:

          ```bash
          pip install \
              scvi-tools>=1.0.0 \
              scanpy>=1.9.0 \
              anndata>=0.9.0 \
              torch>=2.0.0
          ```

          ## Version Compatibility Guide

          ### scvi-tools 1.x vs 0.x API Changes

          The 1.x release introduced breaking changes. Key differences:

          | Operation | 0.x API (deprecated) | 1.x API (current) |
          |-----------|---------------------|-------------------|
          | Setup data | `scvi.data.setup_anndata(adata, ...)` | `scvi.model.SCVI.setup_anndata(adata, ...)` |
          | Register data | `scvi.data.register_tensor_from_anndata(...)` | Built into `setup_anndata` |
          | View setup | `scvi.data.view_anndata_setup(adata)` | `scvi.model.SCVI.view_anndata_setup(adata)` |

          ### Migration from 0.x to 1.x

          ```python
          # OLD (0.x) - DEPRECATED
          import scvi
          scvi.data.setup_anndata(adata, layer="counts", batch_key="batch")
          model = scvi.model.SCVI(adata)

          # NEW (1.x) - CURRENT
          import scvi
          scvi.model.SCVI.setup_anndata(adata, layer="counts", batch_key="batch")
          model = scvi.model.SCVI(adata)
          ```

          ### Model-Specific Setup (1.x)

          Each model has its own setup method:

          ```python
          # scVI
          scvi.model.SCVI.setup_anndata(adata, layer="counts", batch_key="batch")

          # scANVI
          scvi.model.SCANVI.setup_anndata(adata, layer="counts", batch_key="batch", labels_key="cell_type")

          # totalVI
          scvi.model.TOTALVI.setup_anndata(adata, layer="counts", protein_expression_obsm_key="protein")

          # MultiVI (uses MuData)
          scvi.model.MULTIVI.setup_mudata(mdata, rna_layer="counts", atac_layer="counts")

          # PeakVI
          scvi.model.PEAKVI.setup_anndata(adata, batch_key="batch")

          # veloVI
          scvi.external.VELOVI.setup_anndata(adata, spliced_layer="spliced", unspliced_layer="unspliced")
          ```

          ### Minimum Version Requirements

          | Package | Minimum Version | Notes |
          |---------|-----------------|-------|
          | scvi-tools | 1.0.0 | Required for current API |
          | scanpy | 1.9.0 | HVG selection improvements |
          | anndata | 0.9.0 | Improved MuData support |
          | torch | 2.0.0 | Performance improvements |
          | mudata | 0.2.0 | Required for MultiVI |
          | scvelo | 0.2.5 | Required for veloVI |

          ### Check Your Versions

          ```python
          import scvi
          import scanpy as sc
          import anndata
          import torch

          print(f"scvi-tools: {scvi.__version__}")
          print(f"scanpy: {sc.__version__}")
          print(f"anndata: {anndata.__version__}")
          print(f"torch: {torch.__version__}")

          # Check if using 1.x API
          if hasattr(scvi.model.SCVI, 'setup_anndata'):
              print("Using scvi-tools 1.x API")
          else:
              print("WARNING: Using deprecated 0.x API - please upgrade")
          ```

          ### Known Compatibility Issues

          | Issue | Affected Versions | Solution |
          |-------|-------------------|----------|
          | `setup_anndata` not found | scvi-tools < 1.0 | Upgrade to 1.0+ |
          | MuData errors | mudata < 0.2 | `pip install mudata>=0.2.0` |
          | CUDA version mismatch | Any | Reinstall PyTorch for your CUDA |
          | numpy 2.0 issues | Early 2024 builds | `pip install numpy<2.0` |

          ### Upgrading scvi-tools

          ```bash
          # Upgrade to latest
          pip install --upgrade scvi-tools

          # Upgrade all dependencies
          pip install --upgrade scvi-tools scanpy anndata torch

          # If you have issues, clean install
          pip uninstall scvi-tools
          pip cache purge
          pip install scvi-tools
          ```

          ## Testing Installation

          ```python
          # Quick test with sample data
          import scvi
          import scanpy as sc

          # Load test dataset
          adata = scvi.data.heart_cell_atlas_subsampled()
          print(f"Loaded test data: {adata.shape}")

          # Setup and create model (quick test)
          scvi.model.SCVI.setup_anndata(adata, layer="counts", batch_key="cell_source")
          model = scvi.model.SCVI(adata, n_latent=10)
          print("Model created successfully")

          # Quick training test (1 epoch)
          model.train(max_epochs=1)
          print("Training works!")
          ```
    - path: /knowledge-work-plugins/skills/scvi-tools/references/label_transfer.md
      text: |
          # Label Transfer and Reference Mapping with scANVI

          This reference covers using scANVI for transferring cell type annotations from a reference atlas to query data.

          ## Overview

          Reference mapping (also called "label transfer") uses a pre-trained model on annotated reference data to predict cell types in new, unannotated query data. This is faster than re-clustering and more consistent across studies.

          scANVI excels at this because it:
          - Jointly embeds reference and query in shared space
          - Transfers labels probabilistically
          - Handles batch effects between reference and query

          ## When to Use Reference Mapping

          - Annotating new dataset using existing atlas
          - Consistent annotation across multiple studies
          - Speed: no need to re-cluster and manually annotate
          - Quality: leverage expert-curated reference annotations

          ## Workflow Options

          1. **Train new model**: Train scANVI on reference, then map query
          2. **Use pre-trained model**: Load existing model (e.g., from Model Hub)
          3. **scArches**: Extend existing model with query data (preserves reference)

          ## Option 1: Train scANVI on Reference

          ### Step 1: Prepare Reference Data

          ```python
          import scvi
          import scanpy as sc

          # Load reference atlas
          adata_ref = sc.read_h5ad("reference_atlas.h5ad")

          # Check annotations
          print(f"Reference cells: {adata_ref.n_obs}")
          print(f"Cell types: {adata_ref.obs['cell_type'].nunique()}")
          print(adata_ref.obs['cell_type'].value_counts())

          # Ensure raw counts
          adata_ref.layers["counts"] = adata_ref.raw.X.copy() if adata_ref.raw else adata_ref.X.copy()

          # HVG selection
          sc.pp.highly_variable_genes(
              adata_ref,
              n_top_genes=3000,
              flavor="seurat_v3",
              batch_key="batch" if "batch" in adata_ref.obs else None,
              layer="counts"
          )
          adata_ref = adata_ref[:, adata_ref.var["highly_variable"]].copy()
          ```

          ### Step 2: Train scANVI on Reference

          ```python
          # First train scVI (unlabeled)
          scvi.model.SCVI.setup_anndata(
              adata_ref,
              layer="counts",
              batch_key="batch"
          )

          scvi_ref = scvi.model.SCVI(adata_ref, n_latent=30)
          scvi_ref.train(max_epochs=200)

          # Initialize scANVI from scVI
          scanvi_ref = scvi.model.SCANVI.from_scvi_model(
              scvi_ref,
              labels_key="cell_type",
              unlabeled_category="Unknown"
          )

          # Train scANVI
          scanvi_ref.train(max_epochs=50)

          # Save for later use
          scanvi_ref.save("scanvi_reference_model/")
          ```

          ### Step 3: Prepare Query Data

          ```python
          # Load query data
          adata_query = sc.read_h5ad("query_data.h5ad")

          # CRITICAL: Use same genes as reference
          common_genes = adata_ref.var_names.intersection(adata_query.var_names)
          print(f"Common genes: {len(common_genes)}")

          # Subset query to reference genes
          adata_query = adata_query[:, adata_ref.var_names].copy()

          # Handle missing genes (set to 0)
          missing_genes = set(adata_ref.var_names) - set(adata_query.var_names)
          if missing_genes:
              # Add missing genes with zero expression
              import numpy as np
              from scipy.sparse import csr_matrix
              
              zero_matrix = csr_matrix((adata_query.n_obs, len(missing_genes)))
              # ... concat and reorder to match reference
              
          # Store counts
          adata_query.layers["counts"] = adata_query.X.copy()
          ```

          ### Step 4: Map Query to Reference

          ```python
          # Prepare query data for mapping
          scvi.model.SCANVI.prepare_query_anndata(adata_query, scanvi_ref)

          # Create query model from reference
          scanvi_query = scvi.model.SCANVI.load_query_data(
              adata_query,
              scanvi_ref
          )

          # Fine-tune on query (optional but recommended)
          scanvi_query.train(
              max_epochs=100,
              plan_kwargs={"weight_decay": 0.0}
          )

          # Get predictions
          adata_query.obs["predicted_cell_type"] = scanvi_query.predict()

          # Get prediction probabilities
          soft_predictions = scanvi_query.predict(soft=True)
          adata_query.obs["prediction_score"] = soft_predictions.max(axis=1)
          ```

          ### Step 5: Evaluate Predictions

          ```python
          # Confidence scores
          print(f"Mean prediction confidence: {adata_query.obs['prediction_score'].mean():.3f}")

          # Low confidence predictions
          low_conf = adata_query.obs['prediction_score'] < 0.5
          print(f"Low confidence cells: {low_conf.sum()} ({low_conf.mean()*100:.1f}%)")

          # Visualize
          sc.pp.neighbors(adata_query, use_rep="X_scANVI")
          sc.tl.umap(adata_query)
          sc.pl.umap(adata_query, color=['predicted_cell_type', 'prediction_score'])
          ```

          ## Option 2: Use Pre-Trained Models

          ### From Model Hub

          ```python
          # scvi-tools maintains models on HuggingFace
          # Check: https://huggingface.co/scvi-tools

          # Example: Load pre-trained model
          from huggingface_hub import hf_hub_download

          model_path = hf_hub_download(
              repo_id="scvi-tools/example-model",
              filename="model.pt"
          )

          # Load model
          model = scvi.model.SCANVI.load(model_path, adata=adata_query)
          ```

          ### From Published Atlas

          ```python
          # Many atlases provide pre-trained models
          # Example workflow with CellTypist-style model

          # Download reference model
          # model = scvi.model.SCANVI.load("atlas_model/", adata=adata_query)
          ```

          ## Option 3: scArches for Incremental Updates

          scArches extends a reference model without retraining from scratch:

          ```python
          # Load existing reference model
          scanvi_ref = scvi.model.SCANVI.load("reference_model/")

          # Surgery: prepare for query integration
          scanvi_ref.freeze_layers()

          # Map query data
          scvi.model.SCANVI.prepare_query_anndata(adata_query, scanvi_ref)
          scanvi_query = scvi.model.SCANVI.load_query_data(adata_query, scanvi_ref)

          # Train only query-specific parameters
          scanvi_query.train(
              max_epochs=200,
              plan_kwargs={"weight_decay": 0.0}
          )
          ```

          ## Visualize Reference and Query Together

          ```python
          # Concatenate for joint visualization
          adata_ref.obs["dataset"] = "reference"
          adata_query.obs["dataset"] = "query"

          # Get latent representations
          adata_ref.obsm["X_scANVI"] = scanvi_ref.get_latent_representation()
          adata_query.obsm["X_scANVI"] = scanvi_query.get_latent_representation()

          # Combine
          adata_combined = sc.concat([adata_ref, adata_query])

          # Compute combined UMAP
          sc.pp.neighbors(adata_combined, use_rep="X_scANVI")
          sc.tl.umap(adata_combined)

          # Plot
          sc.pl.umap(
              adata_combined,
              color=["dataset", "cell_type", "predicted_cell_type"],
              ncols=2
          )
          ```

          ## Quality Control for Predictions

          ### Confidence Filtering

          ```python
          # Filter predictions by confidence
          confidence_threshold = 0.7

          high_conf = adata_query[adata_query.obs['prediction_score'] >= confidence_threshold].copy()
          low_conf = adata_query[adata_query.obs['prediction_score'] < confidence_threshold].copy()

          print(f"High confidence: {len(high_conf)} ({len(high_conf)/len(adata_query)*100:.1f}%)")
          print(f"Low confidence: {len(low_conf)} ({len(low_conf)/len(adata_query)*100:.1f}%)")
          ```

          ### Marker Validation

          ```python
          # Validate predictions with known markers
          markers = {
              'T cells': ['CD3D', 'CD3E'],
              'B cells': ['CD19', 'MS4A1'],
              'Monocytes': ['CD14', 'LYZ']
          }

          for ct, genes in markers.items():
              ct_cells = adata_query[adata_query.obs['predicted_cell_type'] == ct]
              if len(ct_cells) > 0:
                  for gene in genes:
                      if gene in adata_query.var_names:
                          expr = ct_cells[:, gene].X.mean()
                          print(f"{ct} - {gene}: {expr:.3f}")
          ```

          ## Complete Pipeline

          ```python
          def transfer_labels(
              adata_ref,
              adata_query,
              cell_type_key="cell_type",
              batch_key=None,
              n_top_genes=3000,
              confidence_threshold=0.5
          ):
              """
              Transfer cell type labels from reference to query.
              
              Parameters
              ----------
              adata_ref : AnnData
                  Annotated reference data
              adata_query : AnnData
                  Unannotated query data
              cell_type_key : str
                  Column with cell type annotations in reference
              batch_key : str, optional
                  Batch column
              n_top_genes : int
                  Number of HVGs
              confidence_threshold : float
                  Minimum confidence for predictions
                  
              Returns
              -------
              AnnData with predictions
              """
              import scvi
              import scanpy as sc
              
              # Prepare reference
              adata_ref = adata_ref.copy()
              adata_ref.layers["counts"] = adata_ref.X.copy()
              
              sc.pp.highly_variable_genes(
                  adata_ref,
                  n_top_genes=n_top_genes,
                  flavor="seurat_v3",
                  batch_key=batch_key,
                  layer="counts"
              )
              adata_ref = adata_ref[:, adata_ref.var["highly_variable"]].copy()
              
              # Train reference model
              scvi.model.SCVI.setup_anndata(adata_ref, layer="counts", batch_key=batch_key)
              scvi_ref = scvi.model.SCVI(adata_ref, n_latent=30)
              scvi_ref.train(max_epochs=200)
              
              scanvi_ref = scvi.model.SCANVI.from_scvi_model(
                  scvi_ref,
                  labels_key=cell_type_key,
                  unlabeled_category="Unknown"
              )
              scanvi_ref.train(max_epochs=50)
              
              # Prepare query
              adata_query = adata_query[:, adata_ref.var_names].copy()
              adata_query.layers["counts"] = adata_query.X.copy()
              
              # Map query
              scvi.model.SCANVI.prepare_query_anndata(adata_query, scanvi_ref)
              scanvi_query = scvi.model.SCANVI.load_query_data(adata_query, scanvi_ref)
              scanvi_query.train(max_epochs=100, plan_kwargs={"weight_decay": 0.0})
              
              # Get predictions
              adata_query.obs["predicted_cell_type"] = scanvi_query.predict()
              soft = scanvi_query.predict(soft=True)
              adata_query.obs["prediction_score"] = soft.max(axis=1)
              
              # Mark low confidence
              adata_query.obs["confident_prediction"] = adata_query.obs["prediction_score"] >= confidence_threshold
              
              # Add latent representation
              adata_query.obsm["X_scANVI"] = scanvi_query.get_latent_representation()
              
              return adata_query, scanvi_ref, scanvi_query

          # Usage
          adata_annotated, ref_model, query_model = transfer_labels(
              adata_ref,
              adata_query,
              cell_type_key="cell_type"
          )

          # Visualize
          sc.pp.neighbors(adata_annotated, use_rep="X_scANVI")
          sc.tl.umap(adata_annotated)
          sc.pl.umap(adata_annotated, color=['predicted_cell_type', 'prediction_score'])
          ```

          ## Troubleshooting

          | Issue | Cause | Solution |
          |-------|-------|----------|
          | Many low-confidence predictions | Query has novel cell types | Manually annotate low-confidence cells |
          | Wrong predictions | Reference doesn't match tissue | Use tissue-appropriate reference |
          | Gene mismatch | Different gene naming | Convert gene IDs |
          | All same prediction | Query too different | Check data quality, try different reference |

          ## Key References

          - Xu et al. (2021) "Probabilistic harmonization and annotation of single-cell transcriptomics data with deep generative models"
          - Lotfollahi et al. (2022) "Mapping single-cell data to reference atlases by transfer learning"
    - path: /knowledge-work-plugins/skills/scvi-tools/references/multiome_multivi.md
      text: |
          # Multiome Analysis with MultiVI

          This reference covers joint RNA and ATAC-seq analysis from multiome experiments using MultiVI.

          ## Overview

          MultiVI is a deep generative model for analyzing multiome data (simultaneous RNA-seq and ATAC-seq from the same cells). It:
          - Learns a joint latent representation across modalities
          - Handles missing modalities (RNA-only or ATAC-only cells)
          - Enables batch correction across experiments
          - Supports imputation of missing modalities

          ## Prerequisites

          ```python
          import scvi
          import scanpy as sc
          import mudata as md
          import numpy as np

          print(f"scvi-tools version: {scvi.__version__}")
          ```

          ## Data Formats

          ### Option 1: MuData (Recommended)

          ```python
          # Load multiome data as MuData
          mdata = md.read("multiome.h5mu")

          # Structure:
          # mdata.mod['rna']  - AnnData with RNA counts
          # mdata.mod['atac'] - AnnData with ATAC counts

          print(f"RNA: {mdata.mod['rna'].shape}")
          print(f"ATAC: {mdata.mod['atac'].shape}")
          ```

          ### Option 2: Separate AnnData Objects

          ```python
          # Load separately
          adata_rna = sc.read_h5ad("rna.h5ad")
          adata_atac = sc.read_h5ad("atac.h5ad")

          # Ensure same cells in same order
          common_cells = adata_rna.obs_names.intersection(adata_atac.obs_names)
          adata_rna = adata_rna[common_cells].copy()
          adata_atac = adata_atac[common_cells].copy()
          ```

          ## Step 1: Prepare RNA Data

          ```python
          # RNA preprocessing (standard scvi-tools pipeline)
          adata_rna = mdata.mod['rna'].copy()

          # Filter
          sc.pp.filter_cells(adata_rna, min_genes=200)
          sc.pp.filter_genes(adata_rna, min_cells=3)

          # Store counts
          adata_rna.layers["counts"] = adata_rna.X.copy()

          # HVG selection
          sc.pp.highly_variable_genes(
              adata_rna,
              n_top_genes=4000,
              flavor="seurat_v3",
              layer="counts",
              batch_key="batch"  # If multiple batches
          )

          # Subset to HVGs
          adata_rna = adata_rna[:, adata_rna.var['highly_variable']].copy()
          ```

          ## Step 2: Prepare ATAC Data

          ```python
          # ATAC preprocessing
          adata_atac = mdata.mod['atac'].copy()

          # Filter peaks
          sc.pp.filter_genes(adata_atac, min_cells=10)

          # Binarize accessibility
          adata_atac.X = (adata_atac.X > 0).astype(np.float32)

          # Select top accessible peaks (if too many)
          if adata_atac.n_vars > 50000:
              peak_accessibility = np.array(adata_atac.X.sum(axis=0)).flatten()
              top_peaks = np.argsort(peak_accessibility)[-50000:]
              adata_atac = adata_atac[:, top_peaks].copy()

          # Store in layer
          adata_atac.layers["counts"] = adata_atac.X.copy()
          ```

          ## Step 3: Create Combined MuData

          ```python
          # Ensure matching cells
          common_cells = adata_rna.obs_names.intersection(adata_atac.obs_names)
          adata_rna = adata_rna[common_cells].copy()
          adata_atac = adata_atac[common_cells].copy()

          # Create MuData
          mdata = md.MuData({
              "rna": adata_rna,
              "atac": adata_atac
          })

          print(f"Combined multiome: {mdata.n_obs} cells")
          print(f"RNA features: {mdata.mod['rna'].n_vars}")
          print(f"ATAC features: {mdata.mod['atac'].n_vars}")
          ```

          ## Step 4: Setup MultiVI

          ```python
          # Setup MuData for MultiVI
          scvi.model.MULTIVI.setup_mudata(
              mdata,
              rna_layer="counts",
              atac_layer="counts",
              batch_key="batch",  # Optional
              modalities={
                  "rna_layer": "rna",
                  "batch_key": "rna",
                  "atac_layer": "atac"
              }
          )
          ```

          ## Step 5: Train MultiVI

          ```python
          # Create model
          model = scvi.model.MULTIVI(
              mdata,
              n_latent=20,
              n_layers_encoder=2,
              n_layers_decoder=2
          )

          # Train
          model.train(
              max_epochs=300,
              early_stopping=True,
              early_stopping_patience=10,
              batch_size=128
          )

          # Check training
          model.history['elbo_train'].plot()
          ```

          ## Step 6: Get Joint Representation

          ```python
          # Latent representation
          latent = model.get_latent_representation()

          # Add to MuData
          mdata.obsm["X_MultiVI"] = latent

          # Clustering on joint space
          sc.pp.neighbors(mdata, use_rep="X_MultiVI")
          sc.tl.umap(mdata)
          sc.tl.leiden(mdata, resolution=1.0)

          # Visualize
          sc.pl.umap(mdata, color=['leiden', 'batch'], ncols=2)
          ```

          ## Step 7: Modality-Specific Analysis

          ### Impute Missing Modality

          ```python
          # Impute RNA expression for ATAC-only cells
          # (Useful when integrating with ATAC-only datasets)
          imputed_rna = model.get_normalized_expression(
              modality="rna"
          )

          # Impute accessibility for RNA-only cells
          imputed_atac = model.get_accessibility_estimates()
          ```

          ### Differential Analysis

          ```python
          # Differential expression (RNA)
          de_results = model.differential_expression(
              groupby="leiden",
              group1="0",
              group2="1"
          )

          # Differential accessibility (ATAC)
          da_results = model.differential_accessibility(
              groupby="leiden",
              group1="0",
              group2="1"
          )
          ```

          ## Handling Partial Data

          MultiVI can integrate datasets with only one modality:

          ```python
          # Dataset 1: Full multiome
          # Dataset 2: RNA only
          # Dataset 3: ATAC only

          # Mark missing modalities
          mdata.obs['modality'] = 'paired'  # For cells with both
          # For RNA-only cells, ATAC data should be missing/NaN
          # For ATAC-only cells, RNA data should be missing/NaN

          # MultiVI handles this automatically during training
          ```

          ## Complete Pipeline

          ```python
          def analyze_multiome(
              adata_rna,
              adata_atac,
              batch_key=None,
              n_top_genes=4000,
              n_top_peaks=50000,
              n_latent=20,
              max_epochs=300
          ):
              """
              Complete multiome analysis with MultiVI.

              Parameters
              ----------
              adata_rna : AnnData
                  RNA count data
              adata_atac : AnnData
                  ATAC peak data
              batch_key : str, optional
                  Batch column name
              n_top_genes : int
                  Number of HVGs for RNA
              n_top_peaks : int
                  Number of top peaks for ATAC
              n_latent : int
                  Latent dimensions
              max_epochs : int
                  Maximum training epochs

              Returns
              -------
              MuData with joint representation
              """
              import scvi
              import scanpy as sc
              import mudata as md
              import numpy as np

              # Get common cells
              common_cells = adata_rna.obs_names.intersection(adata_atac.obs_names)
              adata_rna = adata_rna[common_cells].copy()
              adata_atac = adata_atac[common_cells].copy()

              # RNA preprocessing
              sc.pp.filter_genes(adata_rna, min_cells=3)
              adata_rna.layers["counts"] = adata_rna.X.copy()

              if batch_key:
                  sc.pp.highly_variable_genes(
                      adata_rna, n_top_genes=n_top_genes,
                      flavor="seurat_v3", layer="counts", batch_key=batch_key
                  )
              else:
                  sc.pp.normalize_total(adata_rna, target_sum=1e4)
                  sc.pp.log1p(adata_rna)
                  sc.pp.highly_variable_genes(adata_rna, n_top_genes=n_top_genes)
                  adata_rna.X = adata_rna.layers["counts"].copy()

              adata_rna = adata_rna[:, adata_rna.var['highly_variable']].copy()

              # ATAC preprocessing
              sc.pp.filter_genes(adata_atac, min_cells=10)
              adata_atac.X = (adata_atac.X > 0).astype(np.float32)

              if adata_atac.n_vars > n_top_peaks:
                  peak_acc = np.array(adata_atac.X.sum(axis=0)).flatten()
                  top_idx = np.argsort(peak_acc)[-n_top_peaks:]
                  adata_atac = adata_atac[:, top_idx].copy()

              adata_atac.layers["counts"] = adata_atac.X.copy()

              # Create MuData
              mdata = md.MuData({"rna": adata_rna, "atac": adata_atac})

              # Setup and train
              scvi.model.MULTIVI.setup_mudata(
                  mdata,
                  rna_layer="counts",
                  atac_layer="counts",
                  batch_key=batch_key,
                  modalities={"rna_layer": "rna", "batch_key": "rna", "atac_layer": "atac"}
              )

              model = scvi.model.MULTIVI(mdata, n_latent=n_latent)
              model.train(max_epochs=max_epochs, early_stopping=True)

              # Add representation
              mdata.obsm["X_MultiVI"] = model.get_latent_representation()

              # Cluster
              sc.pp.neighbors(mdata, use_rep="X_MultiVI")
              sc.tl.umap(mdata)
              sc.tl.leiden(mdata)

              return mdata, model


          # Usage
          mdata, model = analyze_multiome(
              adata_rna,
              adata_atac,
              batch_key="sample"
          )

          sc.pl.umap(mdata, color=['leiden', 'sample'])
          ```

          ## Peak-to-Gene Linking

          ```python
          # Link ATAC peaks to genes based on correlation in latent space
          # This identifies regulatory relationships

          def link_peaks_to_genes(model, mdata, distance_threshold=100000):
              """
              Link peaks to nearby genes based on correlation.

              Parameters
              ----------
              model : MULTIVI
                  Trained model
              mdata : MuData
                  Multiome data
              distance_threshold : int
                  Maximum distance (bp) to link peak to gene

              Returns
              -------
              DataFrame of peak-gene links
              """
              # Get imputed values
              rna_imputed = model.get_normalized_expression()
              atac_imputed = model.get_accessibility_estimates()

              # Correlate peak accessibility with gene expression
              # for peaks near gene promoters
              # ... (requires genomic coordinates)

              return peak_gene_links
          ```

          ## Troubleshooting

          | Issue | Cause | Solution |
          |-------|-------|----------|
          | Different cell counts | Cells missing in one modality | Use common cells only |
          | Training instability | Imbalanced modalities | Normalize feature counts |
          | Poor clustering | Too few features | Increase n_top_genes/peaks |
          | Memory error | Large ATAC matrix | Reduce peak count, use sparse |
          | Batch dominates | Strong technical effects | Ensure batch_key is set |

          ## Key References

          - Ashuach et al. (2023) "MultiVI: deep generative model for the integration of multimodal data"
    - path: /knowledge-work-plugins/skills/scvi-tools/references/rna_velocity_velovi.md
      text: |
          # RNA Velocity with veloVI

          This reference covers RNA velocity analysis using veloVI, a deep learning approach that improves upon traditional velocity methods.

          ## Overview

          RNA velocity estimates the future state of cells by modeling:
          - **Unspliced RNA**: Newly transcribed, contains introns
          - **Spliced RNA**: Mature mRNA, introns removed

          The ratio of unspliced to spliced indicates whether a gene is being upregulated or downregulated.

          ## Why veloVI?

          Traditional methods (velocyto, scVelo) have limitations:
          - Assume steady-state or dynamical model
          - Sensitive to noise
          - Don't handle batch effects

          veloVI addresses these with:
          - Probabilistic modeling
          - Better uncertainty quantification
          - Integration with scVI framework

          ## Prerequisites

          ```python
          import scvi
          import scvelo as scv
          import scanpy as sc
          import numpy as np

          print(f"scvi-tools version: {scvi.__version__}")
          print(f"scvelo version: {scv.__version__}")
          ```

          ## Step 1: Generate Spliced/Unspliced Counts

          ### From BAM Files (velocyto)

          ```bash
          # Run velocyto on Cell Ranger output
          velocyto run10x /path/to/cellranger_output /path/to/genes.gtf

          # Output: velocyto.loom file with spliced/unspliced layers
          ```

          ### From kb-python (kallisto|bustools)

          ```bash
          # Faster alternative using kallisto
          kb count \
              --workflow lamanno \
              -i index.idx \
              -g t2g.txt \
              -c1 spliced_t2c.txt \
              -c2 unspliced_t2c.txt \
              -x 10xv3 \
              -o output \
              R1.fastq.gz R2.fastq.gz
          ```

          ## Step 2: Load Velocity Data

          ```python
          # Load loom file from velocyto
          adata = scv.read("velocyto_output.loom")

          # Or load from kb-python
          adata = sc.read_h5ad("adata.h5ad")
          # Spliced in adata.layers["spliced"]
          # Unspliced in adata.layers["unspliced"]

          # Check layers
          print("Available layers:", list(adata.layers.keys()))
          print(f"Spliced shape: {adata.layers['spliced'].shape}")
          print(f"Unspliced shape: {adata.layers['unspliced'].shape}")
          ```

          ### Merge with Existing AnnData

          ```python
          # If you have separate loom and h5ad
          ldata = scv.read("velocyto.loom")
          adata = sc.read_h5ad("processed.h5ad")

          # Merge velocity data into processed adata
          adata = scv.utils.merge(adata, ldata)
          ```

          ## Step 3: Preprocessing for Velocity

          ```python
          # Filter and normalize
          scv.pp.filter_and_normalize(
              adata,
              min_shared_counts=20,
              n_top_genes=2000
          )

          # Compute moments (for scVelo comparison)
          scv.pp.moments(adata, n_pcs=30, n_neighbors=30)
          ```

          ## Step 4: Run veloVI

          ### Setup AnnData

          ```python
          # Setup for veloVI
          scvi.model.VELOVI.setup_anndata(
              adata,
              spliced_layer="spliced",
              unspliced_layer="unspliced"
          )
          ```

          ### Train Model

          ```python
          # Create and train veloVI model
          vae = scvi.model.VELOVI(adata)

          vae.train(
              max_epochs=500,
              early_stopping=True,
              batch_size=256
          )

          # Check training
          vae.history["elbo_train"].plot()
          ```

          ### Get Velocity Estimates

          ```python
          # Get latent time
          latent_time = vae.get_latent_time(n_samples=25)
          adata.obs["veloVI_latent_time"] = latent_time

          # Get velocity
          velocities = vae.get_velocity(n_samples=25)
          adata.layers["veloVI_velocity"] = velocities

          # Get expression states
          adata.layers["veloVI_expression"] = vae.get_expression_fit(n_samples=25)
          ```

          ## Step 5: Visualize Velocity

          ### Velocity Streamlines

          ```python
          # Compute velocity graph
          scv.tl.velocity_graph(adata, vkey="veloVI_velocity")

          # Plot streamlines on UMAP
          scv.pl.velocity_embedding_stream(
              adata,
              basis="umap",
              vkey="veloVI_velocity",
              color="cell_type"
          )
          ```

          ### Velocity Arrows

          ```python
          # Individual cell arrows
          scv.pl.velocity_embedding(
              adata,
              basis="umap",
              vkey="veloVI_velocity",
              arrow_length=3,
              arrow_size=2,
              color="cell_type"
          )
          ```

          ### Latent Time

          ```python
          # Plot latent time (pseudotime from velocity)
          sc.pl.umap(adata, color="veloVI_latent_time", cmap="viridis")
          ```

          ## Step 6: Compare with scVelo

          ```python
          # Run standard scVelo for comparison
          scv.tl.velocity(adata, mode="dynamical")
          scv.tl.velocity_graph(adata)

          # Compare velocity fields
          fig, axes = plt.subplots(1, 2, figsize=(12, 5))

          scv.pl.velocity_embedding_stream(
              adata, basis="umap", ax=axes[0], 
              title="scVelo", show=False
          )

          scv.pl.velocity_embedding_stream(
              adata, basis="umap", vkey="veloVI_velocity",
              ax=axes[1], title="veloVI", show=False
          )

          plt.tight_layout()
          ```

          ## Step 7: Gene-Level Analysis

          ### Velocity Phase Portraits

          ```python
          # Plot phase portrait for specific genes
          genes = ["SOX2", "PAX6", "DCX", "NEUROD1"]

          scv.pl.velocity(
              adata,
              var_names=genes,
              vkey="veloVI_velocity",
              colorbar=True
          )
          ```

          ### Gene Dynamics

          ```python
          # Plot expression over latent time
          for gene in genes:
              fig, ax = plt.subplots(figsize=(6, 4))
              
              sc.pl.scatter(
                  adata,
                  x="veloVI_latent_time",
                  y=gene,
                  color="cell_type",
                  ax=ax,
                  show=False
              )
              ax.set_xlabel("Latent Time")
              ax.set_ylabel(f"{gene} Expression")
          ```

          ### Driver Genes

          ```python
          # Find genes driving velocity
          scv.tl.rank_velocity_genes(
              adata,
              vkey="veloVI_velocity",
              groupby="cell_type"
          )

          # Get top genes per cluster
          df = scv.get_df(adata, "rank_velocity_genes/names")
          print(df.head(10))
          ```

          ## Step 8: Uncertainty Quantification

          veloVI provides uncertainty estimates:

          ```python
          # Get velocity with uncertainty
          velocity_mean, velocity_std = vae.get_velocity(
              n_samples=100,
              return_mean=True,
              return_numpy=True
          )

          # Store uncertainty
          adata.layers["velocity_uncertainty"] = velocity_std

          # Visualize uncertainty
          adata.obs["mean_velocity_uncertainty"] = velocity_std.mean(axis=1)
          sc.pl.umap(adata, color="mean_velocity_uncertainty")
          ```

          ## Complete Pipeline

          ```python
          def run_velocity_analysis(
              adata,
              spliced_layer="spliced",
              unspliced_layer="unspliced",
              n_top_genes=2000,
              max_epochs=500
          ):
              """
              Complete RNA velocity analysis with veloVI.
              
              Parameters
              ----------
              adata : AnnData
                  Data with spliced/unspliced layers
              spliced_layer : str
                  Layer name for spliced counts
              unspliced_layer : str
                  Layer name for unspliced counts
              n_top_genes : int
                  Number of velocity genes
              max_epochs : int
                  Training epochs
                  
              Returns
              -------
              AnnData with velocity and model
              """
              import scvi
              import scvelo as scv
              import scanpy as sc
              
              adata = adata.copy()
              
              # Preprocessing
              scv.pp.filter_and_normalize(
                  adata,
                  min_shared_counts=20,
                  n_top_genes=n_top_genes
              )
              
              # Compute moments (needed for some visualizations)
              scv.pp.moments(adata, n_pcs=30, n_neighbors=30)
              
              # Setup veloVI
              scvi.model.VELOVI.setup_anndata(
                  adata,
                  spliced_layer=spliced_layer,
                  unspliced_layer=unspliced_layer
              )
              
              # Train
              model = scvi.model.VELOVI(adata)
              model.train(max_epochs=max_epochs, early_stopping=True)
              
              # Get results
              adata.obs["latent_time"] = model.get_latent_time(n_samples=25)
              adata.layers["velocity"] = model.get_velocity(n_samples=25)
              
              # Compute velocity graph for visualization
              scv.tl.velocity_graph(adata, vkey="velocity")
              
              # Compute UMAP if not present
              if "X_umap" not in adata.obsm:
                  sc.pp.neighbors(adata)
                  sc.tl.umap(adata)
              
              return adata, model

          # Usage
          adata_velocity, model = run_velocity_analysis(adata)

          # Visualize
          scv.pl.velocity_embedding_stream(
              adata_velocity,
              basis="umap",
              vkey="velocity",
              color="cell_type"
          )

          sc.pl.umap(adata_velocity, color="latent_time")
          ```

          ## Advanced: Batch-Aware Velocity

          ```python
          # For multi-batch data, include batch in model
          scvi.model.VELOVI.setup_anndata(
              adata,
              spliced_layer="spliced",
              unspliced_layer="unspliced",
              batch_key="batch"
          )

          model = scvi.model.VELOVI(adata)
          model.train()
          ```

          ## Interpreting Results

          ### Good Velocity Signal

          - Streamlines follow expected differentiation
          - Latent time correlates with known biology
          - Phase portraits show clear dynamics

          ### Poor Velocity Signal

          - Random/chaotic streamlines
          - No correlation with known markers
          - May indicate:
            - Insufficient unspliced reads
            - Cells at steady state
            - Technical issues

          ## Troubleshooting

          | Issue | Cause | Solution |
          |-------|-------|----------|
          | No velocity signal | Low unspliced counts | Check sequencing depth, use kb-python |
          | Reversed direction | Wrong root assignment | Manually set root cells |
          | Noisy streamlines | Too many genes | Reduce n_top_genes |
          | Memory error | Large dataset | Reduce batch_size |

          ## Key References

          - Gayoso et al. (2023) "Deep generative modeling of transcriptional dynamics for RNA velocity analysis in single cells"
          - La Manno et al. (2018) "RNA velocity of single cells"
          - Bergen et al. (2020) "Generalizing RNA velocity to transient cell states through dynamical modeling"
    - path: /knowledge-work-plugins/skills/scvi-tools/references/scarches_mapping.md
      text: |
          # Reference Mapping with scArches

          This reference covers using scArches for mapping query data to pre-trained reference models without retraining from scratch.

          ## Overview

          scArches (single-cell architecture surgery) enables:
          - Mapping new data to existing reference atlases
          - Extending models with new batches/studies
          - Transfer learning without full retraining
          - Preserving reference structure while integrating query

          ## When to Use scArches

          | Scenario | Approach |
          |----------|----------|
          | Map query to existing atlas | scArches query mapping |
          | Extend atlas with new data | scArches model surgery |
          | No pre-trained model available | Train scANVI from scratch |
          | Query very different from reference | Consider retraining |

          ## Prerequisites

          ```python
          import scvi
          import scanpy as sc
          import numpy as np

          print(f"scvi-tools version: {scvi.__version__}")
          ```

          ## Workflow 1: Map Query to Pre-Trained Reference

          ### Step 1: Load Pre-Trained Reference Model

          ```python
          # Load saved reference model
          # The model must have been trained with scvi-tools
          reference_model = scvi.model.SCVI.load("reference_model/")

          # Or load scANVI for label transfer
          reference_model = scvi.model.SCANVI.load("reference_scanvi_model/")

          # Check model info
          print(f"Model type: {type(reference_model)}")
          print(f"Training data shape: {reference_model.adata.shape}")
          ```

          ### Step 2: Prepare Query Data

          ```python
          # Load query data
          adata_query = sc.read_h5ad("query_data.h5ad")

          # CRITICAL: Match genes to reference
          reference_genes = reference_model.adata.var_names
          query_genes = adata_query.var_names

          # Check overlap
          common_genes = reference_genes.intersection(query_genes)
          print(f"Reference genes: {len(reference_genes)}")
          print(f"Query genes: {len(query_genes)}")
          print(f"Overlap: {len(common_genes)}")

          # Subset query to reference genes
          adata_query = adata_query[:, reference_genes].copy()

          # Handle missing genes (filled with zeros automatically by prepare_query_anndata)
          ```

          ### Step 3: Prepare Query AnnData

          ```python
          # Store raw counts
          adata_query.layers["counts"] = adata_query.X.copy()

          # Prepare query for mapping
          # This aligns the query data structure to match the reference
          scvi.model.SCVI.prepare_query_anndata(adata_query, reference_model)
          ```

          ### Step 4: Create Query Model

          ```python
          # Create query model from reference
          # This initializes with reference weights
          query_model = scvi.model.SCVI.load_query_data(
              adata_query,
              reference_model
          )

          # The query model inherits:
          # - Reference architecture
          # - Reference encoder weights (frozen by default)
          # - Decoder is fine-tuned for query
          ```

          ### Step 5: Fine-Tune on Query

          ```python
          # Fine-tune the query model
          # This adjusts decoder weights for query-specific effects
          query_model.train(
              max_epochs=200,
              plan_kwargs={
                  "weight_decay": 0.0  # Less regularization for fine-tuning
              }
          )

          # Check training
          query_model.history['elbo_train'].plot()
          ```

          ### Step 6: Get Query Representation

          ```python
          # Get latent representation
          # Query cells are embedded in same space as reference
          adata_query.obsm["X_scVI"] = query_model.get_latent_representation()

          # Visualize
          sc.pp.neighbors(adata_query, use_rep="X_scVI")
          sc.tl.umap(adata_query)
          sc.pl.umap(adata_query, color=['cell_type', 'batch'])
          ```

          ## Workflow 2: scANVI Query Mapping with Label Transfer

          For transferring cell type labels from reference to query:

          ### Step 1: Load scANVI Reference

          ```python
          # Reference must be scANVI model (trained with labels)
          reference_scanvi = scvi.model.SCANVI.load("scanvi_reference/")

          # Check available labels
          print("Reference cell types:")
          print(reference_scanvi.adata.obs['cell_type'].value_counts())
          ```

          ### Step 2: Prepare and Map Query

          ```python
          # Prepare query
          adata_query.layers["counts"] = adata_query.X.copy()
          adata_query = adata_query[:, reference_scanvi.adata.var_names].copy()

          scvi.model.SCANVI.prepare_query_anndata(adata_query, reference_scanvi)

          # Create query model
          query_scanvi = scvi.model.SCANVI.load_query_data(
              adata_query,
              reference_scanvi
          )

          # Fine-tune
          query_scanvi.train(
              max_epochs=100,
              plan_kwargs={"weight_decay": 0.0}
          )
          ```

          ### Step 3: Get Predictions

          ```python
          # Predict cell types
          predictions = query_scanvi.predict()
          adata_query.obs["predicted_cell_type"] = predictions

          # Get prediction probabilities
          soft_predictions = query_scanvi.predict(soft=True)
          adata_query.obs["prediction_confidence"] = soft_predictions.max(axis=1)

          # Latent representation
          adata_query.obsm["X_scANVI"] = query_scanvi.get_latent_representation()

          # Visualize predictions
          sc.pp.neighbors(adata_query, use_rep="X_scANVI")
          sc.tl.umap(adata_query)
          sc.pl.umap(adata_query, color=['predicted_cell_type', 'prediction_confidence'])
          ```

          ### Step 4: Evaluate Predictions

          ```python
          # Distribution of predictions
          print(adata_query.obs['predicted_cell_type'].value_counts())

          # Confidence statistics
          print(f"Mean confidence: {adata_query.obs['prediction_confidence'].mean():.3f}")
          print(f"Low confidence (<0.5): {(adata_query.obs['prediction_confidence'] < 0.5).sum()}")

          # Filter low-confidence predictions
          high_conf = adata_query[adata_query.obs['prediction_confidence'] >= 0.7].copy()
          print(f"High confidence cells: {len(high_conf)} ({len(high_conf)/len(adata_query)*100:.1f}%)")
          ```

          ## Workflow 3: Model Surgery (Extending Reference)

          Extend an existing reference model with new data:

          ### Step 1: Freeze Reference Layers

          ```python
          # Load reference model
          reference_model = scvi.model.SCVI.load("reference_model/")

          # Get reference representation (before surgery)
          adata_ref = reference_model.adata
          adata_ref.obsm["X_scVI_before"] = reference_model.get_latent_representation()
          ```

          ### Step 2: Prepare Combined Data

          ```python
          # Add batch information
          adata_ref.obs["dataset"] = "reference"
          adata_query.obs["dataset"] = "query"

          # Combine
          adata_combined = sc.concat([adata_ref, adata_query])
          adata_combined.layers["counts"] = adata_combined.X.copy()
          ```

          ### Step 3: Surgery Approach

          ```python
          # Option A: Use load_query_data (recommended)
          scvi.model.SCVI.prepare_query_anndata(adata_query, reference_model)
          extended_model = scvi.model.SCVI.load_query_data(adata_query, reference_model)
          extended_model.train(max_epochs=200)

          # Option B: Retrain with combined data (if query is large)
          # This doesn't preserve reference exactly but may give better results
          scvi.model.SCVI.setup_anndata(
              adata_combined,
              layer="counts",
              batch_key="dataset"
          )
          new_model = scvi.model.SCVI(adata_combined, n_latent=30)
          new_model.train(max_epochs=200)
          ```

          ## Joint Visualization

          Visualize reference and query together:

          ```python
          # Get latent representations
          adata_ref.obsm["X_scVI"] = reference_model.get_latent_representation()
          adata_query.obsm["X_scVI"] = query_model.get_latent_representation()

          # Combine for visualization
          adata_ref.obs["source"] = "reference"
          adata_query.obs["source"] = "query"
          adata_combined = sc.concat([adata_ref, adata_query])

          # Compute joint UMAP
          sc.pp.neighbors(adata_combined, use_rep="X_scVI")
          sc.tl.umap(adata_combined)

          # Visualize
          import matplotlib.pyplot as plt
          fig, axes = plt.subplots(1, 3, figsize=(15, 4))

          sc.pl.umap(adata_combined, color="source", ax=axes[0], show=False, title="Source")
          sc.pl.umap(adata_combined, color="cell_type", ax=axes[1], show=False, title="Cell Type")
          sc.pl.umap(adata_combined, color="batch", ax=axes[2], show=False, title="Batch")

          plt.tight_layout()
          ```

          ## Using Public Atlas Models

          ### From HuggingFace Model Hub

          ```python
          from huggingface_hub import hf_hub_download

          # Download model files
          model_dir = hf_hub_download(
              repo_id="scvi-tools/model-name",  # Replace with actual repo
              filename="model.pt",
              local_dir="./downloaded_model/"
          )

          # Load model
          atlas_model = scvi.model.SCANVI.load(model_dir)
          ```

          ### From CellxGene

          ```python
          # Many CellxGene datasets provide pre-trained models
          # Check dataset documentation for model availability
          # https://cellxgene.cziscience.com/

          # Example workflow:
          # 1. Download reference dataset and model
          # 2. Load model: model = scvi.model.SCANVI.load("cellxgene_model/")
          # 3. Map your query data using steps above
          ```

          ## Complete Pipeline

          ```python
          def map_query_to_reference(
              adata_query,
              reference_model_path,
              model_type="scanvi",
              max_epochs=100,
              confidence_threshold=0.5
          ):
              """
              Map query data to pre-trained reference model.

              Parameters
              ----------
              adata_query : AnnData
                  Query data with raw counts
              reference_model_path : str
                  Path to saved reference model
              model_type : str
                  "scvi" or "scanvi"
              max_epochs : int
                  Fine-tuning epochs
              confidence_threshold : float
                  Minimum prediction confidence (for scANVI)

              Returns
              -------
              Mapped AnnData with predictions (if scANVI)
              """
              import scvi

              # Load reference
              if model_type == "scanvi":
                  reference_model = scvi.model.SCANVI.load(reference_model_path)
                  ModelClass = scvi.model.SCANVI
              else:
                  reference_model = scvi.model.SCVI.load(reference_model_path)
                  ModelClass = scvi.model.SCVI

              # Prepare query
              adata_query = adata_query.copy()
              adata_query = adata_query[:, reference_model.adata.var_names].copy()
              adata_query.layers["counts"] = adata_query.X.copy()

              # Map query
              ModelClass.prepare_query_anndata(adata_query, reference_model)
              query_model = ModelClass.load_query_data(adata_query, reference_model)

              # Fine-tune
              query_model.train(
                  max_epochs=max_epochs,
                  plan_kwargs={"weight_decay": 0.0}
              )

              # Get results
              rep_key = "X_scANVI" if model_type == "scanvi" else "X_scVI"
              adata_query.obsm[rep_key] = query_model.get_latent_representation()

              if model_type == "scanvi":
                  adata_query.obs["predicted_cell_type"] = query_model.predict()
                  soft = query_model.predict(soft=True)
                  adata_query.obs["prediction_confidence"] = soft.max(axis=1)
                  adata_query.obs["confident"] = adata_query.obs["prediction_confidence"] >= confidence_threshold

              # Compute UMAP
              sc.pp.neighbors(adata_query, use_rep=rep_key)
              sc.tl.umap(adata_query)

              return adata_query, query_model


          # Usage
          adata_mapped, model = map_query_to_reference(
              adata_query,
              "reference_scanvi_model/",
              model_type="scanvi"
          )

          # Visualize
          sc.pl.umap(adata_mapped, color=['predicted_cell_type', 'prediction_confidence'])
          ```

          ## Troubleshooting

          | Issue | Cause | Solution |
          |-------|-------|----------|
          | Gene mismatch | Different gene naming | Convert gene IDs (Ensembl ↔ Symbol) |
          | Many low-confidence | Query has novel types | Manually annotate low-confidence cells |
          | Poor mapping | Query too different | Consider retraining with combined data |
          | Memory error | Large query | Process in batches |
          | Version mismatch | Different scvi-tools version | Use same version as reference training |

          ## Key References

          - Lotfollahi et al. (2022) "Mapping single-cell data to reference atlases by transfer learning"
          - Xu et al. (2021) "Probabilistic harmonization and annotation of single-cell transcriptomics data with deep generative models"
    - path: /knowledge-work-plugins/skills/scvi-tools/references/scrna_integration.md
      text: |
          # scRNA-seq Integration with scVI and scANVI

          This reference covers batch correction and dataset integration using scVI (unsupervised) and scANVI (semi-supervised with cell type labels).

          ## Overview

          Single-cell datasets often have batch effects from:
          - Different donors/patients
          - Different experimental batches
          - Different technologies (10x v2 vs v3)
          - Different studies

          scVI and scANVI learn a shared latent space where batch effects are removed while biological variation is preserved.

          ## When to Use Which Model

          | Model | Use When | Labels Needed |
          |-------|----------|---------------|
          | **scVI** | No labels available, exploratory analysis | No |
          | **scANVI** | Have partial/full labels, want better preservation | Yes (partial OK) |

          ## scVI Integration Workflow

          ### Step 1: Prepare Data

          ```python
          import scvi
          import scanpy as sc

          # Load datasets
          adata1 = sc.read_h5ad("dataset1.h5ad")
          adata2 = sc.read_h5ad("dataset2.h5ad")

          # Add batch annotation
          adata1.obs["batch"] = "batch1"
          adata2.obs["batch"] = "batch2"

          # Concatenate
          adata = sc.concat([adata1, adata2], label="batch")

          # Ensure we have raw counts
          # If data is normalized, recover from .raw
          if hasattr(adata, 'raw') and adata.raw is not None:
              adata = adata.raw.to_adata()

          # Store counts
          adata.layers["counts"] = adata.X.copy()
          ```

          ### Step 2: HVG Selection Across Batches

          ```python
          # Select HVGs considering batch
          sc.pp.highly_variable_genes(
              adata,
              n_top_genes=2000,
              flavor="seurat_v3",
              batch_key="batch",
              layer="counts"
          )

          # Subset to HVGs
          adata = adata[:, adata.var["highly_variable"]].copy()
          ```

          ### Step 3: Setup and Train scVI

          ```python
          # Register data with scVI
          scvi.model.SCVI.setup_anndata(
              adata,
              layer="counts",
              batch_key="batch"
          )

          # Create model
          model = scvi.model.SCVI(
              adata,
              n_latent=30,          # Latent dimensions
              n_layers=2,           # Encoder/decoder depth
              gene_likelihood="nb"  # negative binomial (or "zinb")
          )

          # Train
          model.train(
              max_epochs=200,
              early_stopping=True,
              early_stopping_patience=10,
              batch_size=128
          )

          # Plot training history
          model.history["elbo_train"].plot()
          ```

          ### Step 4: Get Integrated Representation

          ```python
          # Get latent representation
          adata.obsm["X_scVI"] = model.get_latent_representation()

          # Use for clustering and visualization
          sc.pp.neighbors(adata, use_rep="X_scVI", n_neighbors=15)
          sc.tl.umap(adata)
          sc.tl.leiden(adata, resolution=1.0)

          # Visualize integration
          sc.pl.umap(adata, color=["batch", "leiden"], ncols=2)
          ```

          ### Step 5: Save Model

          ```python
          # Save model for later use
          model.save("scvi_model/")

          # Load model
          model = scvi.model.SCVI.load("scvi_model/", adata=adata)
          ```

          ## scANVI Integration Workflow

          scANVI extends scVI with cell type labels for better biological preservation.

          ### Step 1: Prepare Data with Labels

          ```python
          # Labels should be in adata.obs
          # Use "Unknown" for unlabeled cells
          print(adata.obs["cell_type"].value_counts())

          # For partially labeled data
          # Mark unlabeled cells
          adata.obs["cell_type_scanvi"] = adata.obs["cell_type"].copy()
          # adata.obs.loc[unlabeled_mask, "cell_type_scanvi"] = "Unknown"
          ```

          ### Step 2: Option A - Train scANVI from Scratch

          ```python
          # Setup for scANVI
          scvi.model.SCANVI.setup_anndata(
              adata,
              layer="counts",
              batch_key="batch",
              labels_key="cell_type"
          )

          # Create model
          scanvi_model = scvi.model.SCANVI(
              adata,
              n_latent=30,
              n_layers=2
          )

          # Train
          scanvi_model.train(max_epochs=200)
          ```

          ### Step 2: Option B - Initialize scANVI from scVI (Recommended)

          ```python
          # First train scVI
          scvi.model.SCVI.setup_anndata(adata, layer="counts", batch_key="batch")
          scvi_model = scvi.model.SCVI(adata, n_latent=30)
          scvi_model.train(max_epochs=200)

          # Initialize scANVI from scVI
          scanvi_model = scvi.model.SCANVI.from_scvi_model(
              scvi_model,
              labels_key="cell_type",
              unlabeled_category="Unknown"  # For partially labeled data
          )

          # Fine-tune scANVI (fewer epochs needed)
          scanvi_model.train(max_epochs=50)
          ```

          ### Step 3: Get Results

          ```python
          # Latent representation
          adata.obsm["X_scANVI"] = scanvi_model.get_latent_representation()

          # Predicted labels for unlabeled cells
          predictions = scanvi_model.predict()
          adata.obs["predicted_cell_type"] = predictions

          # Prediction probabilities
          soft_predictions = scanvi_model.predict(soft=True)

          # Visualization
          sc.pp.neighbors(adata, use_rep="X_scANVI")
          sc.tl.umap(adata)
          sc.pl.umap(adata, color=["batch", "cell_type", "predicted_cell_type"])
          ```

          ## Comparing Integration Quality

          ### Visual Assessment

          ```python
          import matplotlib.pyplot as plt

          fig, axes = plt.subplots(1, 3, figsize=(15, 4))

          # Before integration (on PCA)
          sc.pp.pca(adata)
          sc.pl.pca(adata, color="batch", ax=axes[0], title="Before (PCA)", show=False)

          # After scVI
          sc.pp.neighbors(adata, use_rep="X_scVI")
          sc.tl.umap(adata)
          sc.pl.umap(adata, color="batch", ax=axes[1], title="After scVI", show=False)

          # After scANVI
          sc.pp.neighbors(adata, use_rep="X_scANVI")
          sc.tl.umap(adata)
          sc.pl.umap(adata, color="batch", ax=axes[2], title="After scANVI", show=False)

          plt.tight_layout()
          ```

          ### Quantitative Metrics (scib)

          ```python
          # pip install scib-metrics

          from scib_metrics.benchmark import Benchmarker

          bm = Benchmarker(
              adata,
              batch_key="batch",
              label_key="cell_type",
              embedding_obsm_keys=["X_pca", "X_scVI", "X_scANVI"]
          )

          bm.benchmark()
          bm.plot_results_table()
          ```

          ## Differential Expression

          scVI provides differential expression that accounts for batch effects:

          ```python
          # DE between groups
          de_results = model.differential_expression(
              groupby="cell_type",
              group1="T cells",
              group2="B cells"
          )

          # Filter significant
          de_sig = de_results[
              (de_results["is_de_fdr_0.05"] == True) &
              (abs(de_results["lfc_mean"]) > 1)
          ]

          print(de_sig.head(20))
          ```

          ## Advanced: Multiple Categorical Covariates

          ```python
          # Include additional covariates beyond batch
          scvi.model.SCVI.setup_anndata(
              adata,
              layer="counts",
              batch_key="batch",
              categorical_covariate_keys=["donor", "technology"]
          )

          model = scvi.model.SCVI(adata, n_latent=30)
          model.train()
          ```

          ## Training Tips

          ### For Large Datasets (>100k cells)

          ```python
          model.train(
              max_epochs=100,      # Fewer epochs needed
              batch_size=256,      # Larger batches
              train_size=0.9,      # Less validation
              early_stopping=True
          )
          ```

          ### For Small Datasets (<10k cells)

          ```python
          model = scvi.model.SCVI(
              adata,
              n_latent=10,         # Smaller latent space
              n_layers=1,          # Simpler model
              dropout_rate=0.2     # More regularization
          )

          model.train(
              max_epochs=400,
              batch_size=64
          )
          ```

          ### Monitoring Training

          ```python
          # Check training curves
          import matplotlib.pyplot as plt

          fig, ax = plt.subplots()
          ax.plot(model.history["elbo_train"], label="Train")
          ax.plot(model.history["elbo_validation"], label="Validation")
          ax.set_xlabel("Epoch")
          ax.set_ylabel("ELBO")
          ax.legend()

          # Should see convergence without overfitting
          ```

          ## Complete Pipeline

          ```python
          def integrate_datasets(
              adatas,
              batch_key="batch",
              labels_key=None,
              n_top_genes=2000,
              n_latent=30
          ):
              """
              Integrate multiple scRNA-seq datasets.
              
              Parameters
              ----------
              adatas : dict
                  Dictionary of {batch_name: AnnData}
              batch_key : str
                  Key for batch annotation
              labels_key : str, optional
                  Key for cell type labels (uses scANVI if provided)
              n_top_genes : int
                  Number of HVGs
              n_latent : int
                  Latent dimensions
                  
              Returns
              -------
              AnnData with integrated representation
              """
              import scvi
              import scanpy as sc
              
              # Add batch labels and concatenate
              for batch_name, adata in adatas.items():
                  adata.obs[batch_key] = batch_name
              
              adata = sc.concat(list(adatas.values()), label=batch_key)
              
              # Store counts
              adata.layers["counts"] = adata.X.copy()
              
              # HVG selection
              sc.pp.highly_variable_genes(
                  adata,
                  n_top_genes=n_top_genes,
                  flavor="seurat_v3",
                  batch_key=batch_key,
                  layer="counts"
              )
              adata = adata[:, adata.var["highly_variable"]].copy()
              
              # Train model
              if labels_key and labels_key in adata.obs.columns:
                  # Use scANVI
                  scvi.model.SCVI.setup_anndata(adata, layer="counts", batch_key=batch_key)
                  scvi_model = scvi.model.SCVI(adata, n_latent=n_latent)
                  scvi_model.train(max_epochs=200)
                  
                  model = scvi.model.SCANVI.from_scvi_model(
                      scvi_model,
                      labels_key=labels_key,
                      unlabeled_category="Unknown"
                  )
                  model.train(max_epochs=50)
                  rep_key = "X_scANVI"
              else:
                  # Use scVI
                  scvi.model.SCVI.setup_anndata(adata, layer="counts", batch_key=batch_key)
                  model = scvi.model.SCVI(adata, n_latent=n_latent)
                  model.train(max_epochs=200)
                  rep_key = "X_scVI"
              
              # Add representation
              adata.obsm[rep_key] = model.get_latent_representation()
              
              # Compute neighbors and UMAP
              sc.pp.neighbors(adata, use_rep=rep_key)
              sc.tl.umap(adata)
              sc.tl.leiden(adata)
              
              return adata, model

          # Usage
          adatas = {
              "study1": sc.read_h5ad("study1.h5ad"),
              "study2": sc.read_h5ad("study2.h5ad"),
              "study3": sc.read_h5ad("study3.h5ad")
          }

          adata_integrated, model = integrate_datasets(
              adatas,
              labels_key="cell_type"
          )

          sc.pl.umap(adata_integrated, color=["batch", "leiden", "cell_type"])
          ```

          ## Troubleshooting

          | Issue | Cause | Solution |
          |-------|-------|----------|
          | Batches not mixing | Too few shared genes | Use more HVGs, check gene overlap |
          | Over-correction | Biological variation removed | Use scANVI with labels |
          | Training diverges | Learning rate too high | Reduce lr, increase batch_size |
          | NaN loss | Bad data | Check for all-zero cells/genes |
          | Memory error | Too many cells | Reduce batch_size, use GPU |
    - path: /knowledge-work-plugins/skills/scvi-tools/references/spatial_deconvolution.md
      text: |
          # Spatial Transcriptomics Analysis

          This reference covers spatial transcriptomics analysis using scvi-tools methods: DestVI for deconvolution and resolVI for building spatial models.

          ## Overview

          Spatial transcriptomics technologies like Visium capture gene expression at defined spatial locations, but many platforms have multi-cellular resolution. scvi-tools provides two main approaches:

          - **DestVI**: Deconvolution - estimates cell type proportions at each spot using a single-cell reference
          - **resolVI**: Builds a spatial model that learns gene expression patterns accounting for spatial context

          ## Available Methods in scvi-tools

          | Method | Description | Use Case |
          |--------|-------------|----------|
          | **DestVI** | Variational inference for deconvolution | Estimate cell type proportions per spot |
          | **resolVI** | Spatial gene expression model | Learn spatially-aware representations |
          | **CondSCVI** | Reference model for DestVI | Required for DestVI workflow |

          ## Prerequisites

          ```python
          import scvi
          import scanpy as sc
          import squidpy as sq
          import numpy as np

          print(f"scvi-tools version: {scvi.__version__}")
          ```

          ---

          ## Part 1: DestVI Deconvolution

          ### Step 1: Load Spatial Data

          ```python
          # Load Visium data
          adata_spatial = sc.read_visium("spaceranger_output/")

          # Check structure
          print(f"Spots: {adata_spatial.n_obs}")
          print(f"Genes: {adata_spatial.n_vars}")
          print(f"Spatial coordinates: {adata_spatial.obsm['spatial'].shape}")

          # Basic QC
          sc.pp.calculate_qc_metrics(adata_spatial, inplace=True)
          adata_spatial = adata_spatial[adata_spatial.obs['n_genes_by_counts'] > 200].copy()

          # Store counts
          adata_spatial.layers["counts"] = adata_spatial.X.copy()
          ```

          ### Step 2: Load Single-Cell Reference

          ```python
          # Load reference single-cell data
          adata_sc = sc.read_h5ad("reference_scrna.h5ad")

          # Requirements:
          # - Raw counts
          # - Cell type annotations
          print(f"Reference cells: {adata_sc.n_obs}")
          print(f"Cell types: {adata_sc.obs['cell_type'].nunique()}")
          print(adata_sc.obs['cell_type'].value_counts())

          # Store counts
          adata_sc.layers["counts"] = adata_sc.X.copy()
          ```

          ### Step 3: Prepare Data

          ```python
          # DestVI requires gene overlap between reference and spatial
          common_genes = adata_sc.var_names.intersection(adata_spatial.var_names)
          print(f"Common genes: {len(common_genes)}")

          adata_sc = adata_sc[:, common_genes].copy()
          adata_spatial = adata_spatial[:, common_genes].copy()
          ```

          ### Step 4: Train Reference Model (CondSCVI)

          ```python
          # Train conditional scVI on reference data
          scvi.model.CondSCVI.setup_anndata(
              adata_sc,
              layer="counts",
              labels_key="cell_type"
          )

          sc_model = scvi.model.CondSCVI(
              adata_sc,
              n_latent=20
          )

          sc_model.train(max_epochs=200)
          sc_model.history['elbo_train'].plot()
          ```

          ### Step 5: Train DestVI

          ```python
          # Setup spatial data
          scvi.model.DestVI.setup_anndata(
              adata_spatial,
              layer="counts"
          )

          # Train DestVI using reference model
          spatial_model = scvi.model.DestVI.from_rna_model(
              adata_spatial,
              sc_model
          )

          spatial_model.train(max_epochs=500)
          ```

          ### Step 6: Get Cell Type Proportions

          ```python
          # Infer cell type proportions at each spot
          proportions = spatial_model.get_proportions()

          # Add to adata
          for ct in adata_sc.obs['cell_type'].unique():
              adata_spatial.obs[f'prop_{ct}'] = proportions[ct]

          # Visualize
          sq.pl.spatial_scatter(
              adata_spatial,
              color=[f'prop_{ct}' for ct in adata_sc.obs['cell_type'].unique()[:6]],
              ncols=3
          )
          ```

          ---

          ## Part 2: resolVI Spatial Model

          resolVI is a semi-supervised method that learns cell type assignments and spatially-aware representations directly from spatial data, optionally using initial cell type predictions.

          **Note**: resolVI is in `scvi.external` (not `scvi.model`).

          ### Step 1: Prepare Spatial Data

          ```python
          # Load and preprocess
          adata = sc.read_visium("spaceranger_output/")

          # QC
          sc.pp.calculate_qc_metrics(adata, inplace=True)
          adata = adata[adata.obs['n_genes_by_counts'] > 200].copy()

          # Store counts
          adata.layers["counts"] = adata.X.copy()

          # HVG selection
          sc.pp.highly_variable_genes(
              adata,
              n_top_genes=4000,
              flavor="seurat_v3",
              layer="counts"
          )
          adata = adata[:, adata.var['highly_variable']].copy()

          # Optional: Get initial cell type predictions (e.g., from a reference)
          # adata.obs["predicted_celltype"] = ...
          ```

          ### Step 2: Setup and Train resolVI

          ```python
          # Setup for resolVI (note: scvi.external, not scvi.model)
          scvi.external.RESOLVI.setup_anndata(
              adata,
              labels_key="predicted_celltype",  # Initial cell type predictions
              layer="counts"
          )

          # Create model (semisupervised=True uses the labels)
          model = scvi.external.RESOLVI(adata, semisupervised=True)

          # Train
          model.train(max_epochs=50)
          ```

          ### Step 3: Get Cell Type Predictions

          ```python
          # Get refined cell type predictions
          # soft=True returns probabilities, soft=False returns labels
          cell_type_probs = model.predict(adata, num_samples=3, soft=True)
          cell_type_labels = model.predict(adata, num_samples=3, soft=False)

          adata.obs["resolvi_celltype"] = cell_type_labels

          # Visualize
          sq.pl.spatial_scatter(adata, color="resolvi_celltype")
          ```

          ### Step 4: Get Latent Representation

          ```python
          # Get latent representation
          adata.obsm["X_resolVI"] = model.get_latent_representation(adata)

          # Cluster based on spatial representation
          sc.pp.neighbors(adata, use_rep="X_resolVI")
          sc.tl.umap(adata)
          sc.tl.leiden(adata, resolution=0.5)

          # Visualize clusters spatially
          sq.pl.spatial_scatter(adata, color="leiden")
          ```

          ### Step 5: Differential Expression

          ```python
          # DE between cell types using resolVI
          de_results = model.differential_expression(
              adata,
              groupby="resolvi_celltype",
              group1="T_cell",
              group2="Tumor"
          )

          print(de_results.head(20))
          ```

          ### Step 6: Niche Abundance Analysis

          ```python
          # Analyze how cell type neighborhoods differ between conditions
          # Requires spatial neighbor graph
          sq.gr.spatial_neighbors(adata, coord_type="generic")

          niche_results = model.differential_niche_abundance(
              groupby="resolvi_celltype",
              group1="T_cell",
              group2="Tumor",
              neighbor_key="spatial_neighbors"
          )
          ```

          ### Step 7: Query Mapping (Transfer to New Data)

          ```python
          # Map new spatial data to trained model
          query_adata = sc.read_visium("new_sample/")
          query_adata.layers["counts"] = query_adata.X.copy()

          # Prepare and load query
          model.prepare_query_anndata(query_adata, reference_model=model)
          query_model = model.load_query_data(query_adata, reference_model=model)

          # Fine-tune on query
          query_model.train(max_epochs=20)

          # Get predictions for query
          query_labels = query_model.predict(query_adata, num_samples=3, soft=False)
          ```

          ---

          ## Visualization

          ### Spatial Proportions

          ```python
          import matplotlib.pyplot as plt

          # Plot multiple cell type proportions
          cell_types = ['T_cell', 'Tumor', 'Fibroblast', 'Macrophage']
          fig, axes = plt.subplots(2, 2, figsize=(12, 12))

          for ax, ct in zip(axes.flat, cell_types):
              sq.pl.spatial_scatter(
                  adata_spatial,
                  color=f'prop_{ct}',
                  ax=ax,
                  title=ct,
                  show=False
              )

          plt.tight_layout()
          ```

          ### Enrichment by Region

          ```python
          # Cluster spatial data
          sc.pp.neighbors(adata_spatial)
          sc.tl.leiden(adata_spatial, resolution=0.5)

          # Compare proportions across regions
          import pandas as pd

          cell_types = adata_sc.obs['cell_type'].unique()
          prop_cols = [f'prop_{ct}' for ct in cell_types]
          region_props = adata_spatial.obs.groupby('leiden')[prop_cols].mean()
          print(region_props)

          # Heatmap
          import seaborn as sns
          plt.figure(figsize=(10, 6))
          sns.heatmap(region_props.T, annot=True, cmap='viridis')
          plt.title('Cell Type Proportions by Region')
          ```

          ### Spatial Cell Type Interactions

          ```python
          # Neighborhood enrichment using cell type assignments
          sq.gr.spatial_neighbors(adata_spatial)

          # Create "dominant cell type" annotation
          prop_cols = [f'prop_{ct}' for ct in cell_types]
          adata_spatial.obs['dominant_type'] = adata_spatial.obs[prop_cols].idxmax(axis=1)
          adata_spatial.obs['dominant_type'] = adata_spatial.obs['dominant_type'].str.replace('prop_', '')

          # Co-occurrence analysis
          sq.gr.co_occurrence(adata_spatial, cluster_key='dominant_type')
          sq.pl.co_occurrence(adata_spatial, cluster_key='dominant_type')
          ```

          ---

          ## Complete DestVI Pipeline

          ```python
          def deconvolve_spatial(
              adata_spatial,
              adata_ref,
              cell_type_key="cell_type",
              n_latent=20,
              max_epochs_ref=200,
              max_epochs_spatial=500
          ):
              """
              Perform spatial deconvolution using DestVI.

              Parameters
              ----------
              adata_spatial : AnnData
                  Spatial transcriptomics data
              adata_ref : AnnData
                  Single-cell reference with cell type annotations
              cell_type_key : str
                  Column in adata_ref.obs with cell type labels
              n_latent : int
                  Latent dimensions
              max_epochs_ref : int
                  Training epochs for reference model
              max_epochs_spatial : int
                  Training epochs for spatial model

              Returns
              -------
              AnnData with cell type proportions in obs
              """
              import scvi

              # Get common genes
              common_genes = adata_ref.var_names.intersection(adata_spatial.var_names)
              adata_ref = adata_ref[:, common_genes].copy()
              adata_spatial = adata_spatial[:, common_genes].copy()

              # Ensure counts are stored
              if "counts" not in adata_ref.layers:
                  adata_ref.layers["counts"] = adata_ref.X.copy()
              if "counts" not in adata_spatial.layers:
                  adata_spatial.layers["counts"] = adata_spatial.X.copy()

              # Train reference model
              scvi.model.CondSCVI.setup_anndata(
                  adata_ref,
                  layer="counts",
                  labels_key=cell_type_key
              )

              ref_model = scvi.model.CondSCVI(adata_ref, n_latent=n_latent)
              ref_model.train(max_epochs=max_epochs_ref)

              # Train spatial model
              scvi.model.DestVI.setup_anndata(adata_spatial, layer="counts")

              spatial_model = scvi.model.DestVI.from_rna_model(
                  adata_spatial,
                  ref_model
              )
              spatial_model.train(max_epochs=max_epochs_spatial)

              # Get proportions
              proportions = spatial_model.get_proportions()

              cell_types = adata_ref.obs[cell_type_key].unique()
              for ct in cell_types:
                  adata_spatial.obs[f'prop_{ct}'] = proportions[ct]

              # Add dominant type
              prop_cols = [f'prop_{ct}' for ct in cell_types]
              adata_spatial.obs['dominant_type'] = adata_spatial.obs[prop_cols].idxmax(axis=1)
              adata_spatial.obs['dominant_type'] = adata_spatial.obs['dominant_type'].str.replace('prop_', '')

              return adata_spatial, ref_model, spatial_model

          # Usage
          adata_spatial, ref_model, spatial_model = deconvolve_spatial(
              adata_spatial,
              adata_sc,
              cell_type_key="cell_type"
          )

          # Visualize
          sq.pl.spatial_scatter(
              adata_spatial,
              color=['dominant_type', 'prop_T_cell', 'prop_Tumor'],
              ncols=3
          )
          ```

          ---

          ## Troubleshooting

          | Issue | Cause | Solution |
          |-------|-------|----------|
          | Few common genes | Different gene naming | Convert gene names (Ensembl ↔ Symbol) |
          | Poor deconvolution | Reference doesn't match | Use tissue-matched reference |
          | All spots same type | Over-smoothing | Adjust model parameters, check reference diversity |
          | NaN proportions | Missing cell types | Ensure all expected types in reference |
          | Training slow | Large spatial dataset | Reduce max_epochs, increase batch_size |

          ## Key References

          - Lopez et al. (2022) "DestVI identifies continuums of cell types in spatial transcriptomics data"
          - [scvi-tools spatial tutorials](https://docs.scvi-tools.org/en/stable/tutorials/index.html)
    - path: /knowledge-work-plugins/skills/scvi-tools/references/troubleshooting.md
      text: |
          # Troubleshooting Guide for scvi-tools

          This reference provides a consolidated guide for diagnosing and resolving common issues across all scvi-tools models.

          ## Quick Diagnosis

          | Symptom | Likely Cause | Quick Fix |
          |---------|--------------|-----------|
          | "X should contain integers" | Normalized data in X | Use `layer="counts"` in setup |
          | CUDA out of memory | GPU memory exhausted | Reduce `batch_size`, use smaller model |
          | Training loss is NaN | Bad data or learning rate | Check for all-zero cells/genes |
          | Batches not mixing | Too few shared features | Increase HVGs, check gene overlap |
          | Over-correction | Too aggressive integration | Use scANVI with labels |
          | Import error | Missing dependencies | `pip install scvi-tools[all]` |

          ## Data Format Issues

          ### Issue: CITE-seq protein data from Seurat is CLR-normalized

          **Cause**: Seurat's `NormalizeData(normalization.method = "CLR")` transforms raw ADT counts. totalVI requires raw integer counts for protein data.

          **Symptoms**:
          - Protein values are not integers
          - Protein values contain negative numbers
          - Model training produces poor results

          **Solution**:
          ```python
          # Check if protein data is normalized
          protein = adata.obsm["protein_expression"]
          print(f"Min value: {protein.min()}")  # Should be 0 if raw counts
          print(f"Contains integers: {np.allclose(protein, protein.astype(int))}")

          # If importing from Seurat, use the raw counts assay, not the normalized one
          # In R/Seurat, export the RNA assay's counts slot, not the data slot
          # GetAssayData(seurat_obj, assay = "ADT", slot = "counts")
          ```

          ### Issue: "layer not found" or "X should contain integers"

          **Cause**: scvi-tools requires raw integer counts, not normalized data.

          **Solution**:
          ```python
          # Check if X contains integers
          import numpy as np
          print(f"X max: {adata.X.max()}")
          print(f"Contains integers: {np.allclose(adata.X.data, adata.X.data.astype(int))}")

          # If normalized, recover from raw
          if hasattr(adata, 'raw') and adata.raw is not None:
              adata = adata.raw.to_adata()

          # Or use existing counts layer
          adata.layers["counts"] = adata.X.copy()
          scvi.model.SCVI.setup_anndata(adata, layer="counts")
          ```

          ### Issue: Sparse matrix errors

          **Cause**: Incompatible sparse format or dense array expected.

          **Solution**:
          ```python
          from scipy.sparse import csr_matrix

          # Convert to CSR format (most compatible)
          if hasattr(adata.X, 'toarray'):
              adata.X = csr_matrix(adata.X)

          # Or convert to dense if small enough
          if adata.n_obs * adata.n_vars < 1e8:
              adata.X = adata.X.toarray()
          ```

          ### Issue: NaN or Inf values in data

          **Cause**: Missing values or corrupted data.

          **Solution**:
          ```python
          import numpy as np

          # Check for issues
          X = adata.X.toarray() if hasattr(adata.X, 'toarray') else adata.X
          print(f"NaN count: {np.isnan(X).sum()}")
          print(f"Inf count: {np.isinf(X).sum()}")
          print(f"Negative count: {(X < 0).sum()}")

          # Replace NaN/Inf with 0
          X = np.nan_to_num(X, nan=0, posinf=0, neginf=0)
          X = np.clip(X, 0, None)  # Ensure non-negative
          adata.X = csr_matrix(X)
          ```

          ### Issue: batch_key or labels_key not found

          **Cause**: Column name mismatch in adata.obs.

          **Solution**:
          ```python
          # List available columns
          print(adata.obs.columns.tolist())

          # Check for similar names
          for col in adata.obs.columns:
              if 'batch' in col.lower() or 'sample' in col.lower():
                  print(f"Potential batch column: {col}")
          ```

          ## GPU and Memory Issues

          ### Issue: CUDA out of memory

          **Cause**: Model or batch doesn't fit in GPU memory.

          **Solutions** (try in order):

          ```python
          # 1. Reduce batch size
          model.train(batch_size=64)  # Default is 128

          # 2. Use smaller model architecture
          model = scvi.model.SCVI(
              adata,
              n_latent=10,   # Default is 10-30
              n_layers=1     # Default is 1-2
          )

          # 3. Subset to fewer genes
          sc.pp.highly_variable_genes(adata, n_top_genes=1500)
          adata = adata[:, adata.var['highly_variable']].copy()

          # 4. Clear GPU cache between models
          import torch
          torch.cuda.empty_cache()

          # 5. Use CPU if GPU is too small
          model.train(accelerator="cpu")
          ```

          ### Issue: No GPU detected

          **Cause**: CUDA not installed or version mismatch.

          **Diagnosis**:
          ```python
          import torch
          print(f"CUDA available: {torch.cuda.is_available()}")
          print(f"PyTorch version: {torch.__version__}")
          print(f"CUDA version: {torch.version.cuda}")
          ```

          **Solution**:
          ```bash
          # Check system CUDA
          nvidia-smi
          nvcc --version

          # Reinstall PyTorch with matching CUDA
          pip install torch --index-url https://download.pytorch.org/whl/cu118  # For CUDA 11.8
          # Or
          pip install torch --index-url https://download.pytorch.org/whl/cu121  # For CUDA 12.1
          ```

          ### Issue: Memory error with large datasets

          **Cause**: Dataset too large for system RAM.

          **Solutions**:
          ```python
          # 1. Process in chunks (for very large data)
          # Subsample for initial exploration
          adata_sample = adata[np.random.choice(adata.n_obs, 50000, replace=False)].copy()

          # 2. Use backed mode for AnnData
          adata = sc.read_h5ad("large_data.h5ad", backed='r')

          # 3. Reduce gene count aggressively
          adata = adata[:, adata.var['highly_variable']].copy()
          ```

          ## Training Issues

          ### Issue: Training loss is NaN

          **Cause**: Numerical instability, bad data, or learning rate issues.

          **Solutions**:
          ```python
          # 1. Check for problematic cells/genes
          sc.pp.filter_cells(adata, min_genes=200)
          sc.pp.filter_genes(adata, min_cells=3)

          # 2. Remove cells with zero counts
          adata = adata[adata.X.sum(axis=1) > 0].copy()

          # 3. Use gradient clipping (built into scvi-tools)
          model.train(max_epochs=200, early_stopping=True)
          ```

          ### Issue: Training doesn't converge

          **Cause**: Insufficient epochs, poor hyperparameters, or data issues.

          **Solutions**:
          ```python
          # 1. Train longer
          model.train(max_epochs=400)

          # 2. Check training curves
          import matplotlib.pyplot as plt
          plt.plot(model.history['elbo_train'])
          plt.plot(model.history['elbo_validation'])
          plt.xlabel('Epoch')
          plt.ylabel('ELBO')
          plt.legend(['Train', 'Validation'])

          # 3. Adjust model size for data size
          # Small data (<10k cells): smaller model
          model = scvi.model.SCVI(adata, n_latent=10, n_layers=1, dropout_rate=0.2)

          # Large data (>100k cells): can use larger model
          model = scvi.model.SCVI(adata, n_latent=30, n_layers=2)
          ```

          ### Issue: Overfitting (validation loss increases)

          **Cause**: Model too complex or trained too long.

          **Solutions**:
          ```python
          # 1. Enable early stopping
          model.train(early_stopping=True, early_stopping_patience=10)

          # 2. Add regularization
          model = scvi.model.SCVI(adata, dropout_rate=0.2)

          # 3. Reduce model complexity
          model = scvi.model.SCVI(adata, n_layers=1)
          ```

          ## Integration Issues

          ### Issue: Batches don't mix

          **Cause**: Too few shared features, strong biological differences, or technical issues.

          **Solutions**:
          ```python
          # 1. Check gene overlap between batches
          for batch in adata.obs['batch'].unique():
              batch_genes = adata[adata.obs['batch'] == batch].var_names
              print(f"{batch}: {len(batch_genes)} genes")

          # 2. Use more HVGs
          sc.pp.highly_variable_genes(adata, n_top_genes=4000, batch_key="batch")

          # 3. Train longer
          model.train(max_epochs=400)

          # 4. Increase latent dimensions
          model = scvi.model.SCVI(adata, n_latent=50)
          ```

          ### Issue: Over-correction (biological signal lost)

          **Cause**: Model removes too much variation.

          **Solutions**:
          ```python
          # 1. Use scANVI with cell type labels
          scvi.model.SCANVI.from_scvi_model(scvi_model, labels_key="cell_type")

          # 2. Reduce model capacity
          model = scvi.model.SCVI(adata, n_latent=10)

          # 3. Use categorical covariates instead of batch_key
          scvi.model.SCVI.setup_anndata(
              adata,
              layer="counts",
              categorical_covariate_keys=["batch"]  # Less aggressive than batch_key
          )
          ```

          ### Issue: One batch dominates clusters

          **Cause**: Unbalanced batch sizes or incomplete integration.

          **Solutions**:
          ```python
          # 1. Check batch distribution
          print(adata.obs['batch'].value_counts())

          # 2. Subsample to balance
          from sklearn.utils import resample
          balanced = []
          min_size = adata.obs['batch'].value_counts().min()
          for batch in adata.obs['batch'].unique():
              batch_data = adata[adata.obs['batch'] == batch]
              balanced.append(batch_data[np.random.choice(len(batch_data), min_size, replace=False)])
          adata_balanced = sc.concat(balanced)
          ```

          ## Model-Specific Issues

          ### scANVI: Poor label transfer

          **Solutions**:
          ```python
          # 1. Check label distribution
          print(adata.obs['cell_type'].value_counts())

          # 2. Use Unknown for low-confidence cells
          adata.obs.loc[adata.obs['prediction_score'] < 0.5, 'cell_type'] = 'Unknown'

          # 3. Train scVI longer before scANVI
          scvi_model.train(max_epochs=300)
          scanvi_model = scvi.model.SCANVI.from_scvi_model(scvi_model, labels_key="cell_type")
          scanvi_model.train(max_epochs=100)
          ```

          ### totalVI: Noisy protein signal

          **Solutions**:
          ```python
          # 1. Use denoised protein values
          _, protein_denoised = model.get_normalized_expression(return_mean=True)

          # 2. Check isotype controls
          # Isotype controls should have low expression
          for i, name in enumerate(adata.uns["protein_names"]):
              if 'isotype' in name.lower():
                  print(f"{name}: mean={adata.obsm['protein_expression'][:, i].mean():.1f}")
          ```

          ### PeakVI: Poor clustering

          **Solutions**:
          ```python
          # 1. Use more variable peaks
          from sklearn.feature_selection import VarianceThreshold
          selector = VarianceThreshold(threshold=0.05)
          adata = adata[:, selector.fit(adata.X).get_support()].copy()

          # 2. Binarize data
          adata.X = (adata.X > 0).astype(np.float32)
          ```

          ### MultiVI: Different cell counts between modalities

          **Solutions**:
          ```python
          # Ensure same cells in same order
          common_cells = adata_rna.obs_names.intersection(adata_atac.obs_names)
          adata_rna = adata_rna[common_cells].copy()
          adata_atac = adata_atac[common_cells].copy()
          ```

          ### DestVI: Poor deconvolution

          **Solutions**:
          ```python
          # 1. Check gene overlap
          common_genes = adata_ref.var_names.intersection(adata_spatial.var_names)
          print(f"Common genes: {len(common_genes)}")  # Should be >1000

          # 2. Use tissue-matched reference
          # Reference should contain all cell types expected in spatial data

          # 3. Check reference quality
          print(adata_ref.obs['cell_type'].value_counts())
          ```

          ## Version Compatibility

          ### scvi-tools 1.x vs 0.x API changes

          Key differences:
          ```python
          # 0.x API
          scvi.data.setup_anndata(adata, ...)

          # 1.x API (current)
          scvi.model.SCVI.setup_anndata(adata, ...)
          ```

          ### Check versions
          ```python
          import scvi
          import scanpy as sc
          import anndata
          import torch

          print(f"scvi-tools: {scvi.__version__}")
          print(f"scanpy: {sc.__version__}")
          print(f"anndata: {anndata.__version__}")
          print(f"torch: {torch.__version__}")
          ```

          ### Recommended versions (as of late 2024)
          ```
          scvi-tools>=1.0.0
          scanpy>=1.9.0
          anndata>=0.9.0
          torch>=2.0.0
          ```

          ## Getting Help

          1. **Check documentation**: https://docs.scvi-tools.org/
          2. **GitHub issues**: https://github.com/scverse/scvi-tools/issues
          3. **Discourse forum**: https://discourse.scverse.org/
          4. **Tutorials**: https://docs.scvi-tools.org/en/stable/tutorials/index.html

          When reporting issues, include:
          - scvi-tools version (`scvi.__version__`)
          - Python version
          - Full error traceback
          - Minimal reproducible example
    - path: /knowledge-work-plugins/skills/single-cell-rna-qc/LICENSE.txt
      text: |
          Apache License
          Version 2.0, January 2004
          http://www.apache.org/licenses/

          TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

          1. Definitions.

          "License" shall mean the terms and conditions for use, reproduction,
          and distribution as defined by Sections 1 through 9 of this document.

          "Licensor" shall mean the copyright owner or entity authorized by
          the copyright owner that is granting the License.

          "Legal Entity" shall mean the union of the acting entity and all
          other entities that control, are controlled by, or are under common
          control with that entity. For the purposes of this definition,
          "control" means (i) the power, direct or indirect, to cause the
          direction or management of such entity, whether by contract or
          otherwise, or (ii) ownership of fifty percent (50%) or more of the
          outstanding shares, or (iii) beneficial ownership of such entity.

          "You" (or "Your") shall mean an individual or Legal Entity
          exercising permissions granted by this License.

          "Source" form shall mean the preferred form for making modifications,
          including but not limited to software source code, documentation
          source, and configuration files.

          "Object" form shall mean any form resulting from mechanical
          transformation or translation of a Source form, including but
          not limited to compiled object code, generated documentation,
          and conversions to other media types.

          "Work" shall mean the work of authorship, whether in Source or
          Object form, made available under the License, as indicated by a
          copyright notice that is included in or attached to the work
          (an example is provided in the Appendix below).

          "Derivative Works" shall mean any work, whether in Source or Object
          form, that is based on (or derived from) the Work and for which the
          editorial revisions, annotations, elaborations, or other modifications
          represent, as a whole, an original work of authorship. For the purposes
          of this License, Derivative Works shall not include works that remain
          separable from, or merely link (or bind by name) to the interfaces of,
          the Work and Derivative Works thereof.

          "Contribution" shall mean any work of authorship, including
          the original version of the Work and any modifications or additions
          to that Work or Derivative Works thereof, that is intentionally
          submitted to Licensor for inclusion in the Work by the copyright owner
          or by an individual or Legal Entity authorized to submit on behalf of
          the copyright owner. For the purposes of this definition, "submitted"
          means any form of electronic, verbal, or written communication sent
          to the Licensor or its representatives, including but not limited to
          communication on electronic mailing lists, source code control systems,
          and issue tracking systems that are managed by, or on behalf of, the
          Licensor for the purpose of discussing and improving the Work, but
          excluding communication that is conspicuously marked or otherwise
          designated in writing by the copyright owner as "Not a Contribution."

          "Contributor" shall mean Licensor and any individual or Legal Entity
          on behalf of whom a Contribution has been received by Licensor and
          subsequently incorporated within the Work.

          2. Grant of Copyright License. Subject to the terms and conditions of
          this License, each Contributor hereby grants to You a perpetual,
          worldwide, non-exclusive, no-charge, royalty-free, irrevocable
          copyright license to reproduce, prepare Derivative Works of,
          publicly display, publicly perform, sublicense, and distribute the
          Work and such Derivative Works in Source or Object form.

          3. Grant of Patent License. Subject to the terms and conditions of
          this License, each Contributor hereby grants to You a perpetual,
          worldwide, non-exclusive, no-charge, royalty-free, irrevocable
          (except as stated in this section) patent license to make, have made,
          use, offer to sell, sell, import, and otherwise transfer the Work,
          where such license applies only to those patent claims licensable
          by such Contributor that are necessarily infringed by their
          Contribution(s) alone or by combination of their Contribution(s)
          with the Work to which such Contribution(s) was submitted. If You
          institute patent litigation against any entity (including a
          cross-claim or counterclaim in a lawsuit) alleging that the Work
          or a Contribution incorporated within the Work constitutes direct
          or contributory patent infringement, then any patent licenses
          granted to You under this License for that Work shall terminate
          as of the date such litigation is filed.

          4. Redistribution. You may reproduce and distribute copies of the
          Work or Derivative Works thereof in any medium, with or without
          modifications, and in Source or Object form, provided that You
          meet the following conditions:

          (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

          (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

          (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

          (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

          You may add Your own copyright statement to Your modifications and
          may provide additional or different license terms and conditions
          for use, reproduction, or distribution of Your modifications, or
          for any such Derivative Works as a whole, provided Your use,
          reproduction, and distribution of the Work otherwise complies with
          the conditions stated in this License.

          5. Submission of Contributions. Unless You explicitly state otherwise,
          any Contribution intentionally submitted for inclusion in the Work
          by You to the Licensor shall be under the terms and conditions of
          this License, without any additional terms or conditions.
          Notwithstanding the above, nothing herein shall supersede or modify
          the terms of any separate license agreement you may have executed
          with Licensor regarding such Contributions.

          6. Trademarks. This License does not grant permission to use the trade
          names, trademarks, service marks, or product names of the Licensor,
          except as required for reasonable and customary use in describing the
          origin of the Work and reproducing the content of the NOTICE file.

          7. Disclaimer of Warranty. Unless required by applicable law or
          agreed to in writing, Licensor provides the Work (and each
          Contributor provides its Contributions) on an "AS IS" BASIS,
          WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
          implied, including, without limitation, any warranties or conditions
          of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
          PARTICULAR PURPOSE. You are solely responsible for determining the
          appropriateness of using or redistributing the Work and assume any
          risks associated with Your exercise of permissions under this License.

          8. Limitation of Liability. In no event and under no legal theory,
          whether in tort (including negligence), contract, or otherwise,
          unless required by applicable law (such as deliberate and grossly
          negligent acts) or agreed to in writing, shall any Contributor be
          liable to You for damages, including any direct, indirect, special,
          incidental, or consequential damages of any character arising as a
          result of this License or out of the use or inability to use the
          Work (including but not limited to damages for loss of goodwill,
          work stoppage, computer failure or malfunction, or any and all
          other commercial damages or losses), even if such Contributor
          has been advised of the possibility of such damages.

          9. Accepting Warranty or Additional Liability. While redistributing
          the Work or Derivative Works thereof, You may choose to offer,
          and charge a fee for, acceptance of support, warranty, indemnity,
          or other liability obligations and/or rights consistent with this
          License. However, in accepting such obligations, You may act only
          on Your own behalf and on Your sole responsibility, not on behalf
          of any other Contributor, and only if You agree to indemnify,
          defend, and hold each Contributor harmless for any liability
          incurred by, or claims asserted against, such Contributor by reason
          of your accepting any such warranty or additional liability.

          END OF TERMS AND CONDITIONS

          APPENDIX: How to apply the Apache License to your work.

          To apply the Apache License to your work, attach the following
          boilerplate notice, with the fields enclosed by brackets "[]"
          replaced with your own identifying information. (Don't include
          the brackets!) The text should be enclosed in the appropriate
          comment syntax for the file format. We also recommend that a
          file or class name and description of purpose be included on the
          same "printed page" as the copyright notice for easier
          identification within third-party archives.

          Copyright [yyyy] [name of copyright owner]

          Licensed under the Apache License, Version 2.0 (the "License");
          you may not use this file except in compliance with the License.
          You may obtain a copy of the License at

          http://www.apache.org/licenses/LICENSE-2.0

          Unless required by applicable law or agreed to in writing, software
          distributed under the License is distributed on an "AS IS" BASIS,
          WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
          See the License for the specific language governing permissions and
          limitations under the License.
    - path: /knowledge-work-plugins/skills/single-cell-rna-qc/SKILL.md
      text: |
          ---
          name: single-cell-rna-qc
          description: Performs quality control on single-cell RNA-seq data (.h5ad or .h5 files) using scverse best practices with MAD-based filtering and comprehensive visualizations. Use when users request QC analysis, filtering low-quality cells, assessing data quality, or following scverse/scanpy best practices for single-cell analysis.
          ---

          # Single-Cell RNA-seq Quality Control

          Automated QC workflow for single-cell RNA-seq data following scverse best practices.

          ## When to Use This Skill

          Use when users:
          - Request quality control or QC on single-cell RNA-seq data
          - Want to filter low-quality cells or assess data quality
          - Need QC visualizations or metrics
          - Ask to follow scverse/scanpy best practices
          - Request MAD-based filtering or outlier detection

          **Supported input formats:**
          - `.h5ad` files (AnnData format from scanpy/Python workflows)
          - `.h5` files (10X Genomics Cell Ranger output)

          **Default recommendation**: Use Approach 1 (complete pipeline) unless the user has specific custom requirements or explicitly requests non-standard filtering logic.

          ## Approach 1: Complete QC Pipeline (Recommended for Standard Workflows)

          For standard QC following scverse best practices, use the convenience script `scripts/qc_analysis.py`:

          ```bash
          python3 scripts/qc_analysis.py input.h5ad
          # or for 10X Genomics .h5 files:
          python3 scripts/qc_analysis.py raw_feature_bc_matrix.h5
          ```

          The script automatically detects the file format and loads it appropriately.

          **When to use this approach:**
          - Standard QC workflow with adjustable thresholds (all cells filtered the same way)
          - Batch processing multiple datasets
          - Quick exploratory analysis
          - User wants the "just works" solution

          **Requirements:** anndata, scanpy, scipy, matplotlib, seaborn, numpy

          **Parameters:**

          Customize filtering thresholds and gene patterns using command-line parameters:
          - `--output-dir` - Output directory
          - `--mad-counts`, `--mad-genes`, `--mad-mt` - MAD thresholds for counts/genes/MT%
          - `--mt-threshold` - Hard mitochondrial % cutoff
          - `--min-cells` - Gene filtering threshold
          - `--mt-pattern`, `--ribo-pattern`, `--hb-pattern` - Gene name patterns for different species

          Use `--help` to see current default values.

          **Outputs:**

          All files are saved to `<input_basename>_qc_results/` directory by default (or to the directory specified by `--output-dir`):
          - `qc_metrics_before_filtering.png` - Pre-filtering visualizations
          - `qc_filtering_thresholds.png` - MAD-based threshold overlays
          - `qc_metrics_after_filtering.png` - Post-filtering quality metrics
          - `<input_basename>_filtered.h5ad` - Clean, filtered dataset ready for downstream analysis
          - `<input_basename>_with_qc.h5ad` - Original data with QC annotations preserved

          If copying outputs for user access, copy individual files (not the entire directory) so users can preview them directly.

          ### Workflow Steps

          The script performs the following steps:

          1. **Calculate QC metrics** - Count depth, gene detection, mitochondrial/ribosomal/hemoglobin content
          2. **Apply MAD-based filtering** - Permissive outlier detection using MAD thresholds for counts/genes/MT%
          3. **Filter genes** - Remove genes detected in few cells
          4. **Generate visualizations** - Comprehensive before/after plots with threshold overlays

          ## Approach 2: Modular Building Blocks (For Custom Workflows)

          For custom analysis workflows or non-standard requirements, use the modular utility functions from `scripts/qc_core.py` and `scripts/qc_plotting.py`:

          ```python
          # Run from scripts/ directory, or add scripts/ to sys.path if needed
          import anndata as ad
          from qc_core import calculate_qc_metrics, detect_outliers_mad, filter_cells
          from qc_plotting import plot_qc_distributions  # Only if visualization needed

          adata = ad.read_h5ad('input.h5ad')
          calculate_qc_metrics(adata, inplace=True)
          # ... custom analysis logic here
          ```

          **When to use this approach:**
          - Different workflow needed (skip steps, change order, apply different thresholds to subsets)
          - Conditional logic (e.g., filter neurons differently than other cells)
          - Partial execution (only metrics/visualization, no filtering)
          - Integration with other analysis steps in a larger pipeline
          - Custom filtering criteria beyond what command-line params support

          **Available utility functions:**

          From `qc_core.py` (core QC operations):
          - `calculate_qc_metrics(adata, mt_pattern, ribo_pattern, hb_pattern, inplace=True)` - Calculate QC metrics and annotate adata
          - `detect_outliers_mad(adata, metric, n_mads, verbose=True)` - MAD-based outlier detection, returns boolean mask
          - `apply_hard_threshold(adata, metric, threshold, operator='>', verbose=True)` - Apply hard cutoffs, returns boolean mask
          - `filter_cells(adata, mask, inplace=False)` - Apply boolean mask to filter cells
          - `filter_genes(adata, min_cells=20, min_counts=None, inplace=True)` - Filter genes by detection
          - `print_qc_summary(adata, label='')` - Print summary statistics

          From `qc_plotting.py` (visualization):
          - `plot_qc_distributions(adata, output_path, title)` - Generate comprehensive QC plots
          - `plot_filtering_thresholds(adata, outlier_masks, thresholds, output_path)` - Visualize filtering thresholds
          - `plot_qc_after_filtering(adata, output_path)` - Generate post-filtering plots

          **Example custom workflows:**

          **Example 1: Only calculate metrics and visualize, don't filter yet**
          ```python
          adata = ad.read_h5ad('input.h5ad')
          calculate_qc_metrics(adata, inplace=True)
          plot_qc_distributions(adata, 'qc_before.png', title='Initial QC')
          print_qc_summary(adata, label='Before filtering')
          ```

          **Example 2: Apply only MT% filtering, keep other metrics permissive**
          ```python
          adata = ad.read_h5ad('input.h5ad')
          calculate_qc_metrics(adata, inplace=True)

          # Only filter high MT% cells
          high_mt = apply_hard_threshold(adata, 'pct_counts_mt', 10, operator='>')
          adata_filtered = filter_cells(adata, ~high_mt)
          adata_filtered.write('filtered.h5ad')
          ```

          **Example 3: Different thresholds for different subsets**
          ```python
          adata = ad.read_h5ad('input.h5ad')
          calculate_qc_metrics(adata, inplace=True)

          # Apply type-specific QC (assumes cell_type metadata exists)
          neurons = adata.obs['cell_type'] == 'neuron'
          other_cells = ~neurons

          # Neurons tolerate higher MT%, other cells use stricter threshold
          neuron_qc = apply_hard_threshold(adata[neurons], 'pct_counts_mt', 15, operator='>')
          other_qc = apply_hard_threshold(adata[other_cells], 'pct_counts_mt', 8, operator='>')
          ```

          ## Best Practices

          1. **Be permissive with filtering** - Default thresholds intentionally retain most cells to avoid losing rare populations
          2. **Inspect visualizations** - Always review before/after plots to ensure filtering makes biological sense
          3. **Consider dataset-specific factors** - Some tissues naturally have higher mitochondrial content (e.g., neurons, cardiomyocytes)
          4. **Check gene annotations** - Mitochondrial gene prefixes vary by species (mt- for mouse, MT- for human)
          5. **Iterate if needed** - QC parameters may need adjustment based on the specific experiment or tissue type

          ## Reference Materials

          For detailed QC methodology, parameter rationale, and troubleshooting guidance, see `references/scverse_qc_guidelines.md`. This reference provides:
          - Detailed explanations of each QC metric and why it matters
          - Rationale for MAD-based thresholds and why they're better than fixed cutoffs
          - Guidelines for interpreting QC visualizations (histograms, violin plots, scatter plots)
          - Species-specific considerations for gene annotations
          - When and how to adjust filtering parameters
          - Advanced QC considerations (ambient RNA correction, doublet detection)

          Load this reference when users need deeper understanding of the methodology or when troubleshooting QC issues.

          ## Next Steps After QC

          Typical downstream analysis steps:
          - Ambient RNA correction (SoupX, CellBender)
          - Doublet detection (scDblFinder)
          - Normalization (log-normalize, scran)
          - Feature selection and dimensionality reduction
          - Clustering and cell type annotation
    - path: /knowledge-work-plugins/skills/single-cell-rna-qc/references/scverse_qc_guidelines.md
      text: |
          # scverse Quality Control Guidelines

          This document provides detailed information about quality control best practices for single-cell RNA-seq data, following the scverse ecosystem recommendations.

          ## Quality Control Metrics

          ### Count Depth (Total Counts)
          - **What it measures**: Total number of UMI/reads per cell
          - **Why it matters**: Low count cells may be empty droplets, debris, or poorly captured cells
          - **Typical range**: 500-50,000 counts per cell (varies by protocol)
          - **Red flags**: Bimodal distributions may indicate mixing of high and low-quality cells

          ### Gene Detection (Genes per Cell)
          - **What it measures**: Number of genes with at least 1 count
          - **Why it matters**: Strongly correlates with count depth; low values indicate poor capture
          - **Typical range**: 200-5,000 genes per cell
          - **Red flags**: Very low values (<200) suggest technical failures

          ### Mitochondrial Content
          - **What it measures**: Percentage of counts from mitochondrial genes
          - **Why it matters**: High MT% indicates cell stress, apoptosis, or lysed cells
          - **Typical range**: <5% for most tissues, up to 10-15% for metabolically active cells
          - **Species-specific patterns**:
            - Mouse: Genes start with 'mt-' (e.g., mt-Nd1, mt-Co1)
            - Human: Genes start with 'MT-' (e.g., MT-ND1, MT-CO1)
          - **Context matters**: Some cell types (cardiomyocytes, neurons) naturally have higher MT content

          ### Ribosomal Content
          - **What it measures**: Percentage of counts from ribosomal protein genes
          - **Why it matters**: Can indicate cell state or contamination
          - **Patterns**: Genes start with 'Rpl'/'RPL' (large subunit) or 'Rps'/'RPS' (small subunit)
          - **Note**: High ribosomal content isn't always bad - metabolically active cells have more ribosomes

          ### Hemoglobin Content
          - **What it measures**: Percentage of counts from hemoglobin genes
          - **Why it matters**: Indicates blood contamination in non-blood tissues
          - **Patterns**: Genes matching '^Hb[^(p)]' or '^HB[^(P)]' (excludes Hbp1/HBP1)
          - **When to use**: Particularly important for tissue samples (brain, liver, etc.)

          ## MAD-Based Filtering Rationale

          ### Why MAD Instead of Fixed Thresholds?

          Fixed thresholds (e.g., "remove cells with <500 genes") fail because:
          - Different protocols yield different ranges
          - Different tissues have different characteristics
          - Different species have different gene counts
          - Fixed thresholds are arbitrary and not data-driven

          MAD (Median Absolute Deviation) is robust to outliers and adapts to your dataset:
          ```
          MAD = median(|X - median(X)|)
          Outlier bounds = median ± n_MADs × MAD
          ```

          ### Recommended MAD Thresholds

          Following scverse best practices (deliberately permissive):

          **5 MADs for count depth (log-transformed)**
          - Very permissive to retain rare cell populations
          - Catches extreme outliers (empty droplets, debris)
          - Log transformation handles the typical right-skewed distribution

          **5 MADs for gene counts (log-transformed)**
          - Parallels count depth filtering
          - Most informative when combined with count filtering
          - Log transformation normalizes the distribution

          **3 MADs for mitochondrial percentage**
          - More stringent because high MT% strongly indicates dying cells
          - Uses raw percentages (not log-transformed)
          - Combined with hard threshold for extra stringency

          **Hard threshold: 8% mitochondrial content**
          - Additional filter beyond MAD-based detection
          - Conservative cutoff that works across most tissues
          - Adjust higher (10-15%) for metabolically active cell types

          ### Why Be Permissive?

          The default thresholds intentionally err on the side of keeping cells because:
          1. **Rare populations**: Stringent filtering may remove rare but viable cell types
          2. **Biological variation**: Some healthy cells naturally have extreme values
          3. **Reversibility**: Easier to filter more later than to recover lost cells
          4. **Downstream robustness**: Modern normalization methods handle moderate quality variation

          ## Interpreting QC Visualizations

          ### Histograms
          - **Bimodal distributions**: May indicate mixing of cell types or quality issues
          - **Long tails**: Common for count depth; MAD filtering handles this
          - **Sharp cutoffs**: May indicate prior filtering or technical artifacts

          ### Violin Plots
          - Shows distribution shape and density
          - Median (line) and mean (diamond) should be similar for symmetric distributions
          - Wide distributions suggest high heterogeneity

          ### Scatter Plots

          **Counts vs Genes (colored by MT%)**
          - Should show strong positive correlation (R² > 0.8 typical)
          - Points deviating from trend may be outliers
          - High MT% cells often cluster at low counts/genes

          **Counts vs MT%**
          - Negative correlation expected (dying cells have fewer counts)
          - Vertical stratification may indicate batch effects
          - Cells with high counts + high MT% are suspicious

          **Genes vs MT%**
          - Similar to counts vs MT%, but often weaker correlation
          - Useful for identifying cells with gene detection issues

          ## Gene Filtering

          After filtering cells, remove genes detected in fewer than 20 cells:
          - **Why 20?**: Balances noise reduction with information retention
          - **Benefits**: Reduces dataset size, speeds up computation, removes noisy genes
          - **Trade-offs**: May lose very rare markers; adjust to 10 if studying rare populations

          ## Species-Specific Considerations

          ### Mouse (Mus musculus)
          - Mitochondrial genes: mt-* (lowercase)
          - Ribosomal genes: Rpl*, Rps* (capitalized first letter)
          - Hemoglobin genes: Hb* (but not Hbp1)

          ### Human (Homo sapiens)
          - Mitochondrial genes: MT-* (uppercase)
          - Ribosomal genes: RPL*, RPS* (all uppercase)
          - Hemoglobin genes: HB* (but not HBP1)

          ### Other Species
          Adjust gene name patterns in the script to match your organism's gene nomenclature. Consult Ensembl or your reference annotation for correct prefixes.

          ## When to Adjust Parameters

          Consider adjusting filtering thresholds when:

          **More stringent (lower MADs)**
          - High ambient RNA contamination suspected
          - Many low-quality cells observed in visualizations
          - Downstream analysis shows quality-driven clustering

          **More permissive (higher MADs)**
          - Studying rare cell populations
          - Dataset has high technical quality
          - Cell types naturally have extreme values (e.g., neurons with high MT%)

          **Tissue-specific adjustments**
          - Brain/neurons: May need higher MT% threshold (10-15%)
          - Blood: Can be more stringent with MT% (5-8%)
          - Tumor samples: Often need more permissive thresholds due to biological variation

          ## Advanced QC Considerations

          ### Not Included in This Workflow

          **Ambient RNA correction**
          - Tool: SoupX, CellBender, DecontX
          - When: High background RNA in droplet-based data
          - Effect: Removes contamination from lysed cells

          **Doublet detection**
          - Tool: scDblFinder, scrublet, DoubletFinder
          - When: Always recommended for droplet-based data
          - Effect: Identifies and removes multiplets (2+ cells in one droplet)

          **Cell cycle scoring**
          - Tool: scanpy's score_genes_cell_cycle
          - When: Cell cycle effects confound biological signal
          - Effect: Allows regressing out or accounting for cell cycle phase

          **Batch correction**
          - Tool: Harmony, scVI, ComBat
          - When: Integrating data from multiple batches/experiments
          - Effect: Removes technical batch effects while preserving biology

          ## References

          - scverse Best Practices: https://www.sc-best-practices.org/preprocessing_visualization/quality_control.html
          - Luecken & Theis (2019): Current best practices in single-cell RNA-seq analysis
          - Osorio & Cai (2021): Systematic determination of the mitochondrial proportion in human and mouse genomes
          - Germain et al. (2020): Doublet identification in single-cell sequencing data using scDblFinder
  environment:
    - name: MESHAGENT_TOKEN
      token:  
        identity: claude-bio-research
        api:
          livekit: {}
          queues:
            list: true
          messaging:
            broadcast: true
            list: true
            send: true
          database:
            list_tables: true
          sync: {}
          storage: {}
          containers:
            logs: true
            use_containers: true
          developer:
            logs: true
          agents:
            register_agent: true
            register_public_toolkit: true
            register_private_toolkit: true
            call: true
            use_agents: true
            use_tools: true
            allowed_toolkits: null
